{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "if 'EXECUTE_PIP' not in locals():\n",
        "    EXECUTE_PIP = True    "
      ],
      "metadata": {
        "id": "OhJ_83Sz6JWx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j-gilyQlVH5n"
      },
      "outputs": [],
      "source": [
        "# Solving a locale problem\n",
        "if False:\n",
        "  !pip install turicreate\n",
        "  import turicreate as tc\n",
        "  import os\n",
        "  try:\n",
        "    del os.environ['LC_ALL']\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing TensorRT with all its dependencies\n",
        "if EXECUTE_PIP:\n",
        "  !pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0PiD5Qd5vE",
        "outputId": "f6fb203f-6085-446b-ff31-7e212ba7ea69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.ngc.nvidia.com, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorrt/nvidia_tensorrt-8.4.1.5-cp37-none-linux_x86_64.whl (774.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 774.4 MB 17 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu11/nvidia-cudnn-cu11-2022.5.19.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu11/nvidia-cublas-cu11-2022.4.8.tar.gz (16 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu11/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu117/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl (333.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 333.1 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (57.4.0)\n",
            "Collecting nvidia-cuda-runtime-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu117/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 49.5 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu116\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu116/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl (719.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 719.3 MB 17 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-cublas-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11\n",
            "  Building wheel for nvidia-cublas-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cublas-cu11: filename=nvidia_cublas_cu11-2022.4.8-py3-none-any.whl size=15624 sha256=f53afaa7197c85884acc788c92d87203715b69fb076e915339096f36dbf0569c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/c3/94/1ffd5bac267cfdc2b222a4ec6915278ef18a028a916b9a5ac3\n",
            "  Building wheel for nvidia-cuda-runtime-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cuda-runtime-cu11: filename=nvidia_cuda_runtime_cu11-2022.4.25-py3-none-any.whl size=15696 sha256=eee03e7f3780898c273311202ddcb5ca340ba8256c6a0737e281ec02153072c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/fe/2b/e553db7867508b2268b14ac194e9ac5b3f51f21316c282c96c\n",
            "  Building wheel for nvidia-cudnn-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cudnn-cu11: filename=nvidia_cudnn_cu11-2022.5.19-py3-none-any.whl size=15617 sha256=d4a79153e15fae3d89d41e91ec2bc0fb285cb9e3be6f62217b8473c62101a2a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/32/69/9787704b5f889217708864db5e00812c8c1c349ef89084c59c\n",
            "Successfully built nvidia-cublas-cu11 nvidia-cuda-runtime-cu11 nvidia-cudnn-cu11\n",
            "Installing collected packages: nvidia-cudnn-cu116, nvidia-cuda-runtime-cu117, nvidia-cublas-cu117, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-2022.4.8 nvidia-cublas-cu117-11.10.1.25 nvidia-cuda-runtime-cu11-2022.4.25 nvidia-cuda-runtime-cu117-11.7.60 nvidia-cudnn-cu11-2022.5.19 nvidia-cudnn-cu116-8.4.0.27 nvidia-tensorrt-8.4.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TensorRTUtils:**"
      ],
      "metadata": {
        "id": "t0-7SeTwQje7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorRTUtils\n",
        "if EXECUTE_PIP:\n",
        "  !pip install pycuda\n",
        "  !pip install tensorrt\n",
        "import tensorrt as trt\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class MatrixIterator:\n",
        "    \"\"\"Class to implement an iterator on a matrix\"\"\"\n",
        "\n",
        "    def __init__(self, matrix, n=0, max=0):\n",
        "        self.matrix = matrix\n",
        "        if max > 0:\n",
        "          self.max    = max\n",
        "        else:\n",
        "          self.max    = matrix.shape[0]\n",
        "        self.n      = n\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n <= self.max:\n",
        "            result = self.matrix[self.n,:,:].squeeze()\n",
        "            self.n += 1\n",
        "            return result\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "    def first(self):\n",
        "        return self.matrix[0,:,:].squeeze()\n",
        "\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "class ErrorRecorder(trt.IErrorRecorder):\n",
        "    def __init__(self):\n",
        "        trt.IErrorRecorder.__init__(self)\n",
        "        self.errorsStack = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.errorsStack.clear()\n",
        "    def get_error_code(self, arg0):\n",
        "        #Error code saved in the error tuple first position\n",
        "        return self.errorsStack[arg0][0]\n",
        "    def get_error_desc(self, arg0):\n",
        "        # Error code saved in the error tuple second position\n",
        "        return self.errorsStack[arg0][1]\n",
        "    def has_overflowed(self):\n",
        "        return False\n",
        "    def num_errors(self):\n",
        "        return len(self.errorsStack)\n",
        "    def report_error(self, arg0, arg1):\n",
        "        error = (arg0, arg1)\n",
        "        #Errors will be saved as a list of tuples, each tuple will be a pair of error code and error description\n",
        "        self.errorsStack.append(error)\n",
        "\n",
        "class Logger(trt.ILogger):\n",
        "    def __init__(self):\n",
        "        trt.ILogger.__init__(self)\n",
        "\n",
        "    def log(self, severity, msg):\n",
        "        if severity == trt.ILogger.INTERNAL_ERROR:\n",
        "            print('INTERNAL_ERROR')\n",
        "        elif severity == trt.ILogger.ERROR:\n",
        "            print('TRT - ERROR')\n",
        "        elif severity == trt.ILogger.WARNING:\n",
        "            print('TRT - WARNING')\n",
        "        elif severity == trt.ILogger.INFO:\n",
        "            print('TRT - INFO')\n",
        "        elif severity == trt.ILogger.VERBOSE:\n",
        "            print('TRT - VERBOSE')\n",
        "        else:\n",
        "            print('TRT - Wrong severity')\n",
        "\n",
        "        print(msg)\n",
        "\n",
        "class Int8EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
        "    def __init__(self, calibrationSetPath = None, calibSet = None):\n",
        "        # Whenever you specify a custom constructor for a TensorRT class,\n",
        "        # you MUST call the constructor of the parent explicitly.\n",
        "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
        "\n",
        "        self.cacheFile = calibrationSetPath + '/CacheFile.bin'\n",
        "        self.batchSize = 1\n",
        "        self.currentIndex = 0\n",
        "        self.deviceInput = None\n",
        "        self.currentIndex = 0\n",
        "        self.PreProcessedSetPath = calibrationSetPath + '/PreProcessedSet'\n",
        "        self.PreProcessedSetCount = calibSet.max\n",
        "        self.PreProcessedSize = calibSet.first().size * 4 #float\n",
        "        self.currentIndex = 0\n",
        "\n",
        "        # Allocate enough memory for a whole batch.\n",
        "        self.deviceInput = cuda.mem_alloc(self.PreProcessedSize)\n",
        "\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            print('Calibration cache file already exists - ', self.cacheFile)\n",
        "            return\n",
        "\n",
        "        if os.path.isdir(self.PreProcessedSetPath):\n",
        "            filesCnt = os.listdir(self.PreProcessedSetPath)\n",
        "\n",
        "            if len(filesCnt) == self.PreProcessedSetCount:\n",
        "                print('ERROR - Pre processed file set exists!!!')\n",
        "                return\n",
        "        else:\n",
        "            os.mkdir(self.PreProcessedSetPath)\n",
        "\n",
        "        if self.PreProcessedSetCount == 0:\n",
        "            print('ERROR - Calibration set is empty!!!')\n",
        "\n",
        "        print('Start calibration batches build')\n",
        "\n",
        "        print(f\"Nir: PreProcessedSetCount = {self.PreProcessedSetCount}\") # Debug printing\n",
        "        for idx in range(self.PreProcessedSetCount):\n",
        "            preProcImg = next(calibSet)\n",
        "            if idx % 100 == 0:\n",
        "              print(f\"Nir: {idx} preProcImg shape: {preProcImg.shape}\") # Debug printing\n",
        "            preProcessedFile = open(self.PreProcessedSetPath + '/' + str(idx) + '.bin', mode='wb')\n",
        "            preProcImg.tofile(preProcessedFile)\n",
        "            preProcessedFile.close()\n",
        "\n",
        "        print('End calibration batches build')\n",
        "\n",
        "    def get_algorithm(self):\n",
        "        return trt.CalibrationAlgoType.ENTROPY_CALIBRATION_2\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batchSize\n",
        "\n",
        "    # TensorRT passes along the names of the engine bindings to the get_batch function.\n",
        "    # You don't necessarily have to use them, but they can be useful to understand the order of\n",
        "    # the inputs. The bindings list is expected to have the same ordering as 'names'.\n",
        "    def get_batch(self, names):\n",
        "        if not self.currentIndex < self.PreProcessedSetCount:\n",
        "            return None\n",
        "\n",
        "        print('Get pre processed file index - ', not self.currentIndex)\n",
        "\n",
        "        batchData = np.fromfile(self.PreProcessedSetPath + '/' + str(self.currentIndex) + '.bin', dtype=np.single)\n",
        "        cuda.memcpy_htod(self.deviceInput, batchData)\n",
        "        self.currentIndex += 1\n",
        "\n",
        "        return [self.deviceInput]\n",
        "\n",
        "    def read_calibration_cache(self):\n",
        "        # If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            with open(self.cacheFile, \"rb\") as f:\n",
        "                return f.read()\n",
        "\n",
        "    def write_calibration_cache(self, cache):\n",
        "        with open(self.cacheFile, \"wb\") as f:\n",
        "            f.write(cache)\n",
        "\n",
        "logger = Logger()\n",
        "errorRecorder = ErrorRecorder()\n",
        "\n",
        "builder = trt.Builder(logger)\n",
        "builder.max_batch_size = 1\n",
        "\n",
        "calib = None\n",
        "config = builder.create_builder_config()\n",
        "config.max_workspace_size = 1073741824\n",
        "\n",
        "optimizationProfiler = builder.create_optimization_profile()\n",
        "\n",
        "networkFlags = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "network = builder.create_network(networkFlags)\n",
        "parser = trt.OnnxParser(network, logger)\n",
        "runtime = trt.Runtime(logger)\n",
        "\n",
        "engine = None\n",
        "context = None\n",
        "\n",
        "modelName = None\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "bindings = []\n",
        "stream = None\n",
        "\n",
        "def TrtModelParse(modelPath):\n",
        "    global modelName\n",
        "    global parser\n",
        "    global network\n",
        "\n",
        "    modelName = modelPath.split('.')[0]\n",
        "    parseResult = parser.parse_from_file(modelPath)\n",
        "\n",
        "    if (not parseResult):\n",
        "        for error in range(parser.num_errors):\n",
        "            print(str(parser.get_error(error)))\n",
        "    else:\n",
        "        print(\"Model parsing OK!\")\n",
        "\n",
        "        print(\"Network Description\")\n",
        "\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "\n",
        "        for input in inputs:\n",
        "            print(\"Input '{}' with shape {} and dtype {}\".format(input.name, input.shape, input.dtype))\n",
        "        for output in outputs:\n",
        "            print(\"Output '{}' with shape {} and dtype {}\".format(output.name, output.shape, output.dtype))\n",
        "\n",
        "def TrtModelOptimizeAndSerialize(precision = 'fp32',calibPath=\"\", calibSet=None):\n",
        "    global modelName\n",
        "    global builder\n",
        "    global optimizationProfiler\n",
        "    global calib\n",
        "    global config\n",
        "    global network\n",
        "    global engine\n",
        "    global runtime\n",
        "\n",
        "    global g_DEBUG_network\n",
        "    global g_DEBUG_config\n",
        "\n",
        "    modelOptName = modelName + precision + '.trt.engine'\n",
        "\n",
        "    if os.path.exists(modelOptName):\n",
        "        with open(modelOptName, 'rb') as f:\n",
        "            engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    else:\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        input = network.get_input(0)\n",
        "\n",
        "        inputShape = [1, input.shape[1], input.shape[2], input.shape[3]]\n",
        "\n",
        "        optimizationProfiler.set_shape(input.name, inputShape, inputShape, inputShape)\n",
        "\n",
        "        config.add_optimization_profile(optimizationProfiler)\n",
        "\n",
        "        if precision == 'fp16':\n",
        "            if builder.platform_has_fast_fp16:\n",
        "                config.set_flag(trt.BuilderFlag.FP16)\n",
        "        elif precision == 'int8':\n",
        "            if builder.platform_has_fast_int8:\n",
        "                if builder.platform_has_fast_fp16:\n",
        "                    # Also enable fp16, as some layers may be even more efficient in fp16 than int8\n",
        "                    config.set_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "                config.set_flag(trt.BuilderFlag.INT8)\n",
        "\n",
        "                calib = Int8EntropyCalibrator(calibPath, calibSet)\n",
        "                config.int8_calibrator = calib\n",
        "\n",
        "        g_DEBUG_network = network\n",
        "        g_DEBUG_config  = config\n",
        "        engine = builder.build_engine(network, config)\n",
        "\n",
        "        serializedEngine = engine.serialize()\n",
        "\n",
        "        engineFD = open(modelOptName, 'wb')\n",
        "        engineFD.write(serializedEngine)\n",
        "        engineFD.close()\n",
        "\n",
        "    print('TRT engine - ', engine.device_memory_size, ' Bytes')\n",
        "    engineDeviceMemory = 0\n",
        "    engineDeviceMemory += engine.device_memory_size\n",
        "    print('TRT engine number of layers - ', engine.num_layers)\n",
        "    print('TRT engine number of bindings - ', engine.num_bindings)\n",
        "    print('TRT engine number of profils - ', engine.num_optimization_profiles)\n",
        "\n",
        "    print('Completion optimized model')\n",
        "\n",
        "def ModelInferSetup():\n",
        "    global context\n",
        "    global engine\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "    global stream\n",
        "\n",
        "    stream = cuda.Stream()\n",
        "\n",
        "    #Over all Tensors inputs & outputs of the TRT engine\n",
        "    #TRT hold first all Tensors inputs and after the Tensor outptus\n",
        "    for binding in engine:\n",
        "        #Get current binded Tensor volume size in elemente units\n",
        "        size = trt.volume(engine.get_binding_shape(binding))\n",
        "        #Get current binded Tensor element type\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host page locked bbuffer\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        # Allocate device bbuffer\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "\n",
        "    # Contexts are used to perform inference.\n",
        "    context = engine.create_execution_context()\n",
        "    context.error_recorder = errorRecorder\n",
        "\n",
        "def Inference(externalnputs = None):\n",
        "\n",
        "    global context\n",
        "    global stream\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "\n",
        "    try:\n",
        "        #verify that TRT context generated successfully\n",
        "        if context is not None:\n",
        "            #Verify that inputs to inference are exist\n",
        "            if externalnputs is not None:\n",
        "                #Copy all Tensors inputs data from user memory to TRT host page locked memory before loading it to the device\n",
        "                if len(externalnputs) == len(inputs):\n",
        "                    for index in range(len(externalnputs)):\n",
        "                        if len(inputs[index].host) == externalnputs[index].size:\n",
        "                            np.copyto(inputs[index].host, externalnputs[index].ravel())\n",
        "                        else:\n",
        "                            print('TRT external input size - ', externalnputs[index].size,\n",
        "                                  ' is not equal to model inputs size - ', len(inputs[index].host))\n",
        "                            return None\n",
        "\n",
        "                    # Transfer input data to the GPU from the host page locked memory.\n",
        "                    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "                    # Run asynchronously inference using the user\\internal stream.\n",
        "                    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "                    # Transfer predictions back from the GPU.\n",
        "                    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "\n",
        "                    stream.synchronize()\n",
        "                    # Build a list of Tensors outputs and return only the host outputs.\n",
        "                    return [out.host for out in outputs]\n",
        "                else:\n",
        "                    print('External inputs list size - ', len(externalnputs), ' is not equal to model inputs list size - ', len(inputs))\n",
        "                    return None\n",
        "            else:\n",
        "                print('External inputs list is None ERROR')\n",
        "                return None\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        print('TRT inference exception ERROR - ', msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzctBFwmVnAo",
        "outputId": "114e53da-689c-49c3-8d15-09fb31b4f169"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRT - INFO\n",
            "The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
            "\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 1243 (MiB)\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init builder kernel library: CPU +0, GPU +68, now: CPU 0, GPU 1311 (MiB)\n",
            "TRT - INFO\n",
            "The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
            "\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 1311 (MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:172: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:176: DeprecationWarning: Use set_memory_pool_limit instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **onnxUtils:**"
      ],
      "metadata": {
        "id": "2cyDtE0LQrOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# onnxUtils\n",
        "if EXECUTE_PIP:\n",
        "  !pip install tf2onnx onnx onnxsim\n",
        "import json\n",
        "import time\n",
        "import tf2onnx\n",
        "import onnx\n",
        "#import onnxsim\n",
        "import os.path\n",
        "\n",
        "\n",
        "# Save model into h5 and ONNX formats\n",
        "def convertKerasToONNX(name, model, overwrite_existing = False):\n",
        "    modelFile = name + '.onnx'\n",
        "    if not os.path.isfile(modelFile) or overwrite_existing:\n",
        "        # Save model with ONNX format\n",
        "        (onnx_model_proto, storage) = tf2onnx.convert.from_keras(model)\n",
        "        with open(os.path.join(modelFile), \"wb\") as f:\n",
        "            f.write(onnx_model_proto.SerializeToString())\n",
        "            f.close()\n",
        "    \n",
        "    return modelFile, onnx_model_proto, storage\n",
        "\n",
        "def ModelOnnxCheck(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    isCheckOk = True\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"Onnx model check report:\")\n",
        "\n",
        "    try:\n",
        "        # Perform basic check on the model input\n",
        "        onnx.checker.check_model(name + '.onnx')\n",
        "        isCheckOk = True\n",
        "    except onnx.checker.ValidationError as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "\n",
        "    if isCheckOk:\n",
        "        print('Model check completed Successfully')\n",
        "    else:\n",
        "        print('ERROR - Model check failure')\n",
        "\n",
        "    print('Model onnx checker, check model - ', msg)\n",
        "\n",
        "    return isCheckOk\n",
        "\n",
        "def RemoveInitializerFromInput(model, modelPath):\n",
        "    modelGraphInputs = model.graph.input\n",
        "    startInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    nameToInput = {}\n",
        "    for input in modelGraphInputs:\n",
        "        nameToInput[input.name] = input\n",
        "\n",
        "    for initializer in model.graph.initializer:\n",
        "        if initializer.name in nameToInput:\n",
        "            modelGraphInputs.remove(nameToInput[initializer.name])\n",
        "\n",
        "    endInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    if startInputsCount != endInputsCount:\n",
        "        print('Model includes several Initializers which considered as inputs to the graph - ', startInputsCount - endInputsCount)\n",
        "        print('All Initializers were removed from graph inputs')\n",
        "        print('Replace the model *.onx file with the updated one')\n",
        "        onnx.save(model, modelPath)\n",
        "\n",
        "def ProcessModelInputs(model, modelPath):\n",
        "    RemoveInitializerFromInput(model, modelPath)\n",
        "    modelGraphInputs = model.graph.input\n",
        "\n",
        "    modelInputsDims = {}\n",
        "    modelDynamicInputsDict = {}\n",
        "    modelInputs = modelGraphInputs\n",
        "    modelInputsNames = []\n",
        "    print(str(modelInputs))\n",
        "\n",
        "    for tensorInput in modelInputs:\n",
        "        isInputDynamic = False\n",
        "        modelDynamicInputShape = []\n",
        "        for dim in tensorInput.type.tensor_type.shape.dim:\n",
        "            if dim.dim_value == 0:\n",
        "                isInputDynamic = True\n",
        "                print('CAUTION!!! - Tensor input name' + ' - ', tensorInput.name, ', dimension - ' , dim.dim_param, ', set its value to 1 for Onnx simplify operation')\n",
        "                modelDynamicInputShape.append(1)\n",
        "            else:\n",
        "                modelDynamicInputShape.append(dim.dim_value)\n",
        "\n",
        "        modelInputsNames.append(tensorInput.name)\n",
        "\n",
        "        if isInputDynamic is True:\n",
        "            modelDynamicInputsDict[tensorInput.name] = modelDynamicInputShape\n",
        "\n",
        "    return modelDynamicInputsDict\n",
        "\n",
        "def ModelSimplify(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    nameSimp = name + 'Simp'\n",
        "    model = None\n",
        "    isSimplifiedOK = True\n",
        "\n",
        "    if os.path.exists(nameSimp + '.onnx'):\n",
        "        print('Model Onnx simplify is already exist, No model check and\\or simplify operations is required')\n",
        "        model = onnx.load(nameSimp + '.onnx')\n",
        "        isSimplifiedOK = True\n",
        "    else:\n",
        "        print(\"===============================================================\")\n",
        "        print(\"Onnx model simplifier report:\")\n",
        "        model = onnx.load(name + '.onnx')\n",
        "\n",
        "        modelDynamicInputsDict = ProcessModelInputs(model, name + '.onnx')\n",
        "\n",
        "        try:\n",
        "            print('Start model onnx simplify...')\n",
        "            # Perform simplification on the model input\n",
        "            model, check = onnxsim.simplify(model,input_shapes=modelDynamicInputsDict,\n",
        "                                                  dynamic_input_shape=(len(modelDynamicInputsDict) > 0))\n",
        "            print('Completion model onnx simplify')\n",
        "            if (check):\n",
        "                isSimplifiedOK = True\n",
        "                print('Onnx simplification success!')\n",
        "                print('Save Onnx simplified model to - ', nameSimp + '.onnx')\n",
        "                onnx.save(model, nameSimp + '.onnx')\n",
        "            else:\n",
        "                isSimplifiedOK = False\n",
        "                print('Onnx simplification failure!')\n",
        "                print('Simplified Onnx model could not be generated and validated')\n",
        "        except BaseException as e:\n",
        "            print('Onnx simplification exception - ', e)"
      ],
      "metadata": {
        "id": "yU_AzfphVoRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf600c4-51f3-47ed-e2ce-bff39c895ac0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.12.0-py3-none-any.whl (442 kB)\n",
            "\u001b[K     |████████████████████████████████| 442 kB 32.2 MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting onnxsim\n",
            "  Downloading onnxsim-0.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers~=1.12 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Collecting rich\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (3.0.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: commonmark, rich, onnx, tf2onnx, onnxsim\n",
            "Successfully installed commonmark-0.9.1 onnx-1.12.0 onnxsim-0.4.7 rich-12.5.1 tf2onnx-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **wandb_helpers:**"
      ],
      "metadata": {
        "id": "Yuy6_TxZQvKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb_helpers\n",
        "if EXECUTE_PIP:\n",
        "  !pip install wandb\n",
        "  EXECUTE_PIP = False\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "Dataset = namedtuple(\"Dataset\", [\"images\", \"labels\"])\n",
        "dataset_names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "def start_wandb_run(model_name, config):\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    return wandb.init(project=f\"ml-p2\", entity=\"ml-p2\", name=f\"{model_name}-{timestamp}\" , \n",
        "        notes = f\"Training FCNN model @{timestamp}\", config = config)\n",
        "\n",
        "def read_datasets(wandb_run, dataset_tag = \"latest\"):\n",
        "    '''\n",
        "    Read all datasets from W&B.\n",
        "    Usage example: train_set, validation_set, test_set = wbh.read_datasets(run)\n",
        "    '''\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/fashion-mnist:{dataset_tag}', type='dataset')\n",
        "    data_dir = artifact.download()\n",
        "    return [ read_dataset(data_dir, ds_name) for ds_name in dataset_names ]\n",
        "\n",
        "def read_dataset(data_dir, ds_name):\n",
        "    filename = ds_name + \".npz\"\n",
        "    data = np.load(os.path.join(data_dir, filename))\n",
        "    return Dataset(images = data[\"x\"], labels = data[\"y\"])\n",
        "\n",
        "def read_model(wandb_run, model_name, model_tag = \"latest\") -> tf.keras.models.Model:\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/{model_name}:{model_tag}', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    return tf.keras.models.load_model(artifact_dir)\n",
        "\n",
        "def save_model(wandb_run, model, config, model_name, model_description):\n",
        "    model_file = f'./saved-models/{model_name}.tf'\n",
        "    tf.keras.models.save_model(model, model_file)\n",
        "    model_artifact = wandb.Artifact(model_name, type = \"model\", description=model_description, metadata= dict(config))\n",
        "    model_artifact.add_dir(model_file)\n",
        "    wandb_run.log_artifact(model_artifact)\n",
        "\n",
        "def load_best_model(sweep_id):\n",
        "    api = wandb.Api()\n",
        "    sweep = api.sweep(f\"ml-p2/ml-p2/{sweep_id}\")\n",
        "    runs = sorted(sweep.runs,\n",
        "        key=lambda run: run.summary.get(\"val_accuracy\", 0), reverse=True)\n",
        "    val_acc = runs[0].summary.get(\"val_accuracy\", 0)\n",
        "    print(f\"Best run {runs[0].name} with {val_acc} validation accuracy\")\n",
        "\n",
        "    model_file = runs[0].file(\"model-best.h5\").download(replace=True)\n",
        "    model_file.close()\n",
        "\n",
        "#if (__name__ == \"__main__\"):\n",
        "#    load_best_model(\"6zmewzd0\")"
      ],
      "metadata": {
        "id": "8_iT8JNVVobY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2594c3-8898-4a66-f02f-5784a7fc71c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 18.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 66.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 83.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 79.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2518d3d6d5ebe8d34f30f0b90bbc93bc9e8c5af9a882181172a3717875c70600\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **trt-inference:**"
      ],
      "metadata": {
        "id": "HynoQt2cQzkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trt-inference\n",
        "#!pip install sklearn -qqq\n",
        "\n",
        "#from TensorRTUtils import *\n",
        "#from onnxUtils import convertKerasToONNX\n",
        "#import wandb_helpers as wbh\n",
        "\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import tensorrt as trt\n",
        "import onnx\n",
        "import tf2onnx\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "modelName = \"FCNN\"\n",
        "\n",
        "'''\n",
        "Stage 1: Load an existing model\n",
        "===============================\n",
        "In this part we load the model we created in the previous project\n",
        "which is built to infer from FASHION-MNIST images.\n",
        "It is not a sofisticated model, but the idea to use something we\n",
        "know.\n",
        "'''\n",
        "dataset_path = '.\\\\artifacts\\\\fashion-mnist-v2'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    with start_wandb_run(\"FCNN-metrics\", None) as run:\n",
        "        train_set, validation_set, test_set = read_datasets(run)\n",
        "        model = read_model(run, \"FCNN\", \"latest\")\n",
        "else:\n",
        "    test_set = read_dataset('.\\\\artifacts\\\\fashion-mnist-v2', 'test')\n",
        "    model = tf.keras.models.load_model('.\\\\artifacts\\\\FCNN-v3')\n",
        "\n",
        "'''\n",
        "Stage 2: Convert to ONNX\n",
        "========================\n",
        "Convert the model to ONNX and save it to a file. This will allow\n",
        "us to load the model into a tensor-rt engine.\n",
        "'''\n",
        "modelFile, _, _ = convertKerasToONNX(modelName, model, True)\n",
        "\n",
        "'''\n",
        "Stage 3: Create the tensor-rt engine\n",
        "====================================\n",
        "Now that we a model file, we can load it into a \n",
        "tensor rt engine.\n",
        "We use FP 32 precision.\n",
        "'''\n",
        "TrtModelParse(modelFile)\n",
        "print(\"===================================\")\n",
        "print(\"Before TrtModelOptimizeAndSerialize\")\n",
        "print(\"===================================\")\n",
        "#TrtModelOptimizeAndSerialize(precision='fp32')\n",
        "#TrtModelOptimizeAndSerialize(precision='fp16')\n",
        "calibSet=MatrixIterator(validation_set.images)\n",
        "TrtModelOptimizeAndSerialize(precision='int8', calibPath=\"/content\", calibSet=calibSet)\n",
        "print(\"===================================\")\n",
        "print(\"After TrtModelOptimizeAndSerialize\")\n",
        "print(\"===================================\")\n",
        "ModelInferSetup()\n",
        "\n",
        "'''\n",
        "Stage 4: Inference\n",
        "==================\n",
        "Now the model is ready for inference. The model is executed several\n",
        "times on different images from the test set we've loaded on Stage 1\n",
        "'''\n",
        "inputs = []\n",
        "\n",
        "startTimeCpu = time.time()\n",
        "for i in range(len(test_set)):\n",
        "    img = test_set.images[i]\n",
        "    lbl = test_set.labels[i]\n",
        "    inputs.append(img)\n",
        "    outputsTrt = Inference(externalnputs=inputs)\n",
        "    #print(' topClassIdx - ', np.argmax(outputsTrt[0]))\n",
        "    inputs.clear()\n",
        "    \n",
        "    \n",
        "endTimeCpu = time.time()\n",
        "\n",
        "# total time taken\n",
        "averageTime = (endTimeCpu - startTimeCpu) / 1e-3 / len(test_set)\n",
        "print(f\"TRT Keras inference average time is: {averageTime} milliseconds\")\n",
        "print(f\"TRT Keras inference average FPS is: {1000 / averageTime}\")\n",
        "\n",
        "# Perform the DlewareAnalyzer inference with TRT & ORT\n",
        "\n",
        "#np.testing.assert_allclose(kerasPredictions, onnxPredictions[0], rtol=0, atol=1e-05, err_msg='Keras Vs. Onnx Failure!!!')\n",
        "\n",
        "\n",
        "#y_test = np.argmax(test_set.labels)\n",
        "# predictions = model.predict(test_set.images)\n",
        "# y_test = np.argmax(predictions, axis = 1)\n",
        "# print (classification_report(test_set.labels, y_test))\n",
        "# cm = confusion_matrix(test_set.labels, y_test)\n",
        "\n",
        "# class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "# ax = plt.subplot()\n",
        "# h = sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# # labels, title and ticks\n",
        "# ax.set_xlabel('Predicted labels')\n",
        "# ax.set_ylabel('True labels')\n",
        "# ax.set_title('Confusion Matrix')\n",
        "# ax.xaxis.set_ticklabels(class_names)\n",
        "# ax.yaxis.set_ticklabels(class_names)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "EEa4aCwOWALB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e3c06dcf-b8e1-4386-ad96-89d443f083a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-405b2f6ff935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2onnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging builder.build_engin\n",
        "global g_DEBUG_network\n",
        "global g_DEBUG_config\n",
        "\n",
        "print(g_DEBUG_network)\n",
        "print(g_DEBUG_config)\n",
        "\n",
        "print(f\"PreProcessedSetCount = {g_DEBUG_config.int8_calibrator.PreProcessedSetCount}\")\n",
        "print(f\"PreProcessedSetPath  = {g_DEBUG_config.int8_calibrator.PreProcessedSetPath}\")\n",
        "print(f\"PreProcessedSize     = {g_DEBUG_config.int8_calibrator.PreProcessedSize}\")\n",
        "print(f\"batchSize            = {g_DEBUG_config.int8_calibrator.batchSize}\")\n",
        "print(f\"cacheFile            = {g_DEBUG_config.int8_calibrator.cacheFile}\")\n",
        "print(f\"currentIndex         = {g_DEBUG_config.int8_calibrator.currentIndex}\")\n",
        "print(f\"deviceInput          = {g_DEBUG_config.int8_calibrator.deviceInput}\")\n",
        "\n",
        "engine = builder.build_engine(network, config)\n",
        "engine\n",
        "print(f\"engine               = {engine}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhCGgtrshSpr",
        "outputId": "cec9f009-4f15-419e-cbfd-a4beb44f4145"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorrt.tensorrt.INetworkDefinition object at 0x7fb582c4c4b0>\n",
            "<tensorrt.tensorrt.IBuilderConfig object at 0x7fb582c2dcb0>\n",
            "PreProcessedSetCount = 5000\n",
            "PreProcessedSetPath  = /content/PreProcessedSet\n",
            "PreProcessedSize     = 3136\n",
            "batchSize            = 1\n",
            "cacheFile            = /content/CacheFile.bin\n",
            "currentIndex         = 0\n",
            "deviceInput          = <pycuda._driver.DeviceAllocation object at 0x7fb5727cb670>\n",
            "TRT - VERBOSE\n",
            "Original: 26 layers\n",
            "TRT - VERBOSE\n",
            "After dead-layer removal: 26 layers\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 4) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_1/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 10) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_2/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 16) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_3/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 22) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleErasure on (Unnamed Layer* 25) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Removing (Unnamed Layer* 25) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "After Myelin optimization: 21 layers\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_1/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_2/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_3/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleShuffleFusion on sequential/flatten/Reshape\n",
            "TRT - VERBOSE\n",
            "ShuffleShuffleFusion: Fusing sequential/flatten/Reshape with reshape_before_sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ActivationToPointwiseConversion on sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "Swap the layer type of sequential/dense/Relu from ACTIVATION to POINTWISE\n",
            "TRT - VERBOSE\n",
            "Running: ActivationToPointwiseConversion on sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "Swap the layer type of sequential/dense_1/Relu from ACTIVATION to POINTWISE\n",
            "TRT - VERBOSE\n",
            "Running: ActivationToPointwiseConversion on sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "Swap the layer type of sequential/dense_2/Relu from ACTIVATION to POINTWISE\n",
            "TRT - VERBOSE\n",
            "After final dead-layer removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After vertical fusions: 16 layers\n",
            "TRT - VERBOSE\n",
            "After final dead-layer removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After slice removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After concat removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After tensor merging: 16 layers\n",
            "TRT - VERBOSE\n",
            "Trying to split Reshape and strided tensor\n",
            "TRT - VERBOSE\n",
            "Using cublasLt as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 0, GPU 1319 (MiB)\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1327 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - INFO\n",
            "Timing cache disabled. Turning it on will improve builder speed.\n",
            "TRT - WARNING\n",
            "Calibration Profile is not defined. Running calibration with Profile 0\n",
            "TRT - VERBOSE\n",
            "Constructing calibration profile.\n",
            "TRT - VERBOSE\n",
            "Reserving memory for host IO tensors. Host: 0 bytes\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,28,1,1) -> Float(784,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.032768\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,1,1,1) -> Float(155,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.157856\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1,1,1) -> Float(155,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.02416\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1) -> Float(155,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: PWN(sequential/dense/Relu) (PointWiseV2)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.016352\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1) -> Float(155,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_before_sequential/dense_1/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.020512\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1,1,1) -> Float(144,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_1/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_1/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_1/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0664\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1,1,1) -> Float(144,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense_1/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.11056\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1) -> Float(144,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: PWN(sequential/dense_1/Relu) (PointWiseV2)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.014784\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1) -> Float(144,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_before_sequential/dense_2/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.020224\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1,1,1) -> Float(63,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_2/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_2/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_2/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.050432\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1,1,1) -> Float(63,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense_2/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.100352\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1) -> Float(63,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: PWN(sequential/dense_2/Relu) (PointWiseV2)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.014208\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1) -> Float(63,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_before_sequential/dense_3/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.021472\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1,1,1) -> Float(10,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.052\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(10,1,1,1) -> Float(10,1) ***************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Use build_serialized_network instead.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense_3/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.02048\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(10,1) -> Float(10,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/Softmax (CudaSoftMax)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.034496\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudaSoftMax Tactic: 0x00000000000003ea\n",
            "TRT - VERBOSE\n",
            "Formats and tactics selection completed in 0.160029 seconds.\n",
            "TRT - VERBOSE\n",
            "After reformat layers: 16 layers\n",
            "TRT - VERBOSE\n",
            "Pre-optimized block assignment.\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 3584\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 1073741824\n",
            "TRT - VERBOSE\n",
            "Total Activation Memory: 1073756672\n",
            "TRT - INFO\n",
            "Detected 1 inputs and 1 output network tensors.\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: PWN(sequential/dense/Relu) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_before_sequential/dense_1/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_1/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense_1/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: PWN(sequential/dense_1/Relu) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_before_sequential/dense_2/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_2/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense_2/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: PWN(sequential/dense_2/Relu) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_before_sequential/dense_3/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_3/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense_3/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_3/Softmax Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - INFO\n",
            "Total Host Persistent Memory: 896\n",
            "TRT - INFO\n",
            "Total Device Persistent Memory: 0\n",
            "TRT - INFO\n",
            "Total Scratch Memory: 0\n",
            "TRT - INFO\n",
            "[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB\n",
            "TRT - INFO\n",
            "[BlockAssignment] Algorithm ShiftNTopDown took 0.063773ms to assign 2 blocks to 15 nodes requiring 4608 bytes.\n",
            "TRT - VERBOSE\n",
            "Optimized block assignment.\n",
            "TRT - VERBOSE\n",
            "Block size 3584\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - INFO\n",
            "Total Activation Memory: 4608\n",
            "TRT - VERBOSE\n",
            "Disabling unused tactic source: CUBLAS, CUBLAS_LT\n",
            "TRT - VERBOSE\n",
            "Disabling unused tactic source: EDGE_MASK_CONVOLUTIONS\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1339 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - VERBOSE\n",
            "Engine generation completed in 0.228221 seconds.\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1323 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - VERBOSE\n",
            "Total per-runner device persistent memory is 0\n",
            "TRT - VERBOSE\n",
            "Total per-runner host persistent memory is 896\n",
            "TRT - VERBOSE\n",
            "Allocated activation device memory of size 4608\n",
            "TRT - INFO\n",
            "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4 (MiB)\n",
            "TRT - VERBOSE\n",
            "Calculating Maxima\n",
            "TRT - INFO\n",
            "Starting Calibration.\n",
            "Get pre processed file index -  True\n",
            "TRT - INFO\n",
            "  Post Processing Calibration data in 1.308e-06 seconds.\n",
            "TRT - INFO\n",
            "Calibration completed in 0.251584 seconds.\n",
            "TRT - ERROR\n",
            "4: [standardEngineBuilder.cpp::initCalibrationParams::1420] Error Code 4: Internal Error (Calibration failure occurred with no scaling factors detected. This could be due to no int8 calibrator or insufficient custom scales for network layers. Please see int8 sample to setup calibration correctly.)\n",
            "engine               = None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "#if False:\n",
        "os.listdir('PreProcessedSet')"
      ],
      "metadata": {
        "id": "WepEIpXca02W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  print(f\"train_set_shape: {train_set.images.shape}\")\n",
        "  print(f\"val_set_shape: {validation_set.images.shape}\")\n",
        "  print(f\"test_set_shape: {test_set.images.shape}\")"
      ],
      "metadata": {
        "id": "akbWEe055ILP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  my_iter = iter(validation_set)\n",
        "  curr_val = next(my_iter)\n",
        "  curr_val.shape"
      ],
      "metadata": {
        "id": "WxeJmTurZrz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  #print(validation_set.images)\n",
        "  my_iter = MatrixIterator(validation_set.images)\n",
        "  curr_val = next(my_iter)\n",
        "  type(curr_val)\n",
        "  print(np.asarray(curr_val).shape)\n",
        "  print(np.asarray(curr_val).size)\n",
        "  #print(curr_val)"
      ],
      "metadata": {
        "id": "_d4VOb6ZdsG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  my_list = [0,1,2,3,4]\n",
        "  my_iter = iter(my_list)\n",
        "  print(next(my_iter))\n",
        "  print(next(my_iter))\n",
        "  print(next(my_iter))"
      ],
      "metadata": {
        "id": "T241YoxpoeMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  from subprocess import run\n",
        "  from shlex import split\n",
        "  #string = \"pwd\".encode()\n",
        "  #run(split(\"cd ~\"))\n",
        "  #run(split(string))\n",
        "  #print(\"pwd\".encode())\n",
        "  completed_process = run(split('ls'))\n",
        "  print(completed_process.args)\n",
        "  print(completed_process.returncode)\n",
        "  print(completed_process.stdout)\n",
        "  print(completed_process.stderr)\n",
        "  #CompletedProcess(args=['python', '--version'], returncode=0)"
      ],
      "metadata": {
        "id": "FkVyFWJ2BQ1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  import textwrap\n",
        "  args = 'pwd'\n",
        "  cp = run(args)\n",
        "  print(cp.stdout)"
      ],
      "metadata": {
        "id": "KenyO6d9E1Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  import subprocess\n",
        "  subprocess.run(\"pwd\", shell=True, check=True)\n",
        "  print(completed_process.args)\n",
        "  print(completed_process.returncode)\n",
        "  print(completed_process.stdout)\n",
        "  print(completed_process.stderr)"
      ],
      "metadata": {
        "id": "GiGEU78EFlYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "if False:\n",
        "  #os.getcwd()\n",
        "  #os.mkdir('/content/PreProcessedSet')\n",
        "  os.listdir()"
      ],
      "metadata": {
        "id": "MJuBY6AgGUWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting all the global variables:\n",
        "if False:\n",
        "  del(parser)\n",
        "  del(modelName)\n",
        "  del(builder)\n",
        "  del(optimizationProfiler)\n",
        "  del(calib)\n",
        "  del(config)\n",
        "  del(network)\n",
        "  del(engine)\n",
        "  del(runtime)\n",
        "  del(context)\n",
        "  del(inputs)\n",
        "  del(outputs)\n",
        "  del(bindings)\n",
        "  del(stream)"
      ],
      "metadata": {
        "id": "0kkriKbKrsot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}