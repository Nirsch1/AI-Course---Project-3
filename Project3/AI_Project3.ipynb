{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d3e509cae684737bb8ba1eb4d80ae0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47e58d243edf4d14a23b8422b0a9edb7",
              "IPY_MODEL_8c8ff31dfed343b6939bed3d636cfae4"
            ],
            "layout": "IPY_MODEL_bd0eb9616be24c4c8964cbd1192676f7"
          }
        },
        "47e58d243edf4d14a23b8422b0a9edb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2721fd5067247e5914da6609a75ea59",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b930a4ec93c840d397180784412149ae",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "8c8ff31dfed343b6939bed3d636cfae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7108c46fe0b4507817e9d9a22ef76c3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60e779838391482ebce79bd6e6fde623",
            "value": 1
          }
        },
        "bd0eb9616be24c4c8964cbd1192676f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2721fd5067247e5914da6609a75ea59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b930a4ec93c840d397180784412149ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7108c46fe0b4507817e9d9a22ef76c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e779838391482ebce79bd6e6fde623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j-gilyQlVH5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdbc596-a45a-49cb-9bbe-da3bbc9ca797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.ngc.nvidia.com, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorrt/nvidia_tensorrt-8.4.1.5-cp37-none-linux_x86_64.whl (774.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 774.4 MB 17 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu11/nvidia-cudnn-cu11-2022.5.19.tar.gz (16 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu11/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu11/nvidia-cublas-cu11-2022.4.8.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu117/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl (333.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 333.1 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (0.37.1)\n",
            "Collecting nvidia-cuda-runtime-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu117/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 849 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu116\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu116/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl (719.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 719.3 MB 18 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-cublas-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11\n",
            "  Building wheel for nvidia-cublas-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cublas-cu11: filename=nvidia_cublas_cu11-2022.4.8-py3-none-any.whl size=15624 sha256=7bb8db08e7fefcb57f207743bb5df29b1ea101e933a7412819d090df5332a65b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/c3/94/1ffd5bac267cfdc2b222a4ec6915278ef18a028a916b9a5ac3\n",
            "  Building wheel for nvidia-cuda-runtime-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cuda-runtime-cu11: filename=nvidia_cuda_runtime_cu11-2022.4.25-py3-none-any.whl size=15696 sha256=7e7110ff080c26066a5c620f48ec6a44ab4c02da34cb3930bf7f6c410fcfc74d\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/fe/2b/e553db7867508b2268b14ac194e9ac5b3f51f21316c282c96c\n",
            "  Building wheel for nvidia-cudnn-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cudnn-cu11: filename=nvidia_cudnn_cu11-2022.5.19-py3-none-any.whl size=15617 sha256=62d98be87453719ae481347effa9de34c933353bb5b59945c4c6f0897a5f580a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/32/69/9787704b5f889217708864db5e00812c8c1c349ef89084c59c\n",
            "Successfully built nvidia-cublas-cu11 nvidia-cuda-runtime-cu11 nvidia-cudnn-cu11\n",
            "Installing collected packages: nvidia-cudnn-cu116, nvidia-cuda-runtime-cu117, nvidia-cublas-cu117, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-2022.4.8 nvidia-cublas-cu117-11.10.1.25 nvidia-cuda-runtime-cu11-2022.4.25 nvidia-cuda-runtime-cu117-11.7.60 nvidia-cudnn-cu11-2022.5.19 nvidia-cudnn-cu116-8.4.0.27 nvidia-tensorrt-8.4.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TensorRTUtils:**"
      ],
      "metadata": {
        "id": "t0-7SeTwQje7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorRTUtils\n",
        "!pip install pycuda \n",
        "!pip install tensorrt\n",
        "import tensorrt as trt\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "class ErrorRecorder(trt.IErrorRecorder):\n",
        "    def __init__(self):\n",
        "        trt.IErrorRecorder.__init__(self)\n",
        "        self.errorsStack = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.errorsStack.clear()\n",
        "    def get_error_code(self, arg0):\n",
        "        #Error code saved in the error tuple first position\n",
        "        return self.errorsStack[arg0][0]\n",
        "    def get_error_desc(self, arg0):\n",
        "        # Error code saved in the error tuple second position\n",
        "        return self.errorsStack[arg0][1]\n",
        "    def has_overflowed(self):\n",
        "        return False\n",
        "    def num_errors(self):\n",
        "        return len(self.errorsStack)\n",
        "    def report_error(self, arg0, arg1):\n",
        "        error = (arg0, arg1)\n",
        "        #Errors will be saved as a list of tuples, each tuple will be a pair of error code and error description\n",
        "        self.errorsStack.append(error)\n",
        "\n",
        "class Logger(trt.ILogger):\n",
        "    def __init__(self):\n",
        "        trt.ILogger.__init__(self)\n",
        "\n",
        "    def log(self, severity, msg):\n",
        "        if severity == trt.ILogger.INTERNAL_ERROR:\n",
        "            print('INTERNAL_ERROR')\n",
        "        elif severity == trt.ILogger.ERROR:\n",
        "            print('TRT - ERROR')\n",
        "        elif severity == trt.ILogger.WARNING:\n",
        "            print('TRT - WARNING')\n",
        "        elif severity == trt.ILogger.INFO:\n",
        "            print('TRT - INFO')\n",
        "        elif severity == trt.ILogger.VERBOSE:\n",
        "            print('TRT - VERBOSE')\n",
        "        else:\n",
        "            print('TRT - Wrong severity')\n",
        "\n",
        "        print(msg)\n",
        "\n",
        "class Int8EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
        "    def __init__(self, calibrationSetPath = None, calibSet = None):\n",
        "        # Whenever you specify a custom constructor for a TensorRT class,\n",
        "        # you MUST call the constructor of the parent explicitly.\n",
        "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
        "\n",
        "        self.cacheFile = calibrationSetPath + '/CacheFile.bin'\n",
        "        self.batchSize = 1\n",
        "        self.currentIndex = 0\n",
        "        self.deviceInput = None\n",
        "        self.currentIndex = 0\n",
        "        self.PreProcessedSetPath = calibrationSetPath + '/PreProcessedSet'\n",
        "        self.PreProcessedSetCount = calibSet.n\n",
        "        self.PreProcessedSize = calibSet[0][0].size * 4 #float\n",
        "        self.currentIndex = 0\n",
        "\n",
        "        # Allocate enough memory for a whole batch.\n",
        "        self.deviceInput = cuda.mem_alloc(self.PreProcessedSize)\n",
        "\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            print('Calibration cache file is already exist - ', self.cacheFile)\n",
        "            return\n",
        "\n",
        "        filesCnt = os.listdir(self.PreProcessedSetPath)\n",
        "\n",
        "        if len(filesCnt) == self.PreProcessedSetCount:\n",
        "            print('ERROR - Pre processed file set is exist!!!')\n",
        "            return\n",
        "\n",
        "        if self.PreProcessedSetCount == 0:\n",
        "            print('ERROR - Calibration set is empty!!!')\n",
        "\n",
        "        print('Start calibration batches build')\n",
        "\n",
        "        for idx in range(self.PreProcessedSetCount):\n",
        "            preProcImg, label = calibSet.next()\n",
        "            preProcessedFile = open(self.PreProcessedSetPath + '/' + str(idx) + '.bin', mode='wb')\n",
        "            preProcImg.tofile(preProcessedFile)\n",
        "            preProcessedFile.close()\n",
        "\n",
        "        print('End calibration batches build')\n",
        "\n",
        "    def get_algorithm(self):\n",
        "        return trt.CalibrationAlgoType.ENTROPY_CALIBRATION_2\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batchSize\n",
        "\n",
        "    # TensorRT passes along the names of the engine bindings to the get_batch function.\n",
        "    # You don't necessarily have to use them, but they can be useful to understand the order of\n",
        "    # the inputs. The bindings list is expected to have the same ordering as 'names'.\n",
        "    def get_batch(self, names):\n",
        "        if not self.currentIndex < self.PreProcessedSetCount:\n",
        "            return None\n",
        "\n",
        "        print('Get pre processed file index - ', not self.currentIndex)\n",
        "\n",
        "        batchData = np.fromfile(self.PreProcessedSetPath + '/' + str(self.currentIndex) + '.bin', dtype=np.single)\n",
        "        cuda.memcpy_htod(self.deviceInput, batchData)\n",
        "        self.currentIndex += 1\n",
        "\n",
        "        return [self.deviceInput]\n",
        "\n",
        "    def read_calibration_cache(self):\n",
        "        # If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            with open(self.cacheFile, \"rb\") as f:\n",
        "                return f.read()\n",
        "\n",
        "    def write_calibration_cache(self, cache):\n",
        "        with open(self.cacheFile, \"wb\") as f:\n",
        "            f.write(cache)\n",
        "\n",
        "logger = Logger()\n",
        "errorRecorder = ErrorRecorder()\n",
        "\n",
        "builder = trt.Builder(logger)\n",
        "builder.max_batch_size = 1\n",
        "\n",
        "calib = None\n",
        "config = builder.create_builder_config()\n",
        "config.max_workspace_size = 1073741824\n",
        "\n",
        "optimizationProfiler = builder.create_optimization_profile()\n",
        "\n",
        "networkFlags = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "network = builder.create_network(networkFlags)\n",
        "parser = trt.OnnxParser(network, logger)\n",
        "runtime = trt.Runtime(logger)\n",
        "\n",
        "engine = None\n",
        "context = None\n",
        "\n",
        "modelName = None\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "bindings = []\n",
        "stream = None\n",
        "\n",
        "def TrtModelParse(modelPath):\n",
        "    global modelName\n",
        "    global parser\n",
        "    global network\n",
        "\n",
        "    modelName = modelPath.split('.')[0]\n",
        "    parseResult = parser.parse_from_file(modelPath)\n",
        "\n",
        "    if (not parseResult):\n",
        "        for error in range(parser.num_errors):\n",
        "            print(str(parser.get_error(error)))\n",
        "    else:\n",
        "        print(\"Model parsing OK!\")\n",
        "\n",
        "        print(\"Network Description\")\n",
        "\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "\n",
        "        for input in inputs:\n",
        "            print(\"Input '{}' with shape {} and dtype {}\".format(input.name, input.shape, input.dtype))\n",
        "        for output in outputs:\n",
        "            print(\"Output '{}' with shape {} and dtype {}\".format(output.name, output.shape, output.dtype))\n",
        "\n",
        "def TrtModelOptimizeAndSerialize(precision = 'fp32',calibPath=\"\", calibSet=None):\n",
        "    global modelName\n",
        "    global builder\n",
        "    global optimizationProfiler\n",
        "    global calib\n",
        "    global config\n",
        "    global network\n",
        "    global engine\n",
        "    global runtime\n",
        "\n",
        "    modelOptName = modelName + precision + '.trt.engine'\n",
        "\n",
        "    if os.path.exists(modelOptName):\n",
        "        with open(modelOptName, 'rb') as f:\n",
        "            engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    else:\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        input = network.get_input(0)\n",
        "\n",
        "        inputShape = [1, input.shape[1], input.shape[2], input.shape[3]]\n",
        "\n",
        "        optimizationProfiler.set_shape(input.name, inputShape, inputShape, inputShape)\n",
        "\n",
        "        config.add_optimization_profile(optimizationProfiler)\n",
        "\n",
        "        if precision == 'fp16':\n",
        "            if builder.platform_has_fast_fp16:\n",
        "                config.set_flag(trt.BuilderFlag.FP16)\n",
        "        elif precision == 'int8':\n",
        "            if builder.platform_has_fast_int8:\n",
        "                if builder.platform_has_fast_fp16:\n",
        "                    # Also enable fp16, as some layers may be even more efficient in fp16 than int8\n",
        "                    config.set_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "                config.set_flag(trt.BuilderFlag.INT8)\n",
        "\n",
        "                calib = Int8EntropyCalibrator(calibPath, calibSet)\n",
        "                config.int8_calibrator = calib\n",
        "\n",
        "        engine = builder.build_engine(network, config)\n",
        "\n",
        "        serializedEngine = engine.serialize()\n",
        "\n",
        "        engineFD = open(modelOptName, 'wb')\n",
        "        engineFD.write(serializedEngine)\n",
        "        engineFD.close()\n",
        "\n",
        "    print('TRT engine - ', engine.device_memory_size, ' Bytes')\n",
        "    engineDeviceMemory = 0\n",
        "    engineDeviceMemory += engine.device_memory_size\n",
        "    print('TRT engine number of layers - ', engine.num_layers)\n",
        "    print('TRT engine number of bindings - ', engine.num_bindings)\n",
        "    print('TRT engine number of profils - ', engine.num_optimization_profiles)\n",
        "\n",
        "    print('Completion optimized model')\n",
        "\n",
        "def ModelInferSetup():\n",
        "    global context\n",
        "    global engine\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "    global stream\n",
        "\n",
        "    stream = cuda.Stream()\n",
        "\n",
        "    #Over all Tensors inputs & outputs of the TRT engine\n",
        "    #TRT hold first all Tensors inputs and after the Tensor outptus\n",
        "    for binding in engine:\n",
        "        #Get current binded Tensor volume size in elemente units\n",
        "        size = trt.volume(engine.get_binding_shape(binding))\n",
        "        #Get current binded Tensor element type\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host page locked bbuffer\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        # Allocate device bbuffer\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "\n",
        "    # Contexts are used to perform inference.\n",
        "    context = engine.create_execution_context()\n",
        "    context.error_recorder = errorRecorder\n",
        "\n",
        "def Inference(externalnputs = None):\n",
        "\n",
        "    global context\n",
        "    global stream\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "\n",
        "    try:\n",
        "        #verify that TRT context generated successfully\n",
        "        if context is not None:\n",
        "            #Verify that inputs to inference are exist\n",
        "            if externalnputs is not None:\n",
        "                #Copy all Tensors inputs data from user memory to TRT host page locked memory before loading it to the device\n",
        "                if len(externalnputs) == len(inputs):\n",
        "                    for index in range(len(externalnputs)):\n",
        "                        if len(inputs[index].host) == externalnputs[index].size:\n",
        "                            np.copyto(inputs[index].host, externalnputs[index].ravel())\n",
        "                        else:\n",
        "                            print('TRT external input size - ', externalnputs[index].size,\n",
        "                                  ' is not equal to model inputs size - ', len(inputs[index].host))\n",
        "                            return None\n",
        "\n",
        "                    # Transfer input data to the GPU from the host page locked memory.\n",
        "                    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "                    # Run asynchronously inference using the user\\internal stream.\n",
        "                    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "                    # Transfer predictions back from the GPU.\n",
        "                    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "\n",
        "                    stream.synchronize()\n",
        "                    # Build a list of Tensors outputs and return only the host outputs.\n",
        "                    return [out.host for out in outputs]\n",
        "                else:\n",
        "                    print('External inputs list size - ', len(externalnputs), ' is not equal to model inputs list size - ', len(inputs))\n",
        "                    return None\n",
        "            else:\n",
        "                print('External inputs list is None ERROR')\n",
        "                return None\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        print('TRT inference exception ERROR - ', msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzctBFwmVnAo",
        "outputId": "ea02efe6-9364-4b24-ef6d-8e5b3fa67a81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2022.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 24.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako\n",
            "  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "  Downloading pytools-2022.1.12.tar.gz (70 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70 kB 11.5 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2022.1-cp37-cp37m-linux_x86_64.whl size=629484 sha256=c230bbfdd784b6b81411782401d78ea380beb654a3167342ef8c30773968f3f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/53/c9/caa05618e686df51f017d8a9923f38d915ce31df67ab6628e6\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2022.1.12-py2.py3-none-any.whl size=65034 sha256=aa593bb4fcb2aaba87a3bc7c4af29ff86581339132e8550366f6fb9b82babdd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/5e/9e/76d7430e116b7cab0016fbabb26b896daae1946a3f7dea9915\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: platformdirs, pytools, mako, pycuda\n",
            "Successfully installed mako-1.2.1 platformdirs-2.5.2 pycuda-2022.1 pytools-2022.1.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorrt\n",
            "  Downloading tensorrt-0.0.1.tar.gz (714 bytes)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'tensorrt' candidate (version 0.0.1 at https://files.pythonhosted.org/packages/b0/9b/0588892fb68abccfaf26e56fc3c45a3cab3638ad8b488302fdbfce655455/tensorrt-0.0.1.tar.gz#sha256=1fc07bfdd6f6a876c602580435a7cdebe8abbf51413c818cd25f6bca95b43489 (from https://pypi.org/simple/tensorrt/))\n",
            "Reason for being yanked: This software was not built and distributed by NVIDIA\u001b[0m\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-0.0.1-py3-none-any.whl size=1154 sha256=aa8d75fa27af3a187381cee796bfea47d9a6667012d384b83f452f9df0021f3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/78/48/f3d96950a3a858998e81705789cdfd98298a900c264baf2b5f\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-0.0.1\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 244 (MiB)\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init builder kernel library: CPU +0, GPU +68, now: CPU 0, GPU 312 (MiB)\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 312 (MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: DeprecationWarning: Use set_memory_pool_limit instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **onnxUtils:**"
      ],
      "metadata": {
        "id": "2cyDtE0LQrOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# onnxUtils\n",
        "!pip install tf2onnx onnx onnxsim\n",
        "import json\n",
        "import time\n",
        "import tf2onnx\n",
        "import onnx\n",
        "#import onnxsim\n",
        "import os.path\n",
        "\n",
        "\n",
        "# Save model into h5 and ONNX formats\n",
        "def convertKerasToONNX(name, model, overwrite_existing = False):\n",
        "    modelFile = name + '.onnx'\n",
        "    if not os.path.isfile(modelFile) or overwrite_existing:\n",
        "        # Save model with ONNX format\n",
        "        (onnx_model_proto, storage) = tf2onnx.convert.from_keras(model)\n",
        "        with open(os.path.join(modelFile), \"wb\") as f:\n",
        "            f.write(onnx_model_proto.SerializeToString())\n",
        "            f.close()\n",
        "    \n",
        "    return modelFile, onnx_model_proto, storage\n",
        "\n",
        "def ModelOnnxCheck(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    isCheckOk = True\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"Onnx model check report:\")\n",
        "\n",
        "    try:\n",
        "        # Perform basic check on the model input\n",
        "        onnx.checker.check_model(name + '.onnx')\n",
        "        isCheckOk = True\n",
        "    except onnx.checker.ValidationError as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "\n",
        "    if isCheckOk:\n",
        "        print('Model check completed Successfully')\n",
        "    else:\n",
        "        print('ERROR - Model check failure')\n",
        "\n",
        "    print('Model onnx checker, check model - ', msg)\n",
        "\n",
        "    return isCheckOk\n",
        "\n",
        "def RemoveInitializerFromInput(model, modelPath):\n",
        "    modelGraphInputs = model.graph.input\n",
        "    startInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    nameToInput = {}\n",
        "    for input in modelGraphInputs:\n",
        "        nameToInput[input.name] = input\n",
        "\n",
        "    for initializer in model.graph.initializer:\n",
        "        if initializer.name in nameToInput:\n",
        "            modelGraphInputs.remove(nameToInput[initializer.name])\n",
        "\n",
        "    endInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    if startInputsCount != endInputsCount:\n",
        "        print('Model includes several Initializers which considered as inputs to the graph - ', startInputsCount - endInputsCount)\n",
        "        print('All Initializers were removed from graph inputs')\n",
        "        print('Replace the model *.onx file with the updated one')\n",
        "        onnx.save(model, modelPath)\n",
        "\n",
        "def ProcessModelInputs(model, modelPath):\n",
        "    RemoveInitializerFromInput(model, modelPath)\n",
        "    modelGraphInputs = model.graph.input\n",
        "\n",
        "    modelInputsDims = {}\n",
        "    modelDynamicInputsDict = {}\n",
        "    modelInputs = modelGraphInputs\n",
        "    modelInputsNames = []\n",
        "    print(str(modelInputs))\n",
        "\n",
        "    for tensorInput in modelInputs:\n",
        "        isInputDynamic = False\n",
        "        modelDynamicInputShape = []\n",
        "        for dim in tensorInput.type.tensor_type.shape.dim:\n",
        "            if dim.dim_value == 0:\n",
        "                isInputDynamic = True\n",
        "                print('CAUTION!!! - Tensor input name' + ' - ', tensorInput.name, ', dimension - ' , dim.dim_param, ', set its value to 1 for Onnx simplify operation')\n",
        "                modelDynamicInputShape.append(1)\n",
        "            else:\n",
        "                modelDynamicInputShape.append(dim.dim_value)\n",
        "\n",
        "        modelInputsNames.append(tensorInput.name)\n",
        "\n",
        "        if isInputDynamic is True:\n",
        "            modelDynamicInputsDict[tensorInput.name] = modelDynamicInputShape\n",
        "\n",
        "    return modelDynamicInputsDict\n",
        "\n",
        "def ModelSimplify(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    nameSimp = name + 'Simp'\n",
        "    model = None\n",
        "    isSimplifiedOK = True\n",
        "\n",
        "    if os.path.exists(nameSimp + '.onnx'):\n",
        "        print('Model Onnx simplify is already exist, No model check and\\or simplify operations is required')\n",
        "        model = onnx.load(nameSimp + '.onnx')\n",
        "        isSimplifiedOK = True\n",
        "    else:\n",
        "        print(\"===============================================================\")\n",
        "        print(\"Onnx model simplifier report:\")\n",
        "        model = onnx.load(name + '.onnx')\n",
        "\n",
        "        modelDynamicInputsDict = ProcessModelInputs(model, name + '.onnx')\n",
        "\n",
        "        try:\n",
        "            print('Start model onnx simplify...')\n",
        "            # Perform simplification on the model input\n",
        "            model, check = onnxsim.simplify(model,input_shapes=modelDynamicInputsDict,\n",
        "                                                  dynamic_input_shape=(len(modelDynamicInputsDict) > 0))\n",
        "            print('Completion model onnx simplify')\n",
        "            if (check):\n",
        "                isSimplifiedOK = True\n",
        "                print('Onnx simplification success!')\n",
        "                print('Save Onnx simplified model to - ', nameSimp + '.onnx')\n",
        "                onnx.save(model, nameSimp + '.onnx')\n",
        "            else:\n",
        "                isSimplifiedOK = False\n",
        "                print('Onnx simplification failure!')\n",
        "                print('Simplified Onnx model could not be generated and validated')\n",
        "        except BaseException as e:\n",
        "            print('Onnx simplification exception - ', e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU_AzfphVoRZ",
        "outputId": "37b2da15-f1c5-412b-bbdc-d358ff081037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.12.0-py3-none-any.whl (442 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442 kB 29.1 MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.1 MB 63.9 MB/s \n",
            "\u001b[?25hCollecting onnxsim\n",
            "  Downloading onnxsim-0.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0 MB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.15.0)\n",
            "Collecting flatbuffers~=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Collecting rich\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2.10)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (2.6.1)\n",
            "Installing collected packages: commonmark, rich, onnx, flatbuffers, tf2onnx, onnxsim\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "Successfully installed commonmark-0.9.1 flatbuffers-1.12 onnx-1.12.0 onnxsim-0.4.7 rich-12.5.1 tf2onnx-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **onnxUtils:**"
      ],
      "metadata": {
        "id": "Yuy6_TxZQvKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb_helpers\n",
        "!pip install wandb\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "Dataset = namedtuple(\"Dataset\", [\"images\", \"labels\"])\n",
        "dataset_names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "def start_wandb_run(model_name, config):\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    return wandb.init(project=f\"ml-p2\", entity=\"ml-p2\", name=f\"{model_name}-{timestamp}\" , \n",
        "        notes = f\"Training FCNN model @{timestamp}\", config = config)\n",
        "\n",
        "def read_datasets(wandb_run, dataset_tag = \"latest\"):\n",
        "    '''\n",
        "    Read all datasets from W&B.\n",
        "    Usage example: train_set, validation_set, test_set = wbh.read_datasets(run)\n",
        "    '''\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/fashion-mnist:{dataset_tag}', type='dataset')\n",
        "    data_dir = artifact.download()\n",
        "    return [ read_dataset(data_dir, ds_name) for ds_name in dataset_names ]\n",
        "\n",
        "def read_dataset(data_dir, ds_name):\n",
        "    filename = ds_name + \".npz\"\n",
        "    data = np.load(os.path.join(data_dir, filename))\n",
        "    return Dataset(images = data[\"x\"], labels = data[\"y\"])\n",
        "\n",
        "def read_model(wandb_run, model_name, model_tag = \"latest\") -> tf.keras.models.Model:\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/{model_name}:{model_tag}', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    return tf.keras.models.load_model(artifact_dir)\n",
        "\n",
        "def save_model(wandb_run, model, config, model_name, model_description):\n",
        "    model_file = f'./saved-models/{model_name}.tf'\n",
        "    tf.keras.models.save_model(model, model_file)\n",
        "    model_artifact = wandb.Artifact(model_name, type = \"model\", description=model_description, metadata= dict(config))\n",
        "    model_artifact.add_dir(model_file)\n",
        "    wandb_run.log_artifact(model_artifact)\n",
        "\n",
        "def load_best_model(sweep_id):\n",
        "    api = wandb.Api()\n",
        "    sweep = api.sweep(f\"ml-p2/ml-p2/{sweep_id}\")\n",
        "    runs = sorted(sweep.runs,\n",
        "        key=lambda run: run.summary.get(\"val_accuracy\", 0), reverse=True)\n",
        "    val_acc = runs[0].summary.get(\"val_accuracy\", 0)\n",
        "    print(f\"Best run {runs[0].name} with {val_acc} validation accuracy\")\n",
        "\n",
        "    model_file = runs[0].file(\"model-best.h5\").download(replace=True)\n",
        "    model_file.close()\n",
        "\n",
        "if (__name__ == \"__main__\"):\n",
        "    load_best_model(\"6zmewzd0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8_iT8JNVVobY",
        "outputId": "163ff6bc-9e7c-4b87-f2ab-ec01063d9e97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 16.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 64.8 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 60.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 62.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 66.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 66.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156 kB 76.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=827ff3833296a42945d5bd77b6cf998866dc08bf2d3ca33f9181fefe8c34ccbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best run Conv-142858 with 0.9169999957084656 validation accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **trt-inference:**"
      ],
      "metadata": {
        "id": "HynoQt2cQzkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trt-inference\n",
        "#!pip install sklearn -qqq\n",
        "\n",
        "#!pip install TensorRTUtils onnxUtils wandb_helpers\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import tensorrt as trt\n",
        "#from TensorRTUtils import *\n",
        "import onnx\n",
        "import tf2onnx\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "import os\n",
        "#from onnxUtils import convertKerasToONNX\n",
        "#import wandb_helpers as wbh\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "modelName = \"FCNN\"\n",
        "\n",
        "'''\n",
        "Stage 1: Load an existing model\n",
        "===============================\n",
        "In this part we load the model we created in the previous project\n",
        "which is built to infer from FASHION-MNIST images.\n",
        "It is not a sofisticated model, but the idea to use something we\n",
        "know.\n",
        "'''\n",
        "dataset_path = '.\\\\artifacts\\\\fashion-mnist-v2'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    with start_wandb_run(\"FCNN-metrics\", None) as run:\n",
        "        train_set, validation_set, test_set = read_datasets(run)\n",
        "        model = read_model(run, \"FCNN\", \"latest\")\n",
        "else:\n",
        "    test_set = read_dataset('.\\\\artifacts\\\\fashion-mnist-v2', 'test')\n",
        "    model = tf.keras.models.load_model('.\\\\artifacts\\\\FCNN-v3')\n",
        "\n",
        "'''\n",
        "Stage 2: Convert to ONNX\n",
        "========================\n",
        "Convert the model to ONNX and save it to a file. This will allow\n",
        "us to load the model into a tensor-rt engine.\n",
        "'''\n",
        "modelFile, _, _ = convertKerasToONNX(modelName, model, True)\n",
        "\n",
        "'''\n",
        "Stage 3: Create the tensor-rt engine\n",
        "====================================\n",
        "Now that we a model file, we can load it into a \n",
        "tensor rt engine.\n",
        "We use FP 32 precision.\n",
        "'''\n",
        "TrtModelParse(modelFile)\n",
        "TrtModelOptimizeAndSerialize(precision='fp32')\n",
        "ModelInferSetup()\n",
        "\n",
        "'''\n",
        "Stage 4: Inference\n",
        "==================\n",
        "Now the model is ready for inference. The model is executed several\n",
        "times on different images from the test set we've loaded on Stage 1\n",
        "'''\n",
        "inputs = []\n",
        "\n",
        "startTimeCpu = time.time()\n",
        "for i in range(len(test_set)):\n",
        "    img = test_set.images[i]\n",
        "    lbl = test_set.labels[i]\n",
        "    inputs.append(img)\n",
        "    outputsTrt = Inference(externalnputs=inputs)\n",
        "    #print(' topClassIdx - ', np.argmax(outputsTrt[0]))\n",
        "    inputs.clear()\n",
        "    \n",
        "    \n",
        "endTimeCpu = time.time()\n",
        "\n",
        "# total time taken\n",
        "averageTime = (endTimeCpu - startTimeCpu) / 1e-3 / len(test_set)\n",
        "print(f\"TRT Keras inference average time is: {averageTime} milliseconds\")\n",
        "print(f\"TRT Keras inference average FPS is: {1000 / averageTime}\")\n",
        "\n",
        "# Perform the DlewareAnalyzer inference with TRT & ORT\n",
        "\n",
        "#np.testing.assert_allclose(kerasPredictions, onnxPredictions[0], rtol=0, atol=1e-05, err_msg='Keras Vs. Onnx Failure!!!')\n",
        "\n",
        "\n",
        "#y_test = np.argmax(test_set.labels)\n",
        "# predictions = model.predict(test_set.images)\n",
        "# y_test = np.argmax(predictions, axis = 1)\n",
        "# print (classification_report(test_set.labels, y_test))\n",
        "# cm = confusion_matrix(test_set.labels, y_test)\n",
        "\n",
        "# class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "# ax = plt.subplot()\n",
        "# h = sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# # labels, title and ticks\n",
        "# ax.set_xlabel('Predicted labels')\n",
        "# ax.set_ylabel('True labels')\n",
        "# ax.set_title('Confusion Matrix')\n",
        "# ax.xaxis.set_ticklabels(class_names)\n",
        "# ax.yaxis.set_ticklabels(class_names)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6d3e509cae684737bb8ba1eb4d80ae0c",
            "47e58d243edf4d14a23b8422b0a9edb7",
            "8c8ff31dfed343b6939bed3d636cfae4",
            "bd0eb9616be24c4c8964cbd1192676f7",
            "f2721fd5067247e5914da6609a75ea59",
            "b930a4ec93c840d397180784412149ae",
            "c7108c46fe0b4507817e9d9a22ef76c3",
            "60e779838391482ebce79bd6e6fde623"
          ]
        },
        "id": "EEa4aCwOWALB",
        "outputId": "7d5a7267-7f32-4baf-e147-eff1ab6bae05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnirsch\u001b[0m (\u001b[33mml-p2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220817_052226-24gw6vvv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ml-p2/ml-p2/runs/24gw6vvv\" target=\"_blank\">FCNN-metrics-052226</a></strong> to <a href=\"https://wandb.ai/ml-p2/ml-p2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact fashion-mnist:latest, 418.77MB. 3 files... Done. 0:0:4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d3e509cae684737bb8ba1eb4d80ae0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">FCNN-metrics-052226</strong>: <a href=\"https://wandb.ai/ml-p2/ml-p2/runs/24gw6vvv\" target=\"_blank\">https://wandb.ai/ml-p2/ml-p2/runs/24gw6vvv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220817_052226-24gw6vvv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRT - INFO\n",
            "----------------------------------------------------------------\n",
            "TRT - INFO\n",
            "Input filename:   FCNN.onnx\n",
            "TRT - INFO\n",
            "ONNX IR version:  0.0.7\n",
            "TRT - INFO\n",
            "Opset version:    13\n",
            "TRT - INFO\n",
            "Producer name:    tf2onnx\n",
            "TRT - INFO\n",
            "Producer version: 1.12.0 a58786\n",
            "TRT - INFO\n",
            "Domain:           \n",
            "TRT - INFO\n",
            "Model version:    0\n",
            "TRT - INFO\n",
            "Doc string:       \n",
            "TRT - INFO\n",
            "----------------------------------------------------------------\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::GridAnchor_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::NMS_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::Reorg_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::Region_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::Clip_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::LReLU_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::PriorBox_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::Normalize_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::ScatterND version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::RPROI_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::BatchedNMS_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::FlattenConcat_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::CropAndResize version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::CropAndResizeDynamic version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::DetectionLayer_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::EfficientNMS_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::ProposalDynamic version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::Proposal version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::ProposalLayer_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::ResizeNearest_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::Split version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::SpecialSlice_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::CoordConvAC version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::GenerateDetection_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::NMSDynamic_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::PillarScatterPlugin version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
            "TRT - VERBOSE\n",
            "Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Adding network input: input_1 with dtype: float32, dimensions: (-1, 28, 28, 1)\n",
            "TRT - VERBOSE\n",
            "Registering tensor: input_1 for ONNX tensor: input_1\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_3/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_2/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_1/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: const_fold_opt__7\n",
            "TRT - WARNING\n",
            "onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/flatten/Reshape [Reshape]\n",
            "TRT - VERBOSE\n",
            "Searching for input: input_1\n",
            "TRT - VERBOSE\n",
            "Searching for input: const_fold_opt__7\n",
            "TRT - VERBOSE\n",
            "sequential/flatten/Reshape [Reshape] inputs: [input_1 -> (-1, 28, 28, 1)[FLOAT]], [const_fold_opt__7 -> (2)[INT32]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/flatten/Reshape for ONNX node: sequential/flatten/Reshape\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/flatten/Reshape:0 for ONNX tensor: sequential/flatten/Reshape:0\n",
            "TRT - VERBOSE\n",
            "sequential/flatten/Reshape [Reshape] outputs: [sequential/flatten/Reshape:0 -> (-1, 784)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/flatten/Reshape:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul [MatMul] inputs: [sequential/flatten/Reshape:0 -> (-1, 784)[FLOAT]], [sequential/dense/MatMul/ReadVariableOp:0 -> (784, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/MatMul for ONNX node: sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense/MatMul:0 for ONNX tensor: sequential/dense/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul [MatMul] outputs: [sequential/dense/MatMul:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/BiasAdd [Add] inputs: [sequential/dense/MatMul:0 -> (-1, 155)[FLOAT]], [sequential/dense/BiasAdd/ReadVariableOp:0 -> (155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/BiasAdd for ONNX node: sequential/dense/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense/BiasAdd:0 for ONNX tensor: sequential/dense/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/BiasAdd [Add] outputs: [sequential/dense/BiasAdd:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense/Relu [Relu]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/Relu [Relu] inputs: [sequential/dense/BiasAdd:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/Relu for ONNX node: sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense/Relu:0 for ONNX tensor: sequential/dense/Relu:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/Relu [Relu] outputs: [sequential/dense/Relu:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_1/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/Relu:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/MatMul [MatMul] inputs: [sequential/dense/Relu:0 -> (-1, 155)[FLOAT]], [sequential/dense_1/MatMul/ReadVariableOp:0 -> (155, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense_1/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/MatMul for ONNX node: sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_1/MatMul:0 for ONNX tensor: sequential/dense_1/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/MatMul [MatMul] outputs: [sequential/dense_1/MatMul:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_1/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/BiasAdd [Add] inputs: [sequential/dense_1/MatMul:0 -> (-1, 144)[FLOAT]], [sequential/dense_1/BiasAdd/ReadVariableOp:0 -> (144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/BiasAdd for ONNX node: sequential/dense_1/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_1/BiasAdd:0 for ONNX tensor: sequential/dense_1/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/BiasAdd [Add] outputs: [sequential/dense_1/BiasAdd:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_1/Relu [Relu]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/Relu [Relu] inputs: [sequential/dense_1/BiasAdd:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/Relu for ONNX node: sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_1/Relu:0 for ONNX tensor: sequential/dense_1/Relu:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/Relu [Relu] outputs: [sequential/dense_1/Relu:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_2/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/Relu:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/MatMul [MatMul] inputs: [sequential/dense_1/Relu:0 -> (-1, 144)[FLOAT]], [sequential/dense_2/MatMul/ReadVariableOp:0 -> (144, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense_2/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/MatMul for ONNX node: sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_2/MatMul:0 for ONNX tensor: sequential/dense_2/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/MatMul [MatMul] outputs: [sequential/dense_2/MatMul:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_2/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/BiasAdd [Add] inputs: [sequential/dense_2/MatMul:0 -> (-1, 63)[FLOAT]], [sequential/dense_2/BiasAdd/ReadVariableOp:0 -> (63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/BiasAdd for ONNX node: sequential/dense_2/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_2/BiasAdd:0 for ONNX tensor: sequential/dense_2/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/BiasAdd [Add] outputs: [sequential/dense_2/BiasAdd:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_2/Relu [Relu]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/Relu [Relu] inputs: [sequential/dense_2/BiasAdd:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/Relu for ONNX node: sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_2/Relu:0 for ONNX tensor: sequential/dense_2/Relu:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/Relu [Relu] outputs: [sequential/dense_2/Relu:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_3/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/Relu:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/MatMul [MatMul] inputs: [sequential/dense_2/Relu:0 -> (-1, 63)[FLOAT]], [sequential/dense_3/MatMul/ReadVariableOp:0 -> (63, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense_3/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/MatMul for ONNX node: sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_3/MatMul:0 for ONNX tensor: sequential/dense_3/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/MatMul [MatMul] outputs: [sequential/dense_3/MatMul:0 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_3/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/BiasAdd [Add] inputs: [sequential/dense_3/MatMul:0 -> (-1, 10)[FLOAT]], [sequential/dense_3/BiasAdd/ReadVariableOp:0 -> (10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/BiasAdd for ONNX node: sequential/dense_3/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_3/BiasAdd:0 for ONNX tensor: sequential/dense_3/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/BiasAdd [Add] outputs: [sequential/dense_3/BiasAdd:0 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_3/Softmax [Softmax]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/Softmax [Softmax] inputs: [sequential/dense_3/BiasAdd:0 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/Softmax for ONNX node: sequential/dense_3/Softmax\n",
            "TRT - VERBOSE\n",
            "Registering tensor: dense_3_0 for ONNX tensor: dense_3\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/Softmax [Softmax] outputs: [dense_3 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Marking dense_3_0 as output: dense_3\n",
            "Model parsing OK!\n",
            "Network Description\n",
            "Input 'input_1' with shape (-1, 28, 28, 1) and dtype DataType.FLOAT\n",
            "Output 'dense_3' with shape (-1, 10) and dtype DataType.FLOAT\n",
            "TRT - VERBOSE\n",
            "Applying generic optimizations to the graph for inference.\n",
            "TRT - VERBOSE\n",
            "Original: 26 layers\n",
            "TRT - VERBOSE\n",
            "After dead-layer removal: 26 layers\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 4) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_1/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 10) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_2/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 16) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_3/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 22) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleErasure on (Unnamed Layer* 25) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Removing (Unnamed Layer* 25) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "After Myelin optimization: 21 layers\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_1/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_2/MatMul\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:226: DeprecationWarning: Use build_serialized_network instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_2/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_3/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleShuffleFusion on sequential/flatten/Reshape\n",
            "TRT - VERBOSE\n",
            "ShuffleShuffleFusion: Fusing sequential/flatten/Reshape with reshape_before_sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Applying ScaleNodes fusions.\n",
            "TRT - VERBOSE\n",
            "After scale fusion: 16 layers\n",
            "TRT - VERBOSE\n",
            "Running: SqueezePushDownFork on reshape_after_sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "-----------SqueezePushDown kSQUEEZE_FORK case: sequential/dense/MatMul --> reshape_after_sequential/dense/MatMul --> sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "Running: SqueezePushDownFork on reshape_after_sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "-----------SqueezePushDown kSQUEEZE_FORK case: sequential/dense_1/MatMul --> reshape_after_sequential/dense_1/MatMul --> sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "Running: SqueezePushDownFork on reshape_after_sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "-----------SqueezePushDown kSQUEEZE_FORK case: sequential/dense_2/MatMul --> reshape_after_sequential/dense_2/MatMul --> sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleShuffleFusion on squeeze_after_sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "ShuffleShuffleFusion: Fusing squeeze_after_sequential/dense/Relu with reshape_before_sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleShuffleFusion on squeeze_after_sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "ShuffleShuffleFusion: Fusing squeeze_after_sequential/dense_1/Relu with reshape_before_sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleShuffleFusion on squeeze_after_sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "ShuffleShuffleFusion: Fusing squeeze_after_sequential/dense_2/Relu with reshape_before_sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReluFusion on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "ConvReluFusion: Fusing sequential/dense/MatMul with sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleErasure on squeeze_after_sequential/dense/Relu + reshape_before_sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Removing squeeze_after_sequential/dense/Relu + reshape_before_sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReluFusion on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "ConvReluFusion: Fusing sequential/dense_1/MatMul with sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleErasure on squeeze_after_sequential/dense_1/Relu + reshape_before_sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Removing squeeze_after_sequential/dense_1/Relu + reshape_before_sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReluFusion on sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "ConvReluFusion: Fusing sequential/dense_2/MatMul with sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleErasure on squeeze_after_sequential/dense_2/Relu + reshape_before_sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Removing squeeze_after_sequential/dense_2/Relu + reshape_before_sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "After dupe layer removal: 7 layers\n",
            "TRT - VERBOSE\n",
            "After final dead-layer removal: 7 layers\n",
            "TRT - VERBOSE\n",
            "After tensor merging: 7 layers\n",
            "TRT - VERBOSE\n",
            "After vertical fusions: 7 layers\n",
            "TRT - VERBOSE\n",
            "After dupe layer removal: 7 layers\n",
            "TRT - VERBOSE\n",
            "After final dead-layer removal: 7 layers\n",
            "TRT - VERBOSE\n",
            "After tensor merging: 7 layers\n",
            "TRT - VERBOSE\n",
            "After slice removal: 7 layers\n",
            "TRT - VERBOSE\n",
            "After concat removal: 7 layers\n",
            "TRT - VERBOSE\n",
            "Trying to split Reshape and strided tensor\n",
            "TRT - VERBOSE\n",
            "Graph construction and optimization completed in 0.07517 seconds.\n",
            "TRT - VERBOSE\n",
            "Using cublasLt as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +224, now: CPU 0, GPU 963 (MiB)\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +52, now: CPU 0, GPU 1015 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - INFO\n",
            "Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "TRT - VERBOSE\n",
            "Constructing optimization profile number 0 [1/1].\n",
            "TRT - VERBOSE\n",
            "Reserving memory for host IO tensors. Host: 0 bytes\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(784,28,1,1) -> Float(784,1,28,28) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(input_1 -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.00816312\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0259052\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0229447\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.00816312\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(784,28,1,1) -> Float(28,28:32,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(input_1 -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0257255\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0295351\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0240632\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0240632\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(784,1,1,1) -> Float(784,1,784,784) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(reshape_before_sequential/dense/MatMul_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0263409\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0345536\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0270974\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.0263409\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(784,1,784,784) -> Float(784,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(reshape_before_sequential/dense/MatMul_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0252038\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0345035\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0261415\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.0252038\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(25,1:32,1,1) -> Float(784,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(reshape_before_sequential/dense/MatMul_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0318817\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0323433\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0273157\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0273157\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(25,1:32,1,1) -> Float(784,1,784,784) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(reshape_before_sequential/dense/MatMul_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.027872\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0344704\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0243528\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0243528\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(155,1,1,1) -> Float(155,1,155,155) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense/Relu_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0257452\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0353888\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0242385\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0242385\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(155,1,155,155) -> Float(155,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense/Relu_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0252381\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0308082\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0260464\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.0252381\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(144,1,1,1) -> Float(144,1,144,144) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense_1/Relu_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0271696\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0371141\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0253044\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0253044\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(144,1,144,144) -> Float(144,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense_1/Relu_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0281298\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0340363\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0262096\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0262096\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(63,1,1,1) -> Float(63,1,63,63) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense_2/Relu_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.00661333\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0261694\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0271943\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.00661333\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(63,1,63,63) -> Float(63,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense_2/Relu_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0224576\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0314492\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0230699\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.0224576\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning Reformat: Float(10,1,10,10) -> Float(10,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: Optimizer Reformat(sequential/dense_3/MatMul_out_tensor -> <out>) (Reformat)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003e8 Time: 0.0236096\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.0318487\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0239779\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x00000000000003e8 Time: 0.0236096\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,28,1,1) -> Float(784,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0285752\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000001 Time: 0.045496\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0285752\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,1,28,28) -> Float(784,1,784,784) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0223822\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000001 Time: 0.0510659\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0223822\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(28,28:32,1,1) -> Float(25,1:32,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0232348\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000001 Time: 0.0525958\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000000 Time: 0.0232348\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,1,1,1) -> Float(155,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul + sequential/dense/Relu (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul + sequential/dense/Relu (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul + sequential/dense/Relu (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.129589\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000001 Time: 0.115865\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000002 Time: 0.194837\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000004 Time: 2.26705\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000005 Time: 0.291477\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000038 Time: 0.115403\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000039 Time: 0.0183916\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x000000000000003a Time: 0.19464\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x000000000000003c Time: 2.26729\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x000000000000003d Time: 0.293461\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000039 Time: 0.0183916\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul + sequential/dense/Relu (CublasConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0313823\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000001 Time: 0.0291804\n",
            "TRT - VERBOSE\n",
            "Fastest Tactic: 0x0000000000000001 Time: 0.0291804\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul + sequential/dense/Relu (CaskConvolution)\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul + sequential/dense/Relu Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x18597bd4a7d0164d Time: 0.366795\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul + sequential/dense/Relu Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x195431d38ba5af88 Time: 0.370005\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul + sequential/dense/Relu Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x25eed4cfa195d49d Time: 0.233483\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul + sequential/dense/Relu Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x365602d0613d4c36 Time: 0.366379\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul + sequential/dense/Relu Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
            "TRT - VERBOSE\n",
            "Deleting timing cache: 16 entries, served 0 hits since creation.\n",
            "TRT - ERROR\n",
            "1: Unexpected exception \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-587d78f109a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m '''\n\u001b[1;32m     56\u001b[0m \u001b[0mTrtModelParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mTrtModelOptimizeAndSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fp32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mModelInferSetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b98dc0f94ec7>\u001b[0m in \u001b[0;36mTrtModelOptimizeAndSerialize\u001b[0;34m(precision, calibPath, calibSet)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mserializedEngine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mengineFD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelOptName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'serialize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dxO-TM00QgvQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}