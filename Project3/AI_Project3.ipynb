{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea338d4484b3470a8945226542f56536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eee49b48a5b4bee89a6e429a3e24117",
              "IPY_MODEL_3a54fae7177940c993ed31f75bd56f20"
            ],
            "layout": "IPY_MODEL_817ab0ad814a4d0bb3be9a34452759e7"
          }
        },
        "6eee49b48a5b4bee89a6e429a3e24117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eafe14e5fa6e48c78830df261652afb9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_80d3b713db90420d9eea9dc5b7599209",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "3a54fae7177940c993ed31f75bd56f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb66f0b572be436bbef90df5b797e6d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_572823ce19f845f2b708b88da4c37422",
            "value": 1
          }
        },
        "817ab0ad814a4d0bb3be9a34452759e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafe14e5fa6e48c78830df261652afb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d3b713db90420d9eea9dc5b7599209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb66f0b572be436bbef90df5b797e6d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "572823ce19f845f2b708b88da4c37422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-gilyQlVH5n"
      },
      "outputs": [],
      "source": [
        "# Solving a locale problem\n",
        "if False:\n",
        "  !pip install turicreate\n",
        "  import turicreate as tc\n",
        "  import os\n",
        "  try:\n",
        "    del os.environ['LC_ALL']\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing TensorRT with all its dependencies\n",
        "!pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0PiD5Qd5vE",
        "outputId": "8594bd7f-0968-4c75-c888-3b87508a45e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.ngc.nvidia.com, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorrt/nvidia_tensorrt-8.4.1.5-cp37-none-linux_x86_64.whl (774.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 774.4 MB 16 kB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu11/nvidia-cublas-cu11-2022.4.8.tar.gz (16 kB)\n",
            "Collecting nvidia-cudnn-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu11/nvidia-cudnn-cu11-2022.5.19.tar.gz (16 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu11/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu117/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl (333.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 333.1 MB 34 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (0.37.1)\n",
            "Collecting nvidia-cuda-runtime-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu117/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 849 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu116\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu116/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl (719.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 719.3 MB 18 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-cublas-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11\n",
            "  Building wheel for nvidia-cublas-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cublas-cu11: filename=nvidia_cublas_cu11-2022.4.8-py3-none-any.whl size=15624 sha256=df648331b8c5e87523095da5245005eced4eaa7c62cc5d44eeab2cde74c278ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/c3/94/1ffd5bac267cfdc2b222a4ec6915278ef18a028a916b9a5ac3\n",
            "  Building wheel for nvidia-cuda-runtime-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cuda-runtime-cu11: filename=nvidia_cuda_runtime_cu11-2022.4.25-py3-none-any.whl size=15696 sha256=62074361270b30dfe7225a31fd7aea3976be928e25c5102fa5c2c46c8f6d6ac4\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/fe/2b/e553db7867508b2268b14ac194e9ac5b3f51f21316c282c96c\n",
            "  Building wheel for nvidia-cudnn-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cudnn-cu11: filename=nvidia_cudnn_cu11-2022.5.19-py3-none-any.whl size=15617 sha256=6241fb1c721d2fc1c8c05a76367589a5c2f8ec435d3975c534dd1c566e014bfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/32/69/9787704b5f889217708864db5e00812c8c1c349ef89084c59c\n",
            "Successfully built nvidia-cublas-cu11 nvidia-cuda-runtime-cu11 nvidia-cudnn-cu11\n",
            "Installing collected packages: nvidia-cudnn-cu116, nvidia-cuda-runtime-cu117, nvidia-cublas-cu117, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-2022.4.8 nvidia-cublas-cu117-11.10.1.25 nvidia-cuda-runtime-cu11-2022.4.25 nvidia-cuda-runtime-cu117-11.7.60 nvidia-cudnn-cu11-2022.5.19 nvidia-cudnn-cu116-8.4.0.27 nvidia-tensorrt-8.4.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TensorRTUtils:**"
      ],
      "metadata": {
        "id": "t0-7SeTwQje7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorRTUtils\n",
        "!pip install pycuda\n",
        "!pip install tensorrt\n",
        "import tensorrt as trt\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class MatrixIterator:\n",
        "    \"\"\"Class to implement an iterator on a matrix\"\"\"\n",
        "\n",
        "    def __init__(self, matrix, n=0, max=0):\n",
        "        self.matrix = matrix\n",
        "        self.max    = max\n",
        "        self.n      = n\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.n = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n <= self.max:\n",
        "            result = self.matrix[self.n,:,:].squeeze()\n",
        "            self.n += 1\n",
        "            return result\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "    def first(self):\n",
        "        return self.matrix[0,:,:].squeeze()\n",
        "\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "class ErrorRecorder(trt.IErrorRecorder):\n",
        "    def __init__(self):\n",
        "        trt.IErrorRecorder.__init__(self)\n",
        "        self.errorsStack = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.errorsStack.clear()\n",
        "    def get_error_code(self, arg0):\n",
        "        #Error code saved in the error tuple first position\n",
        "        return self.errorsStack[arg0][0]\n",
        "    def get_error_desc(self, arg0):\n",
        "        # Error code saved in the error tuple second position\n",
        "        return self.errorsStack[arg0][1]\n",
        "    def has_overflowed(self):\n",
        "        return False\n",
        "    def num_errors(self):\n",
        "        return len(self.errorsStack)\n",
        "    def report_error(self, arg0, arg1):\n",
        "        error = (arg0, arg1)\n",
        "        #Errors will be saved as a list of tuples, each tuple will be a pair of error code and error description\n",
        "        self.errorsStack.append(error)\n",
        "\n",
        "class Logger(trt.ILogger):\n",
        "    def __init__(self):\n",
        "        trt.ILogger.__init__(self)\n",
        "\n",
        "    def log(self, severity, msg):\n",
        "        if severity == trt.ILogger.INTERNAL_ERROR:\n",
        "            print('INTERNAL_ERROR')\n",
        "        elif severity == trt.ILogger.ERROR:\n",
        "            print('TRT - ERROR')\n",
        "        elif severity == trt.ILogger.WARNING:\n",
        "            print('TRT - WARNING')\n",
        "        elif severity == trt.ILogger.INFO:\n",
        "            print('TRT - INFO')\n",
        "        elif severity == trt.ILogger.VERBOSE:\n",
        "            print('TRT - VERBOSE')\n",
        "        else:\n",
        "            print('TRT - Wrong severity')\n",
        "\n",
        "        print(msg)\n",
        "\n",
        "class Int8EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
        "    def __init__(self, calibrationSetPath = None, calibSet = None):\n",
        "        # Whenever you specify a custom constructor for a TensorRT class,\n",
        "        # you MUST call the constructor of the parent explicitly.\n",
        "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
        "\n",
        "        self.cacheFile = calibrationSetPath + '/CacheFile.bin'\n",
        "        self.batchSize = 1\n",
        "        self.currentIndex = 0\n",
        "        self.deviceInput = None\n",
        "        self.currentIndex = 0\n",
        "        self.PreProcessedSetPath = calibrationSetPath + '/PreProcessedSet'\n",
        "        self.PreProcessedSetCount = calibSet.n\n",
        "        self.PreProcessedSize = calibSet.first().size * 4 #float\n",
        "        self.currentIndex = 0\n",
        "\n",
        "        # Allocate enough memory for a whole batch.\n",
        "        self.deviceInput = cuda.mem_alloc(self.PreProcessedSize)\n",
        "\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            print('Calibration cache file is already exist - ', self.cacheFile)\n",
        "            return\n",
        "\n",
        "        if os.path.isdir(self.PreProcessedSetPath):\n",
        "            filesCnt = os.listdir(self.PreProcessedSetPath)\n",
        "\n",
        "            if len(filesCnt) == self.PreProcessedSetCount:\n",
        "                print('ERROR - Pre processed file set exists!!!')\n",
        "                return\n",
        "\n",
        "        if self.PreProcessedSetCount == 0:\n",
        "            print('ERROR - Calibration set is empty!!!')\n",
        "\n",
        "        print('Start calibration batches build')\n",
        "\n",
        "        for idx in range(self.PreProcessedSetCount):\n",
        "            preProcImg = next(calibSet)\n",
        "            preProcessedFile = open(self.PreProcessedSetPath + '/' + str(idx) + '.bin', mode='wb')\n",
        "            preProcImg.tofile(preProcessedFile)\n",
        "            preProcessedFile.close()\n",
        "\n",
        "        print('End calibration batches build')\n",
        "\n",
        "    def get_algorithm(self):\n",
        "        return trt.CalibrationAlgoType.ENTROPY_CALIBRATION_2\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batchSize\n",
        "\n",
        "    # TensorRT passes along the names of the engine bindings to the get_batch function.\n",
        "    # You don't necessarily have to use them, but they can be useful to understand the order of\n",
        "    # the inputs. The bindings list is expected to have the same ordering as 'names'.\n",
        "    def get_batch(self, names):\n",
        "        if not self.currentIndex < self.PreProcessedSetCount:\n",
        "            return None\n",
        "\n",
        "        print('Get pre processed file index - ', not self.currentIndex)\n",
        "\n",
        "        batchData = np.fromfile(self.PreProcessedSetPath + '/' + str(self.currentIndex) + '.bin', dtype=np.single)\n",
        "        cuda.memcpy_htod(self.deviceInput, batchData)\n",
        "        self.currentIndex += 1\n",
        "\n",
        "        return [self.deviceInput]\n",
        "\n",
        "    def read_calibration_cache(self):\n",
        "        # If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            with open(self.cacheFile, \"rb\") as f:\n",
        "                return f.read()\n",
        "\n",
        "    def write_calibration_cache(self, cache):\n",
        "        with open(self.cacheFile, \"wb\") as f:\n",
        "            f.write(cache)\n",
        "\n",
        "logger = Logger()\n",
        "errorRecorder = ErrorRecorder()\n",
        "\n",
        "builder = trt.Builder(logger)\n",
        "builder.max_batch_size = 1\n",
        "\n",
        "calib = None\n",
        "config = builder.create_builder_config()\n",
        "config.max_workspace_size = 1073741824\n",
        "\n",
        "optimizationProfiler = builder.create_optimization_profile()\n",
        "\n",
        "networkFlags = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "network = builder.create_network(networkFlags)\n",
        "parser = trt.OnnxParser(network, logger)\n",
        "runtime = trt.Runtime(logger)\n",
        "\n",
        "engine = None\n",
        "context = None\n",
        "\n",
        "modelName = None\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "bindings = []\n",
        "stream = None\n",
        "\n",
        "def TrtModelParse(modelPath):\n",
        "    global modelName\n",
        "    global parser\n",
        "    global network\n",
        "\n",
        "    modelName = modelPath.split('.')[0]\n",
        "    parseResult = parser.parse_from_file(modelPath)\n",
        "\n",
        "    if (not parseResult):\n",
        "        for error in range(parser.num_errors):\n",
        "            print(str(parser.get_error(error)))\n",
        "    else:\n",
        "        print(\"Model parsing OK!\")\n",
        "\n",
        "        print(\"Network Description\")\n",
        "\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "\n",
        "        for input in inputs:\n",
        "            print(\"Input '{}' with shape {} and dtype {}\".format(input.name, input.shape, input.dtype))\n",
        "        for output in outputs:\n",
        "            print(\"Output '{}' with shape {} and dtype {}\".format(output.name, output.shape, output.dtype))\n",
        "\n",
        "def TrtModelOptimizeAndSerialize(precision = 'fp32',calibPath=\"\", calibSet=None):\n",
        "    global modelName\n",
        "    global builder\n",
        "    global optimizationProfiler\n",
        "    global calib\n",
        "    global config\n",
        "    global network\n",
        "    global engine\n",
        "    global runtime\n",
        "\n",
        "    modelOptName = modelName + precision + '.trt.engine'\n",
        "\n",
        "    if os.path.exists(modelOptName):\n",
        "        with open(modelOptName, 'rb') as f:\n",
        "            engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    else:\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        input = network.get_input(0)\n",
        "\n",
        "        inputShape = [1, input.shape[1], input.shape[2], input.shape[3]]\n",
        "\n",
        "        optimizationProfiler.set_shape(input.name, inputShape, inputShape, inputShape)\n",
        "\n",
        "        config.add_optimization_profile(optimizationProfiler)\n",
        "\n",
        "        if precision == 'fp16':\n",
        "            if builder.platform_has_fast_fp16:\n",
        "                config.set_flag(trt.BuilderFlag.FP16)\n",
        "        elif precision == 'int8':\n",
        "            if builder.platform_has_fast_int8:\n",
        "                if builder.platform_has_fast_fp16:\n",
        "                    # Also enable fp16, as some layers may be even more efficient in fp16 than int8\n",
        "                    config.set_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "                config.set_flag(trt.BuilderFlag.INT8)\n",
        "\n",
        "                calib = Int8EntropyCalibrator(calibPath, calibSet)\n",
        "                config.int8_calibrator = calib\n",
        "\n",
        "        engine = builder.build_engine(network, config)\n",
        "\n",
        "        serializedEngine = engine.serialize()\n",
        "\n",
        "        engineFD = open(modelOptName, 'wb')\n",
        "        engineFD.write(serializedEngine)\n",
        "        engineFD.close()\n",
        "\n",
        "    print('TRT engine - ', engine.device_memory_size, ' Bytes')\n",
        "    engineDeviceMemory = 0\n",
        "    engineDeviceMemory += engine.device_memory_size\n",
        "    print('TRT engine number of layers - ', engine.num_layers)\n",
        "    print('TRT engine number of bindings - ', engine.num_bindings)\n",
        "    print('TRT engine number of profils - ', engine.num_optimization_profiles)\n",
        "\n",
        "    print('Completion optimized model')\n",
        "\n",
        "def ModelInferSetup():\n",
        "    global context\n",
        "    global engine\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "    global stream\n",
        "\n",
        "    stream = cuda.Stream()\n",
        "\n",
        "    #Over all Tensors inputs & outputs of the TRT engine\n",
        "    #TRT hold first all Tensors inputs and after the Tensor outptus\n",
        "    for binding in engine:\n",
        "        #Get current binded Tensor volume size in elemente units\n",
        "        size = trt.volume(engine.get_binding_shape(binding))\n",
        "        #Get current binded Tensor element type\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host page locked bbuffer\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        # Allocate device bbuffer\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "\n",
        "    # Contexts are used to perform inference.\n",
        "    context = engine.create_execution_context()\n",
        "    context.error_recorder = errorRecorder\n",
        "\n",
        "def Inference(externalnputs = None):\n",
        "\n",
        "    global context\n",
        "    global stream\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "\n",
        "    try:\n",
        "        #verify that TRT context generated successfully\n",
        "        if context is not None:\n",
        "            #Verify that inputs to inference are exist\n",
        "            if externalnputs is not None:\n",
        "                #Copy all Tensors inputs data from user memory to TRT host page locked memory before loading it to the device\n",
        "                if len(externalnputs) == len(inputs):\n",
        "                    for index in range(len(externalnputs)):\n",
        "                        if len(inputs[index].host) == externalnputs[index].size:\n",
        "                            np.copyto(inputs[index].host, externalnputs[index].ravel())\n",
        "                        else:\n",
        "                            print('TRT external input size - ', externalnputs[index].size,\n",
        "                                  ' is not equal to model inputs size - ', len(inputs[index].host))\n",
        "                            return None\n",
        "\n",
        "                    # Transfer input data to the GPU from the host page locked memory.\n",
        "                    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "                    # Run asynchronously inference using the user\\internal stream.\n",
        "                    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "                    # Transfer predictions back from the GPU.\n",
        "                    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "\n",
        "                    stream.synchronize()\n",
        "                    # Build a list of Tensors outputs and return only the host outputs.\n",
        "                    return [out.host for out in outputs]\n",
        "                else:\n",
        "                    print('External inputs list size - ', len(externalnputs), ' is not equal to model inputs list size - ', len(inputs))\n",
        "                    return None\n",
        "            else:\n",
        "                print('External inputs list is None ERROR')\n",
        "                return None\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        print('TRT inference exception ERROR - ', msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "kzctBFwmVnAo",
        "outputId": "3801f7ad-ad1e-4f2e-fa7b-d245fe9274d0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-befee7a4d950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TensorRTUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pycuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoinit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   result = _run_command(\n\u001b[0;32m--> 437\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     raise NotImplementedError(\n\u001b[0;32m--> 164\u001b[0;31m         'A UTF-8 locale is required. Got {}'.format(locale_encoding))\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_pty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **onnxUtils:**"
      ],
      "metadata": {
        "id": "2cyDtE0LQrOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# onnxUtils\n",
        "!pip install tf2onnx onnx onnxsim\n",
        "import json\n",
        "import time\n",
        "import tf2onnx\n",
        "import onnx\n",
        "#import onnxsim\n",
        "import os.path\n",
        "\n",
        "\n",
        "# Save model into h5 and ONNX formats\n",
        "def convertKerasToONNX(name, model, overwrite_existing = False):\n",
        "    modelFile = name + '.onnx'\n",
        "    if not os.path.isfile(modelFile) or overwrite_existing:\n",
        "        # Save model with ONNX format\n",
        "        (onnx_model_proto, storage) = tf2onnx.convert.from_keras(model)\n",
        "        with open(os.path.join(modelFile), \"wb\") as f:\n",
        "            f.write(onnx_model_proto.SerializeToString())\n",
        "            f.close()\n",
        "    \n",
        "    return modelFile, onnx_model_proto, storage\n",
        "\n",
        "def ModelOnnxCheck(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    isCheckOk = True\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"Onnx model check report:\")\n",
        "\n",
        "    try:\n",
        "        # Perform basic check on the model input\n",
        "        onnx.checker.check_model(name + '.onnx')\n",
        "        isCheckOk = True\n",
        "    except onnx.checker.ValidationError as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "\n",
        "    if isCheckOk:\n",
        "        print('Model check completed Successfully')\n",
        "    else:\n",
        "        print('ERROR - Model check failure')\n",
        "\n",
        "    print('Model onnx checker, check model - ', msg)\n",
        "\n",
        "    return isCheckOk\n",
        "\n",
        "def RemoveInitializerFromInput(model, modelPath):\n",
        "    modelGraphInputs = model.graph.input\n",
        "    startInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    nameToInput = {}\n",
        "    for input in modelGraphInputs:\n",
        "        nameToInput[input.name] = input\n",
        "\n",
        "    for initializer in model.graph.initializer:\n",
        "        if initializer.name in nameToInput:\n",
        "            modelGraphInputs.remove(nameToInput[initializer.name])\n",
        "\n",
        "    endInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    if startInputsCount != endInputsCount:\n",
        "        print('Model includes several Initializers which considered as inputs to the graph - ', startInputsCount - endInputsCount)\n",
        "        print('All Initializers were removed from graph inputs')\n",
        "        print('Replace the model *.onx file with the updated one')\n",
        "        onnx.save(model, modelPath)\n",
        "\n",
        "def ProcessModelInputs(model, modelPath):\n",
        "    RemoveInitializerFromInput(model, modelPath)\n",
        "    modelGraphInputs = model.graph.input\n",
        "\n",
        "    modelInputsDims = {}\n",
        "    modelDynamicInputsDict = {}\n",
        "    modelInputs = modelGraphInputs\n",
        "    modelInputsNames = []\n",
        "    print(str(modelInputs))\n",
        "\n",
        "    for tensorInput in modelInputs:\n",
        "        isInputDynamic = False\n",
        "        modelDynamicInputShape = []\n",
        "        for dim in tensorInput.type.tensor_type.shape.dim:\n",
        "            if dim.dim_value == 0:\n",
        "                isInputDynamic = True\n",
        "                print('CAUTION!!! - Tensor input name' + ' - ', tensorInput.name, ', dimension - ' , dim.dim_param, ', set its value to 1 for Onnx simplify operation')\n",
        "                modelDynamicInputShape.append(1)\n",
        "            else:\n",
        "                modelDynamicInputShape.append(dim.dim_value)\n",
        "\n",
        "        modelInputsNames.append(tensorInput.name)\n",
        "\n",
        "        if isInputDynamic is True:\n",
        "            modelDynamicInputsDict[tensorInput.name] = modelDynamicInputShape\n",
        "\n",
        "    return modelDynamicInputsDict\n",
        "\n",
        "def ModelSimplify(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    nameSimp = name + 'Simp'\n",
        "    model = None\n",
        "    isSimplifiedOK = True\n",
        "\n",
        "    if os.path.exists(nameSimp + '.onnx'):\n",
        "        print('Model Onnx simplify is already exist, No model check and\\or simplify operations is required')\n",
        "        model = onnx.load(nameSimp + '.onnx')\n",
        "        isSimplifiedOK = True\n",
        "    else:\n",
        "        print(\"===============================================================\")\n",
        "        print(\"Onnx model simplifier report:\")\n",
        "        model = onnx.load(name + '.onnx')\n",
        "\n",
        "        modelDynamicInputsDict = ProcessModelInputs(model, name + '.onnx')\n",
        "\n",
        "        try:\n",
        "            print('Start model onnx simplify...')\n",
        "            # Perform simplification on the model input\n",
        "            model, check = onnxsim.simplify(model,input_shapes=modelDynamicInputsDict,\n",
        "                                                  dynamic_input_shape=(len(modelDynamicInputsDict) > 0))\n",
        "            print('Completion model onnx simplify')\n",
        "            if (check):\n",
        "                isSimplifiedOK = True\n",
        "                print('Onnx simplification success!')\n",
        "                print('Save Onnx simplified model to - ', nameSimp + '.onnx')\n",
        "                onnx.save(model, nameSimp + '.onnx')\n",
        "            else:\n",
        "                isSimplifiedOK = False\n",
        "                print('Onnx simplification failure!')\n",
        "                print('Simplified Onnx model could not be generated and validated')\n",
        "        except BaseException as e:\n",
        "            print('Onnx simplification exception - ', e)"
      ],
      "metadata": {
        "id": "yU_AzfphVoRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4faead11-a7c4-472c-eabd-78737daaabcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.12.0-py3-none-any.whl (442 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.1 MB 66.3 MB/s \n",
            "\u001b[?25hCollecting onnxsim\n",
            "  Downloading onnxsim-0.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.12)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Collecting rich\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235 kB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2.10)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (2.6.1)\n",
            "Installing collected packages: commonmark, rich, onnx, tf2onnx, onnxsim\n",
            "Successfully installed commonmark-0.9.1 onnx-1.12.0 onnxsim-0.4.7 rich-12.5.1 tf2onnx-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **wandb_helpers:**"
      ],
      "metadata": {
        "id": "Yuy6_TxZQvKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb_helpers\n",
        "!pip install wandb\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "Dataset = namedtuple(\"Dataset\", [\"images\", \"labels\"])\n",
        "dataset_names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "def start_wandb_run(model_name, config):\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    return wandb.init(project=f\"ml-p2\", entity=\"ml-p2\", name=f\"{model_name}-{timestamp}\" , \n",
        "        notes = f\"Training FCNN model @{timestamp}\", config = config)\n",
        "\n",
        "def read_datasets(wandb_run, dataset_tag = \"latest\"):\n",
        "    '''\n",
        "    Read all datasets from W&B.\n",
        "    Usage example: train_set, validation_set, test_set = wbh.read_datasets(run)\n",
        "    '''\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/fashion-mnist:{dataset_tag}', type='dataset')\n",
        "    data_dir = artifact.download()\n",
        "    return [ read_dataset(data_dir, ds_name) for ds_name in dataset_names ]\n",
        "\n",
        "def read_dataset(data_dir, ds_name):\n",
        "    filename = ds_name + \".npz\"\n",
        "    data = np.load(os.path.join(data_dir, filename))\n",
        "    return Dataset(images = data[\"x\"], labels = data[\"y\"])\n",
        "\n",
        "def read_model(wandb_run, model_name, model_tag = \"latest\") -> tf.keras.models.Model:\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/{model_name}:{model_tag}', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    return tf.keras.models.load_model(artifact_dir)\n",
        "\n",
        "def save_model(wandb_run, model, config, model_name, model_description):\n",
        "    model_file = f'./saved-models/{model_name}.tf'\n",
        "    tf.keras.models.save_model(model, model_file)\n",
        "    model_artifact = wandb.Artifact(model_name, type = \"model\", description=model_description, metadata= dict(config))\n",
        "    model_artifact.add_dir(model_file)\n",
        "    wandb_run.log_artifact(model_artifact)\n",
        "\n",
        "def load_best_model(sweep_id):\n",
        "    api = wandb.Api()\n",
        "    sweep = api.sweep(f\"ml-p2/ml-p2/{sweep_id}\")\n",
        "    runs = sorted(sweep.runs,\n",
        "        key=lambda run: run.summary.get(\"val_accuracy\", 0), reverse=True)\n",
        "    val_acc = runs[0].summary.get(\"val_accuracy\", 0)\n",
        "    print(f\"Best run {runs[0].name} with {val_acc} validation accuracy\")\n",
        "\n",
        "    model_file = runs[0].file(\"model-best.h5\").download(replace=True)\n",
        "    model_file.close()\n",
        "\n",
        "#if (__name__ == \"__main__\"):\n",
        "#    load_best_model(\"6zmewzd0\")"
      ],
      "metadata": {
        "id": "8_iT8JNVVobY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123c1918-e6c2-43ea-b49c-555ec0bf7cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 76.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 77.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 79.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 78.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156 kB 63.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f84c1969cff294ea97ef60d5f327e67d98be79e78182ed0bc62d7fe95a5ee374\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **trt-inference:**"
      ],
      "metadata": {
        "id": "HynoQt2cQzkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trt-inference\n",
        "#!pip install sklearn -qqq\n",
        "\n",
        "#from TensorRTUtils import *\n",
        "#from onnxUtils import convertKerasToONNX\n",
        "#import wandb_helpers as wbh\n",
        "\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import tensorrt as trt\n",
        "import onnx\n",
        "import tf2onnx\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "modelName = \"FCNN\"\n",
        "\n",
        "'''\n",
        "Stage 1: Load an existing model\n",
        "===============================\n",
        "In this part we load the model we created in the previous project\n",
        "which is built to infer from FASHION-MNIST images.\n",
        "It is not a sofisticated model, but the idea to use something we\n",
        "know.\n",
        "'''\n",
        "dataset_path = '.\\\\artifacts\\\\fashion-mnist-v2'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    with start_wandb_run(\"FCNN-metrics\", None) as run:\n",
        "        train_set, validation_set, test_set = read_datasets(run)\n",
        "        model = read_model(run, \"FCNN\", \"latest\")\n",
        "else:\n",
        "    test_set = read_dataset('.\\\\artifacts\\\\fashion-mnist-v2', 'test')\n",
        "    model = tf.keras.models.load_model('.\\\\artifacts\\\\FCNN-v3')\n",
        "\n",
        "'''\n",
        "Stage 2: Convert to ONNX\n",
        "========================\n",
        "Convert the model to ONNX and save it to a file. This will allow\n",
        "us to load the model into a tensor-rt engine.\n",
        "'''\n",
        "modelFile, _, _ = convertKerasToONNX(modelName, model, True)\n",
        "\n",
        "'''\n",
        "Stage 3: Create the tensor-rt engine\n",
        "====================================\n",
        "Now that we a model file, we can load it into a \n",
        "tensor rt engine.\n",
        "We use FP 32 precision.\n",
        "'''\n",
        "TrtModelParse(modelFile)\n",
        "#TrtModelOptimizeAndSerialize(precision='fp32')\n",
        "#TrtModelOptimizeAndSerialize(precision='fp16')\n",
        "calibSet=MatrixIterator(validation_set.images)\n",
        "TrtModelOptimizeAndSerialize(precision='int8', calibSet=calibSet)\n",
        "ModelInferSetup()\n",
        "\n",
        "'''\n",
        "Stage 4: Inference\n",
        "==================\n",
        "Now the model is ready for inference. The model is executed several\n",
        "times on different images from the test set we've loaded on Stage 1\n",
        "'''\n",
        "inputs = []\n",
        "\n",
        "startTimeCpu = time.time()\n",
        "for i in range(len(test_set)):\n",
        "    img = test_set.images[i]\n",
        "    lbl = test_set.labels[i]\n",
        "    inputs.append(img)\n",
        "    outputsTrt = Inference(externalnputs=inputs)\n",
        "    #print(' topClassIdx - ', np.argmax(outputsTrt[0]))\n",
        "    inputs.clear()\n",
        "    \n",
        "    \n",
        "endTimeCpu = time.time()\n",
        "\n",
        "# total time taken\n",
        "averageTime = (endTimeCpu - startTimeCpu) / 1e-3 / len(test_set)\n",
        "print(f\"TRT Keras inference average time is: {averageTime} milliseconds\")\n",
        "print(f\"TRT Keras inference average FPS is: {1000 / averageTime}\")\n",
        "\n",
        "# Perform the DlewareAnalyzer inference with TRT & ORT\n",
        "\n",
        "#np.testing.assert_allclose(kerasPredictions, onnxPredictions[0], rtol=0, atol=1e-05, err_msg='Keras Vs. Onnx Failure!!!')\n",
        "\n",
        "\n",
        "#y_test = np.argmax(test_set.labels)\n",
        "# predictions = model.predict(test_set.images)\n",
        "# y_test = np.argmax(predictions, axis = 1)\n",
        "# print (classification_report(test_set.labels, y_test))\n",
        "# cm = confusion_matrix(test_set.labels, y_test)\n",
        "\n",
        "# class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "# ax = plt.subplot()\n",
        "# h = sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# # labels, title and ticks\n",
        "# ax.set_xlabel('Predicted labels')\n",
        "# ax.set_ylabel('True labels')\n",
        "# ax.set_title('Confusion Matrix')\n",
        "# ax.xaxis.set_ticklabels(class_names)\n",
        "# ax.yaxis.set_ticklabels(class_names)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "EEa4aCwOWALB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea338d4484b3470a8945226542f56536",
            "6eee49b48a5b4bee89a6e429a3e24117",
            "3a54fae7177940c993ed31f75bd56f20",
            "817ab0ad814a4d0bb3be9a34452759e7",
            "eafe14e5fa6e48c78830df261652afb9",
            "80d3b713db90420d9eea9dc5b7599209",
            "bb66f0b572be436bbef90df5b797e6d0",
            "572823ce19f845f2b708b88da4c37422"
          ]
        },
        "outputId": "b13cefac-1f43-4022-e616-4b5162469668"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220818_065153-2ggyjqfx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ml-p2/ml-p2/runs/2ggyjqfx\" target=\"_blank\">FCNN-metrics-065153</a></strong> to <a href=\"https://wandb.ai/ml-p2/ml-p2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact fashion-mnist:latest, 418.77MB. 3 files... Done. 0:0:0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea338d4484b3470a8945226542f56536"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">FCNN-metrics-065153</strong>: <a href=\"https://wandb.ai/ml-p2/ml-p2/runs/2ggyjqfx\" target=\"_blank\">https://wandb.ai/ml-p2/ml-p2/runs/2ggyjqfx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220818_065153-2ggyjqfx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRT - INFO\n",
            "----------------------------------------------------------------\n",
            "TRT - INFO\n",
            "Input filename:   FCNN.onnx\n",
            "TRT - INFO\n",
            "ONNX IR version:  0.0.7\n",
            "TRT - INFO\n",
            "Opset version:    13\n",
            "TRT - INFO\n",
            "Producer name:    tf2onnx\n",
            "TRT - INFO\n",
            "Producer version: 1.12.0 a58786\n",
            "TRT - INFO\n",
            "Domain:           \n",
            "TRT - INFO\n",
            "Model version:    0\n",
            "TRT - INFO\n",
            "Doc string:       \n",
            "TRT - INFO\n",
            "----------------------------------------------------------------\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::GridAnchor_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::NMS_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::Reorg_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::Region_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::Clip_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::LReLU_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::PriorBox_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::Normalize_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::ScatterND version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::RPROI_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::CropAndResize version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::ProposalDynamic version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::Proposal version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::Split version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::CoordConvAC version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::PillarScatterPlugin version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
            "TRT - VERBOSE\n",
            "Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
            "TRT - VERBOSE\n",
            "Adding network input: input_1 with dtype: float32, dimensions: (-1, 28, 28, 1)\n",
            "TRT - VERBOSE\n",
            "Registering tensor: input_1 for ONNX tensor: input_1\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_3/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_2/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_1/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Importing initializer: const_fold_opt__56\n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/flatten/Reshape [Reshape]\n",
            "TRT - VERBOSE\n",
            "Searching for input: input_1\n",
            "TRT - VERBOSE\n",
            "Searching for input: const_fold_opt__56\n",
            "TRT - VERBOSE\n",
            "sequential/flatten/Reshape [Reshape] inputs: [input_1 -> (-1, 28, 28, 1)[FLOAT]], [const_fold_opt__56 -> (2)[INT32]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/flatten/Reshape for ONNX node: sequential/flatten/Reshape\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/flatten/Reshape:0 for ONNX tensor: sequential/flatten/Reshape:0\n",
            "TRT - VERBOSE\n",
            "sequential/flatten/Reshape [Reshape] outputs: [sequential/flatten/Reshape:0 -> (-1, 784)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/flatten/Reshape:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul [MatMul] inputs: [sequential/flatten/Reshape:0 -> (-1, 784)[FLOAT]], [sequential/dense/MatMul/ReadVariableOp:0 -> (784, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/MatMul for ONNX node: sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense/MatMul:0 for ONNX tensor: sequential/dense/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/MatMul [MatMul] outputs: [sequential/dense/MatMul:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/BiasAdd [Add] inputs: [sequential/dense/MatMul:0 -> (-1, 155)[FLOAT]], [sequential/dense/BiasAdd/ReadVariableOp:0 -> (155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/BiasAdd for ONNX node: sequential/dense/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense/BiasAdd:0 for ONNX tensor: sequential/dense/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/BiasAdd [Add] outputs: [sequential/dense/BiasAdd:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense/Relu [Relu]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/Relu [Relu] inputs: [sequential/dense/BiasAdd:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense/Relu for ONNX node: sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense/Relu:0 for ONNX tensor: sequential/dense/Relu:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense/Relu [Relu] outputs: [sequential/dense/Relu:0 -> (-1, 155)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_1/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense/Relu:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/MatMul [MatMul] inputs: [sequential/dense/Relu:0 -> (-1, 155)[FLOAT]], [sequential/dense_1/MatMul/ReadVariableOp:0 -> (155, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense_1/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/MatMul for ONNX node: sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_1/MatMul:0 for ONNX tensor: sequential/dense_1/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/MatMul [MatMul] outputs: [sequential/dense_1/MatMul:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_1/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/BiasAdd [Add] inputs: [sequential/dense_1/MatMul:0 -> (-1, 144)[FLOAT]], [sequential/dense_1/BiasAdd/ReadVariableOp:0 -> (144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/BiasAdd for ONNX node: sequential/dense_1/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_1/BiasAdd:0 for ONNX tensor: sequential/dense_1/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/BiasAdd [Add] outputs: [sequential/dense_1/BiasAdd:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_1/Relu [Relu]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/Relu [Relu] inputs: [sequential/dense_1/BiasAdd:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_1/Relu for ONNX node: sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_1/Relu:0 for ONNX tensor: sequential/dense_1/Relu:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_1/Relu [Relu] outputs: [sequential/dense_1/Relu:0 -> (-1, 144)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_2/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_1/Relu:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/MatMul [MatMul] inputs: [sequential/dense_1/Relu:0 -> (-1, 144)[FLOAT]], [sequential/dense_2/MatMul/ReadVariableOp:0 -> (144, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense_2/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/MatMul for ONNX node: sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_2/MatMul:0 for ONNX tensor: sequential/dense_2/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/MatMul [MatMul] outputs: [sequential/dense_2/MatMul:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_2/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/BiasAdd [Add] inputs: [sequential/dense_2/MatMul:0 -> (-1, 63)[FLOAT]], [sequential/dense_2/BiasAdd/ReadVariableOp:0 -> (63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/BiasAdd for ONNX node: sequential/dense_2/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_2/BiasAdd:0 for ONNX tensor: sequential/dense_2/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/BiasAdd [Add] outputs: [sequential/dense_2/BiasAdd:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_2/Relu [Relu]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/Relu [Relu] inputs: [sequential/dense_2/BiasAdd:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_2/Relu for ONNX node: sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_2/Relu:0 for ONNX tensor: sequential/dense_2/Relu:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_2/Relu [Relu] outputs: [sequential/dense_2/Relu:0 -> (-1, 63)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_3/MatMul [MatMul]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_2/Relu:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/MatMul [MatMul] inputs: [sequential/dense_2/Relu:0 -> (-1, 63)[FLOAT]], [sequential/dense_3/MatMul/ReadVariableOp:0 -> (63, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/MatMul/ReadVariableOp:0 for ONNX node: sequential/dense_3/MatMul/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/MatMul for ONNX node: sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_3/MatMul:0 for ONNX tensor: sequential/dense_3/MatMul:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/MatMul [MatMul] outputs: [sequential/dense_3/MatMul:0 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_3/BiasAdd [Add]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/MatMul:0\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/BiasAdd [Add] inputs: [sequential/dense_3/MatMul:0 -> (-1, 10)[FLOAT]], [sequential/dense_3/BiasAdd/ReadVariableOp:0 -> (10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/BiasAdd/ReadVariableOp:0 for ONNX node: sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/BiasAdd for ONNX node: sequential/dense_3/BiasAdd\n",
            "TRT - VERBOSE\n",
            "Registering tensor: sequential/dense_3/BiasAdd:0 for ONNX tensor: sequential/dense_3/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/BiasAdd [Add] outputs: [sequential/dense_3/BiasAdd:0 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Parsing node: sequential/dense_3/Softmax [Softmax]\n",
            "TRT - VERBOSE\n",
            "Searching for input: sequential/dense_3/BiasAdd:0\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/Softmax [Softmax] inputs: [sequential/dense_3/BiasAdd:0 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Registering layer: sequential/dense_3/Softmax for ONNX node: sequential/dense_3/Softmax\n",
            "TRT - VERBOSE\n",
            "Registering tensor: dense_3_0 for ONNX tensor: dense_3\n",
            "TRT - VERBOSE\n",
            "sequential/dense_3/Softmax [Softmax] outputs: [dense_3 -> (-1, 10)[FLOAT]], \n",
            "TRT - VERBOSE\n",
            "Marking dense_3_0 as output: dense_3\n",
            "Model parsing OK!\n",
            "Network Description\n",
            "Input 'input_1' with shape (-1, 28, 28, 1) and dtype DataType.FLOAT\n",
            "Output 'dense_3' with shape (-1, 10) and dtype DataType.FLOAT\n",
            "ERROR - Calibration set is empty!!!\n",
            "Start calibration batches build\n",
            "End calibration batches build\n",
            "TRT - VERBOSE\n",
            "Original: 26 layers\n",
            "TRT - VERBOSE\n",
            "After dead-layer removal: 26 layers\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 4) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_1/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_1/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 10) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_2/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_2/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 16) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ConstShuffleFusion on sequential/dense_3/BiasAdd/ReadVariableOp:0\n",
            "TRT - VERBOSE\n",
            "ConstShuffleFusion: Fusing sequential/dense_3/BiasAdd/ReadVariableOp:0 with (Unnamed Layer* 22) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleErasure on (Unnamed Layer* 25) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "Removing (Unnamed Layer* 25) [Shuffle]\n",
            "TRT - VERBOSE\n",
            "After Myelin optimization: 21 layers\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_1/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_2/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: MatMulToConvTransform on sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Convert layer type of sequential/dense_3/MatMul from MATRIX_MULTIPLY to CONVOLUTION\n",
            "TRT - VERBOSE\n",
            "Running: ShuffleShuffleFusion on sequential/flatten/Reshape\n",
            "TRT - VERBOSE\n",
            "ShuffleShuffleFusion: Fusing sequential/flatten/Reshape with reshape_before_sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_1/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_2/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ConvReshapeBiasAddFusion on sequential/dense_3/MatMul\n",
            "TRT - VERBOSE\n",
            "Running: ActivationToPointwiseConversion on sequential/dense/Relu\n",
            "TRT - VERBOSE\n",
            "Swap the layer type of sequential/dense/Relu from ACTIVATION to POINTWISE\n",
            "TRT - VERBOSE\n",
            "Running: ActivationToPointwiseConversion on sequential/dense_1/Relu\n",
            "TRT - VERBOSE\n",
            "Swap the layer type of sequential/dense_1/Relu from ACTIVATION to POINTWISE\n",
            "TRT - VERBOSE\n",
            "Running: ActivationToPointwiseConversion on sequential/dense_2/Relu\n",
            "TRT - VERBOSE\n",
            "Swap the layer type of sequential/dense_2/Relu from ACTIVATION to POINTWISE\n",
            "TRT - VERBOSE\n",
            "After final dead-layer removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After vertical fusions: 16 layers\n",
            "TRT - VERBOSE\n",
            "After final dead-layer removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After slice removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After concat removal: 16 layers\n",
            "TRT - VERBOSE\n",
            "After tensor merging: 16 layers\n",
            "TRT - VERBOSE\n",
            "Trying to split Reshape and strided tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:250: DeprecationWarning: Use build_serialized_network instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRT - VERBOSE\n",
            "Using cublasLt as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +222, now: CPU 0, GPU 1059 (MiB)\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +52, now: CPU 0, GPU 1111 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - INFO\n",
            "Timing cache disabled. Turning it on will improve builder speed.\n",
            "TRT - WARNING\n",
            "Calibration Profile is not defined. Running calibration with Profile 0\n",
            "TRT - VERBOSE\n",
            "Constructing calibration profile.\n",
            "TRT - VERBOSE\n",
            "Reserving memory for host IO tensors. Host: 0 bytes\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing reformatting costs\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,28,1,1) -> Float(784,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.03888\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(784,1,1,1) -> Float(155,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 2403.34\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1,1,1) -> Float(155,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.031968\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1) -> Float(155,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: PWN(sequential/dense/Relu) (PointWiseV2)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.02128\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1) -> Float(155,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_before_sequential/dense_1/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.035488\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(155,1,1,1) -> Float(144,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_1/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_1/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_1/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.073728\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1,1,1) -> Float(144,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense_1/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.029984\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1) -> Float(144,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: PWN(sequential/dense_1/Relu) (PointWiseV2)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.016032\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1) -> Float(144,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_before_sequential/dense_2/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.03408\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(144,1,1,1) -> Float(63,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_2/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_2/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_2/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.066528\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1,1,1) -> Float(63,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense_2/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.0216\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1) -> Float(63,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: PWN(sequential/dense_2/Relu) (PointWiseV2)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.015776\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1) -> Float(63,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_before_sequential/dense_3/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.02048\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(63,1,1,1) -> Float(10,1,1,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/MatMul (CudaDepthwiseConvolution)\n",
            "TRT - VERBOSE\n",
            "CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/MatMul (FusedConvActConvolution)\n",
            "TRT - VERBOSE\n",
            "FusedConvActConvolution has no valid tactics for this config, skipping\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/MatMul (CudnnConvolution)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.057344\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(10,1,1,1) -> Float(10,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: reshape_after_sequential/dense_3/MatMul (Shuffle)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 Time: 0.02048\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x0000000000000000 A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
            "TRT - VERBOSE\n",
            "=============== Computing costs for \n",
            "TRT - VERBOSE\n",
            "*************** Autotuning format combination: Float(10,1) -> Float(10,1) ***************\n",
            "TRT - VERBOSE\n",
            "--------------- Timing Runner: sequential/dense_3/Softmax (CudaSoftMax)\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea Time: 0.036864\n",
            "TRT - VERBOSE\n",
            "Tactic: 0x00000000000003ea A valid tactic is found. Rest of the tactics are skipped.\n",
            "TRT - VERBOSE\n",
            ">>>>>>>>>>>>>>> Chose Runner Type: CudaSoftMax Tactic: 0x00000000000003ea\n",
            "TRT - VERBOSE\n",
            "Formats and tactics selection completed in 4.19896 seconds.\n",
            "TRT - VERBOSE\n",
            "After reformat layers: 16 layers\n",
            "TRT - VERBOSE\n",
            "Pre-optimized block assignment.\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 3584\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 512\n",
            "TRT - VERBOSE\n",
            "Block size 1073741824\n",
            "TRT - VERBOSE\n",
            "Total Activation Memory: 1073756672\n",
            "TRT - INFO\n",
            "Detected 1 inputs and 1 output network tensors.\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/flatten/Reshape + reshape_before_sequential/dense/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: PWN(sequential/dense/Relu) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_before_sequential/dense_1/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_1/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense_1/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: PWN(sequential/dense_1/Relu) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_before_sequential/dense_2/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_2/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense_2/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: PWN(sequential/dense_2/Relu) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_before_sequential/dense_3/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_3/MatMul Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: reshape_after_sequential/dense_3/MatMul Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - VERBOSE\n",
            "Layer: sequential/dense_3/Softmax Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
            "TRT - INFO\n",
            "Total Host Persistent Memory: 896\n",
            "TRT - INFO\n",
            "Total Device Persistent Memory: 0\n",
            "TRT - INFO\n",
            "Total Scratch Memory: 0\n",
            "TRT - INFO\n",
            "[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB\n",
            "TRT - INFO\n",
            "[BlockAssignment] Algorithm ShiftNTopDown took 0.069268ms to assign 2 blocks to 15 nodes requiring 4608 bytes.\n",
            "TRT - VERBOSE\n",
            "Optimized block assignment.\n",
            "TRT - VERBOSE\n",
            "Block size 3584\n",
            "TRT - VERBOSE\n",
            "Block size 1024\n",
            "TRT - INFO\n",
            "Total Activation Memory: 4608\n",
            "TRT - VERBOSE\n",
            "Disabling unused tactic source: CUBLAS, CUBLAS_LT\n",
            "TRT - VERBOSE\n",
            "Disabling unused tactic source: EDGE_MASK_CONVOLUTIONS\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 0, GPU 1339 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - VERBOSE\n",
            "Engine generation completed in 5.14619 seconds.\n",
            "TRT - VERBOSE\n",
            "Using cuDNN as a tactic source\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1323 (MiB)\n",
            "TRT - WARNING\n",
            "TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.4.0\n",
            "TRT - VERBOSE\n",
            "Total per-runner device persistent memory is 0\n",
            "TRT - VERBOSE\n",
            "Total per-runner host persistent memory is 896\n",
            "TRT - VERBOSE\n",
            "Allocated activation device memory of size 4608\n",
            "TRT - INFO\n",
            "[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4 (MiB)\n",
            "TRT - VERBOSE\n",
            "Calculating Maxima\n",
            "TRT - INFO\n",
            "Starting Calibration.\n",
            "TRT - INFO\n",
            "  Post Processing Calibration data in 2.612e-06 seconds.\n",
            "TRT - INFO\n",
            "Calibration completed in 5.15922 seconds.\n",
            "TRT - ERROR\n",
            "4: [standardEngineBuilder.cpp::initCalibrationParams::1420] Error Code 4: Internal Error (Calibration failure occurred with no scaling factors detected. This could be due to no int8 calibrator or insufficient custom scales for network layers. Please see int8 sample to setup calibration correctly.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-0c70616f1a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#TrtModelOptimizeAndSerialize(precision='fp16')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mcalibSet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMatrixIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mTrtModelOptimizeAndSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibSet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalibSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mModelInferSetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-ee72eb18c846>\u001b[0m in \u001b[0;36mTrtModelOptimizeAndSerialize\u001b[0;34m(precision, calibPath, calibSet)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mserializedEngine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mengineFD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelOptName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'serialize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "print(f\"train_set_shape: {train_set.images.shape}\")\n",
        "print(f\"val_set_shape: {validation_set.images.shape}\")\n",
        "print(f\"test_set_shape: {test_set.images.shape}\")"
      ],
      "metadata": {
        "id": "akbWEe055ILP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41906560-862a-4378-ec48-fa82cd5b9412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_set_shape: (55000, 28, 28)\n",
            "val_set_shape: (5000, 28, 28)\n",
            "test_set_shape: (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "my_iter = iter(validation_set)\n",
        "curr_val = next(my_iter)\n",
        "curr_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxeJmTurZrz3",
        "outputId": "bf3e9e43-9e19-418b-bd8a-91e43a536147"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "#print(validation_set.images)\n",
        "my_iter = MatrixIterator(validation_set.images)\n",
        "curr_val = next(my_iter)\n",
        "type(curr_val)\n",
        "print(np.asarray(curr_val).shape)\n",
        "print(np.asarray(curr_val).size)\n",
        "#print(curr_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d4VOb6ZdsG0",
        "outputId": "2811bf39-d365-4bc6-c8fa-7e11a909a359"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "my_list = [0,1,2,3,4]\n",
        "my_iter = iter(my_list)\n",
        "print(next(my_iter))\n",
        "print(next(my_iter))\n",
        "print(next(my_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T241YoxpoeMx",
        "outputId": "404b4521-03f3-4df4-cd92-d21af756fb9b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting all the global variables:\n",
        "if False:\n",
        "  del(parser)\n",
        "  del(modelName)\n",
        "  del(builder)\n",
        "  del(optimizationProfiler)\n",
        "  del(calib)\n",
        "  del(config)\n",
        "  del(network)\n",
        "  del(engine)\n",
        "  del(runtime)\n",
        "  del(context)\n",
        "  del(inputs)\n",
        "  del(outputs)\n",
        "  del(bindings)\n",
        "  del(stream)"
      ],
      "metadata": {
        "id": "0kkriKbKrsot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}