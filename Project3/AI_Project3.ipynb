{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j-gilyQlVH5n"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  !pip install turicreate\n",
        "  import turicreate as tc\n",
        "  import os\n",
        "  try:\n",
        "    del os.environ['LC_ALL']\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0PiD5Qd5vE",
        "outputId": "31ba51cb-b7f5-476d-e710-6f928cb1fbd9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.ngc.nvidia.com, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nvidia-tensorrt in /usr/local/lib/python3.7/dist-packages (8.4.1.5)\n",
            "Requirement already satisfied: nvidia-cublas-cu11 in /usr/local/lib/python3.7/dist-packages (from nvidia-tensorrt) (2022.4.8)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11 in /usr/local/lib/python3.7/dist-packages (from nvidia-tensorrt) (2022.4.25)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11 in /usr/local/lib/python3.7/dist-packages (from nvidia-tensorrt) (2022.5.19)\n",
            "Requirement already satisfied: nvidia-cublas-cu117 in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11->nvidia-tensorrt) (11.10.1.25)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (0.37.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu117 in /usr/local/lib/python3.7/dist-packages (from nvidia-cuda-runtime-cu11->nvidia-tensorrt) (11.7.60)\n",
            "Requirement already satisfied: nvidia-cudnn-cu116 in /usr/local/lib/python3.7/dist-packages (from nvidia-cudnn-cu11->nvidia-tensorrt) (8.4.0.27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TensorRTUtils:**"
      ],
      "metadata": {
        "id": "t0-7SeTwQje7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorRTUtils\n",
        "!pip install pycuda\n",
        "!pip install tensorrt\n",
        "import tensorrt as trt\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "class ErrorRecorder(trt.IErrorRecorder):\n",
        "    def __init__(self):\n",
        "        trt.IErrorRecorder.__init__(self)\n",
        "        self.errorsStack = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.errorsStack.clear()\n",
        "    def get_error_code(self, arg0):\n",
        "        #Error code saved in the error tuple first position\n",
        "        return self.errorsStack[arg0][0]\n",
        "    def get_error_desc(self, arg0):\n",
        "        # Error code saved in the error tuple second position\n",
        "        return self.errorsStack[arg0][1]\n",
        "    def has_overflowed(self):\n",
        "        return False\n",
        "    def num_errors(self):\n",
        "        return len(self.errorsStack)\n",
        "    def report_error(self, arg0, arg1):\n",
        "        error = (arg0, arg1)\n",
        "        #Errors will be saved as a list of tuples, each tuple will be a pair of error code and error description\n",
        "        self.errorsStack.append(error)\n",
        "\n",
        "class Logger(trt.ILogger):\n",
        "    def __init__(self):\n",
        "        trt.ILogger.__init__(self)\n",
        "\n",
        "    def log(self, severity, msg):\n",
        "        if severity == trt.ILogger.INTERNAL_ERROR:\n",
        "            print('INTERNAL_ERROR')\n",
        "        elif severity == trt.ILogger.ERROR:\n",
        "            print('TRT - ERROR')\n",
        "        elif severity == trt.ILogger.WARNING:\n",
        "            print('TRT - WARNING')\n",
        "        elif severity == trt.ILogger.INFO:\n",
        "            print('TRT - INFO')\n",
        "        elif severity == trt.ILogger.VERBOSE:\n",
        "            print('TRT - VERBOSE')\n",
        "        else:\n",
        "            print('TRT - Wrong severity')\n",
        "\n",
        "        print(msg)\n",
        "\n",
        "class Int8EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
        "    def __init__(self, calibrationSetPath = None, calibSet = None):\n",
        "        # Whenever you specify a custom constructor for a TensorRT class,\n",
        "        # you MUST call the constructor of the parent explicitly.\n",
        "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
        "\n",
        "        self.cacheFile = calibrationSetPath + '/CacheFile.bin'\n",
        "        self.batchSize = 1\n",
        "        self.currentIndex = 0\n",
        "        self.deviceInput = None\n",
        "        self.currentIndex = 0\n",
        "        self.PreProcessedSetPath = calibrationSetPath + '/PreProcessedSet'\n",
        "        self.PreProcessedSetCount = calibSet.n\n",
        "        self.PreProcessedSize = calibSet[0][0].size * 4 #float\n",
        "        self.currentIndex = 0\n",
        "\n",
        "        # Allocate enough memory for a whole batch.\n",
        "        self.deviceInput = cuda.mem_alloc(self.PreProcessedSize)\n",
        "\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            print('Calibration cache file is already exist - ', self.cacheFile)\n",
        "            return\n",
        "\n",
        "        filesCnt = os.listdir(self.PreProcessedSetPath)\n",
        "\n",
        "        if len(filesCnt) == self.PreProcessedSetCount:\n",
        "            print('ERROR - Pre processed file set is exist!!!')\n",
        "            return\n",
        "\n",
        "        if self.PreProcessedSetCount == 0:\n",
        "            print('ERROR - Calibration set is empty!!!')\n",
        "\n",
        "        print('Start calibration batches build')\n",
        "\n",
        "        for idx in range(self.PreProcessedSetCount):\n",
        "            preProcImg, label = calibSet.next()\n",
        "            preProcessedFile = open(self.PreProcessedSetPath + '/' + str(idx) + '.bin', mode='wb')\n",
        "            preProcImg.tofile(preProcessedFile)\n",
        "            preProcessedFile.close()\n",
        "\n",
        "        print('End calibration batches build')\n",
        "\n",
        "    def get_algorithm(self):\n",
        "        return trt.CalibrationAlgoType.ENTROPY_CALIBRATION_2\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batchSize\n",
        "\n",
        "    # TensorRT passes along the names of the engine bindings to the get_batch function.\n",
        "    # You don't necessarily have to use them, but they can be useful to understand the order of\n",
        "    # the inputs. The bindings list is expected to have the same ordering as 'names'.\n",
        "    def get_batch(self, names):\n",
        "        if not self.currentIndex < self.PreProcessedSetCount:\n",
        "            return None\n",
        "\n",
        "        print('Get pre processed file index - ', not self.currentIndex)\n",
        "\n",
        "        batchData = np.fromfile(self.PreProcessedSetPath + '/' + str(self.currentIndex) + '.bin', dtype=np.single)\n",
        "        cuda.memcpy_htod(self.deviceInput, batchData)\n",
        "        self.currentIndex += 1\n",
        "\n",
        "        return [self.deviceInput]\n",
        "\n",
        "    def read_calibration_cache(self):\n",
        "        # If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.\n",
        "        if os.path.exists(self.cacheFile):\n",
        "            with open(self.cacheFile, \"rb\") as f:\n",
        "                return f.read()\n",
        "\n",
        "    def write_calibration_cache(self, cache):\n",
        "        with open(self.cacheFile, \"wb\") as f:\n",
        "            f.write(cache)\n",
        "\n",
        "logger = Logger()\n",
        "errorRecorder = ErrorRecorder()\n",
        "\n",
        "builder = trt.Builder(logger)\n",
        "builder.max_batch_size = 1\n",
        "\n",
        "calib = None\n",
        "config = builder.create_builder_config()\n",
        "config.max_workspace_size = 1073741824\n",
        "\n",
        "optimizationProfiler = builder.create_optimization_profile()\n",
        "\n",
        "networkFlags = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "network = builder.create_network(networkFlags)\n",
        "parser = trt.OnnxParser(network, logger)\n",
        "runtime = trt.Runtime(logger)\n",
        "\n",
        "engine = None\n",
        "context = None\n",
        "\n",
        "modelName = None\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "bindings = []\n",
        "stream = None\n",
        "\n",
        "def TrtModelParse(modelPath):\n",
        "    global modelName\n",
        "    global parser\n",
        "    global network\n",
        "\n",
        "    modelName = modelPath.split('.')[0]\n",
        "    parseResult = parser.parse_from_file(modelPath)\n",
        "\n",
        "    if (not parseResult):\n",
        "        for error in range(parser.num_errors):\n",
        "            print(str(parser.get_error(error)))\n",
        "    else:\n",
        "        print(\"Model parsing OK!\")\n",
        "\n",
        "        print(\"Network Description\")\n",
        "\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "\n",
        "        for input in inputs:\n",
        "            print(\"Input '{}' with shape {} and dtype {}\".format(input.name, input.shape, input.dtype))\n",
        "        for output in outputs:\n",
        "            print(\"Output '{}' with shape {} and dtype {}\".format(output.name, output.shape, output.dtype))\n",
        "\n",
        "def TrtModelOptimizeAndSerialize(precision = 'fp32',calibPath=\"\", calibSet=None):\n",
        "    global modelName\n",
        "    global builder\n",
        "    global optimizationProfiler\n",
        "    global calib\n",
        "    global config\n",
        "    global network\n",
        "    global engine\n",
        "    global runtime\n",
        "\n",
        "    modelOptName = modelName + precision + '.trt.engine'\n",
        "\n",
        "    if os.path.exists(modelOptName):\n",
        "        with open(modelOptName, 'rb') as f:\n",
        "            engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    else:\n",
        "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "        input = network.get_input(0)\n",
        "\n",
        "        inputShape = [1, input.shape[1], input.shape[2], input.shape[3]]\n",
        "\n",
        "        optimizationProfiler.set_shape(input.name, inputShape, inputShape, inputShape)\n",
        "\n",
        "        config.add_optimization_profile(optimizationProfiler)\n",
        "\n",
        "        if precision == 'fp16':\n",
        "            if builder.platform_has_fast_fp16:\n",
        "                config.set_flag(trt.BuilderFlag.FP16)\n",
        "        elif precision == 'int8':\n",
        "            if builder.platform_has_fast_int8:\n",
        "                if builder.platform_has_fast_fp16:\n",
        "                    # Also enable fp16, as some layers may be even more efficient in fp16 than int8\n",
        "                    config.set_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "                config.set_flag(trt.BuilderFlag.INT8)\n",
        "\n",
        "                calib = Int8EntropyCalibrator(calibPath, calibSet)\n",
        "                config.int8_calibrator = calib\n",
        "\n",
        "        engine = builder.build_engine(network, config)\n",
        "\n",
        "        serializedEngine = engine.serialize()\n",
        "\n",
        "        engineFD = open(modelOptName, 'wb')\n",
        "        engineFD.write(serializedEngine)\n",
        "        engineFD.close()\n",
        "\n",
        "    print('TRT engine - ', engine.device_memory_size, ' Bytes')\n",
        "    engineDeviceMemory = 0\n",
        "    engineDeviceMemory += engine.device_memory_size\n",
        "    print('TRT engine number of layers - ', engine.num_layers)\n",
        "    print('TRT engine number of bindings - ', engine.num_bindings)\n",
        "    print('TRT engine number of profils - ', engine.num_optimization_profiles)\n",
        "\n",
        "    print('Completion optimized model')\n",
        "\n",
        "def ModelInferSetup():\n",
        "    global context\n",
        "    global engine\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "    global stream\n",
        "\n",
        "    stream = cuda.Stream()\n",
        "\n",
        "    #Over all Tensors inputs & outputs of the TRT engine\n",
        "    #TRT hold first all Tensors inputs and after the Tensor outptus\n",
        "    for binding in engine:\n",
        "        #Get current binded Tensor volume size in elemente units\n",
        "        size = trt.volume(engine.get_binding_shape(binding))\n",
        "        #Get current binded Tensor element type\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host page locked bbuffer\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        # Allocate device bbuffer\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "\n",
        "    # Contexts are used to perform inference.\n",
        "    context = engine.create_execution_context()\n",
        "    context.error_recorder = errorRecorder\n",
        "\n",
        "def Inference(externalnputs = None):\n",
        "\n",
        "    global context\n",
        "    global stream\n",
        "    global inputs\n",
        "    global outputs\n",
        "    global bindings\n",
        "\n",
        "    try:\n",
        "        #verify that TRT context generated successfully\n",
        "        if context is not None:\n",
        "            #Verify that inputs to inference are exist\n",
        "            if externalnputs is not None:\n",
        "                #Copy all Tensors inputs data from user memory to TRT host page locked memory before loading it to the device\n",
        "                if len(externalnputs) == len(inputs):\n",
        "                    for index in range(len(externalnputs)):\n",
        "                        if len(inputs[index].host) == externalnputs[index].size:\n",
        "                            np.copyto(inputs[index].host, externalnputs[index].ravel())\n",
        "                        else:\n",
        "                            print('TRT external input size - ', externalnputs[index].size,\n",
        "                                  ' is not equal to model inputs size - ', len(inputs[index].host))\n",
        "                            return None\n",
        "\n",
        "                    # Transfer input data to the GPU from the host page locked memory.\n",
        "                    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "                    # Run asynchronously inference using the user\\internal stream.\n",
        "                    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "                    # Transfer predictions back from the GPU.\n",
        "                    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "\n",
        "                    stream.synchronize()\n",
        "                    # Build a list of Tensors outputs and return only the host outputs.\n",
        "                    return [out.host for out in outputs]\n",
        "                else:\n",
        "                    print('External inputs list size - ', len(externalnputs), ' is not equal to model inputs list size - ', len(inputs))\n",
        "                    return None\n",
        "            else:\n",
        "                print('External inputs list is None ERROR')\n",
        "                return None\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        print('TRT inference exception ERROR - ', msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzctBFwmVnAo",
        "outputId": "5cb06dd1-cda0-427f-82ad-1312aac636f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.7/dist-packages (2022.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda) (2022.1.12)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.2.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (2.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "TRT - INFO\n",
            "The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
            "\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 312 (MiB)\n",
            "TRT - INFO\n",
            "The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
            "\n",
            "TRT - INFO\n",
            "[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 312 (MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: DeprecationWarning: Use set_memory_pool_limit instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **onnxUtils:**"
      ],
      "metadata": {
        "id": "2cyDtE0LQrOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# onnxUtils\n",
        "!pip install tf2onnx onnx onnxsim\n",
        "import json\n",
        "import time\n",
        "import tf2onnx\n",
        "import onnx\n",
        "#import onnxsim\n",
        "import os.path\n",
        "\n",
        "\n",
        "# Save model into h5 and ONNX formats\n",
        "def convertKerasToONNX(name, model, overwrite_existing = False):\n",
        "    modelFile = name + '.onnx'\n",
        "    if not os.path.isfile(modelFile) or overwrite_existing:\n",
        "        # Save model with ONNX format\n",
        "        (onnx_model_proto, storage) = tf2onnx.convert.from_keras(model)\n",
        "        with open(os.path.join(modelFile), \"wb\") as f:\n",
        "            f.write(onnx_model_proto.SerializeToString())\n",
        "            f.close()\n",
        "    \n",
        "    return modelFile, onnx_model_proto, storage\n",
        "\n",
        "def ModelOnnxCheck(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    isCheckOk = True\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"Onnx model check report:\")\n",
        "\n",
        "    try:\n",
        "        # Perform basic check on the model input\n",
        "        onnx.checker.check_model(name + '.onnx')\n",
        "        isCheckOk = True\n",
        "    except onnx.checker.ValidationError as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "    except BaseException as e:\n",
        "        msg = e\n",
        "        isCheckOk=False\n",
        "\n",
        "    if isCheckOk:\n",
        "        print('Model check completed Successfully')\n",
        "    else:\n",
        "        print('ERROR - Model check failure')\n",
        "\n",
        "    print('Model onnx checker, check model - ', msg)\n",
        "\n",
        "    return isCheckOk\n",
        "\n",
        "def RemoveInitializerFromInput(model, modelPath):\n",
        "    modelGraphInputs = model.graph.input\n",
        "    startInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    nameToInput = {}\n",
        "    for input in modelGraphInputs:\n",
        "        nameToInput[input.name] = input\n",
        "\n",
        "    for initializer in model.graph.initializer:\n",
        "        if initializer.name in nameToInput:\n",
        "            modelGraphInputs.remove(nameToInput[initializer.name])\n",
        "\n",
        "    endInputsCount = len(modelGraphInputs)\n",
        "\n",
        "    if startInputsCount != endInputsCount:\n",
        "        print('Model includes several Initializers which considered as inputs to the graph - ', startInputsCount - endInputsCount)\n",
        "        print('All Initializers were removed from graph inputs')\n",
        "        print('Replace the model *.onx file with the updated one')\n",
        "        onnx.save(model, modelPath)\n",
        "\n",
        "def ProcessModelInputs(model, modelPath):\n",
        "    RemoveInitializerFromInput(model, modelPath)\n",
        "    modelGraphInputs = model.graph.input\n",
        "\n",
        "    modelInputsDims = {}\n",
        "    modelDynamicInputsDict = {}\n",
        "    modelInputs = modelGraphInputs\n",
        "    modelInputsNames = []\n",
        "    print(str(modelInputs))\n",
        "\n",
        "    for tensorInput in modelInputs:\n",
        "        isInputDynamic = False\n",
        "        modelDynamicInputShape = []\n",
        "        for dim in tensorInput.type.tensor_type.shape.dim:\n",
        "            if dim.dim_value == 0:\n",
        "                isInputDynamic = True\n",
        "                print('CAUTION!!! - Tensor input name' + ' - ', tensorInput.name, ', dimension - ' , dim.dim_param, ', set its value to 1 for Onnx simplify operation')\n",
        "                modelDynamicInputShape.append(1)\n",
        "            else:\n",
        "                modelDynamicInputShape.append(dim.dim_value)\n",
        "\n",
        "        modelInputsNames.append(tensorInput.name)\n",
        "\n",
        "        if isInputDynamic is True:\n",
        "            modelDynamicInputsDict[tensorInput.name] = modelDynamicInputShape\n",
        "\n",
        "    return modelDynamicInputsDict\n",
        "\n",
        "def ModelSimplify(name):\n",
        "\n",
        "    msg = 'OK'\n",
        "    nameSimp = name + 'Simp'\n",
        "    model = None\n",
        "    isSimplifiedOK = True\n",
        "\n",
        "    if os.path.exists(nameSimp + '.onnx'):\n",
        "        print('Model Onnx simplify is already exist, No model check and\\or simplify operations is required')\n",
        "        model = onnx.load(nameSimp + '.onnx')\n",
        "        isSimplifiedOK = True\n",
        "    else:\n",
        "        print(\"===============================================================\")\n",
        "        print(\"Onnx model simplifier report:\")\n",
        "        model = onnx.load(name + '.onnx')\n",
        "\n",
        "        modelDynamicInputsDict = ProcessModelInputs(model, name + '.onnx')\n",
        "\n",
        "        try:\n",
        "            print('Start model onnx simplify...')\n",
        "            # Perform simplification on the model input\n",
        "            model, check = onnxsim.simplify(model,input_shapes=modelDynamicInputsDict,\n",
        "                                                  dynamic_input_shape=(len(modelDynamicInputsDict) > 0))\n",
        "            print('Completion model onnx simplify')\n",
        "            if (check):\n",
        "                isSimplifiedOK = True\n",
        "                print('Onnx simplification success!')\n",
        "                print('Save Onnx simplified model to - ', nameSimp + '.onnx')\n",
        "                onnx.save(model, nameSimp + '.onnx')\n",
        "            else:\n",
        "                isSimplifiedOK = False\n",
        "                print('Onnx simplification failure!')\n",
        "                print('Simplified Onnx model could not be generated and validated')\n",
        "        except BaseException as e:\n",
        "            print('Onnx simplification exception - ', e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU_AzfphVoRZ",
        "outputId": "7ae18a0c-63ab-443e-8cc4-5e486176a036"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.7/dist-packages (1.12.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.7/dist-packages (1.12.0)\n",
            "Requirement already satisfied: onnxsim in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.12)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.7/dist-packages (from onnxsim) (12.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2.10)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (2.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **wandb_helpers:**"
      ],
      "metadata": {
        "id": "Yuy6_TxZQvKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb_helpers\n",
        "!pip install wandb\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "Dataset = namedtuple(\"Dataset\", [\"images\", \"labels\"])\n",
        "dataset_names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "def start_wandb_run(model_name, config):\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    return wandb.init(project=f\"ml-p2\", entity=\"ml-p2\", name=f\"{model_name}-{timestamp}\" , \n",
        "        notes = f\"Training FCNN model @{timestamp}\", config = config)\n",
        "\n",
        "def read_datasets(wandb_run, dataset_tag = \"latest\"):\n",
        "    '''\n",
        "    Read all datasets from W&B.\n",
        "    Usage example: train_set, validation_set, test_set = wbh.read_datasets(run)\n",
        "    '''\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/fashion-mnist:{dataset_tag}', type='dataset')\n",
        "    data_dir = artifact.download()\n",
        "    return [ read_dataset(data_dir, ds_name) for ds_name in dataset_names ]\n",
        "\n",
        "def read_dataset(data_dir, ds_name):\n",
        "    filename = ds_name + \".npz\"\n",
        "    data = np.load(os.path.join(data_dir, filename))\n",
        "    return Dataset(images = data[\"x\"], labels = data[\"y\"])\n",
        "\n",
        "def read_model(wandb_run, model_name, model_tag = \"latest\") -> tf.keras.models.Model:\n",
        "    artifact = wandb_run.use_artifact(f'ml-p2/ml-p2/{model_name}:{model_tag}', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    return tf.keras.models.load_model(artifact_dir)\n",
        "\n",
        "def save_model(wandb_run, model, config, model_name, model_description):\n",
        "    model_file = f'./saved-models/{model_name}.tf'\n",
        "    tf.keras.models.save_model(model, model_file)\n",
        "    model_artifact = wandb.Artifact(model_name, type = \"model\", description=model_description, metadata= dict(config))\n",
        "    model_artifact.add_dir(model_file)\n",
        "    wandb_run.log_artifact(model_artifact)\n",
        "\n",
        "def load_best_model(sweep_id):\n",
        "    api = wandb.Api()\n",
        "    sweep = api.sweep(f\"ml-p2/ml-p2/{sweep_id}\")\n",
        "    runs = sorted(sweep.runs,\n",
        "        key=lambda run: run.summary.get(\"val_accuracy\", 0), reverse=True)\n",
        "    val_acc = runs[0].summary.get(\"val_accuracy\", 0)\n",
        "    print(f\"Best run {runs[0].name} with {val_acc} validation accuracy\")\n",
        "\n",
        "    model_file = runs[0].file(\"model-best.h5\").download(replace=True)\n",
        "    model_file.close()\n",
        "\n",
        "#if (__name__ == \"__main__\"):\n",
        "#    load_best_model(\"6zmewzd0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_iT8JNVVobY",
        "outputId": "21cb0f72-dce1-476e-e18a-bd10d24ea7d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **trt-inference:**"
      ],
      "metadata": {
        "id": "HynoQt2cQzkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trt-inference\n",
        "#!pip install sklearn -qqq\n",
        "\n",
        "#from TensorRTUtils import *\n",
        "#from onnxUtils import convertKerasToONNX\n",
        "#import wandb_helpers as wbh\n",
        "\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import tensorrt as trt\n",
        "import onnx\n",
        "import tf2onnx\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "modelName = \"FCNN\"\n",
        "\n",
        "'''\n",
        "Stage 1: Load an existing model\n",
        "===============================\n",
        "In this part we load the model we created in the previous project\n",
        "which is built to infer from FASHION-MNIST images.\n",
        "It is not a sofisticated model, but the idea to use something we\n",
        "know.\n",
        "'''\n",
        "dataset_path = '.\\\\artifacts\\\\fashion-mnist-v2'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    with start_wandb_run(\"FCNN-metrics\", None) as run:\n",
        "        train_set, validation_set, test_set = read_datasets(run)\n",
        "        model = read_model(run, \"FCNN\", \"latest\")\n",
        "else:\n",
        "    test_set = read_dataset('.\\\\artifacts\\\\fashion-mnist-v2', 'test')\n",
        "    model = tf.keras.models.load_model('.\\\\artifacts\\\\FCNN-v3')\n",
        "\n",
        "'''\n",
        "Stage 2: Convert to ONNX\n",
        "========================\n",
        "Convert the model to ONNX and save it to a file. This will allow\n",
        "us to load the model into a tensor-rt engine.\n",
        "'''\n",
        "modelFile, _, _ = convertKerasToONNX(modelName, model, True)\n",
        "\n",
        "'''\n",
        "Stage 3: Create the tensor-rt engine\n",
        "====================================\n",
        "Now that we a model file, we can load it into a \n",
        "tensor rt engine.\n",
        "We use FP 32 precision.\n",
        "'''\n",
        "TrtModelParse(modelFile)\n",
        "#TrtModelOptimizeAndSerialize(precision='fp32')\n",
        "TrtModelOptimizeAndSerialize(precision='fp16')\n",
        "ModelInferSetup()\n",
        "\n",
        "'''\n",
        "Stage 4: Inference\n",
        "==================\n",
        "Now the model is ready for inference. The model is executed several\n",
        "times on different images from the test set we've loaded on Stage 1\n",
        "'''\n",
        "inputs = []\n",
        "\n",
        "startTimeCpu = time.time()\n",
        "for i in range(len(test_set)):\n",
        "    img = test_set.images[i]\n",
        "    lbl = test_set.labels[i]\n",
        "    inputs.append(img)\n",
        "    outputsTrt = Inference(externalnputs=inputs)\n",
        "    #print(' topClassIdx - ', np.argmax(outputsTrt[0]))\n",
        "    inputs.clear()\n",
        "    \n",
        "    \n",
        "endTimeCpu = time.time()\n",
        "\n",
        "# total time taken\n",
        "averageTime = (endTimeCpu - startTimeCpu) / 1e-3 / len(test_set)\n",
        "print(f\"TRT Keras inference average time is: {averageTime} milliseconds\")\n",
        "print(f\"TRT Keras inference average FPS is: {1000 / averageTime}\")\n",
        "\n",
        "# Perform the DlewareAnalyzer inference with TRT & ORT\n",
        "\n",
        "#np.testing.assert_allclose(kerasPredictions, onnxPredictions[0], rtol=0, atol=1e-05, err_msg='Keras Vs. Onnx Failure!!!')\n",
        "\n",
        "\n",
        "#y_test = np.argmax(test_set.labels)\n",
        "# predictions = model.predict(test_set.images)\n",
        "# y_test = np.argmax(predictions, axis = 1)\n",
        "# print (classification_report(test_set.labels, y_test))\n",
        "# cm = confusion_matrix(test_set.labels, y_test)\n",
        "\n",
        "# class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "# ax = plt.subplot()\n",
        "# h = sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# # labels, title and ticks\n",
        "# ax.set_xlabel('Predicted labels')\n",
        "# ax.set_ylabel('True labels')\n",
        "# ax.set_title('Confusion Matrix')\n",
        "# ax.xaxis.set_ticklabels(class_names)\n",
        "# ax.yaxis.set_ticklabels(class_names)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "EEa4aCwOWALB",
        "outputId": "5a4ab3c2-8c77-41b4-cec2-fa4adc3e65e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem at: <ipython-input-15-1c7ac82602ae> 16 start_wandb_run\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 1043, in init\n",
            "    run = wi.init()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 553, in init\n",
            "    manager._inform_init(settings=self.settings, run_id=self.settings.run_id)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_manager.py\", line 161, in _inform_init\n",
            "    svc_iface._svc_inform_init(settings=settings, run_id=run_id)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/service/service_sock.py\", line 39, in _svc_inform_init\n",
            "    self._sock_client.send(inform_init=inform_init)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 140, in send\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 84, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 81, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 61, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data[total_sent:])\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setting up manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inform_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_manager.py\u001b[0m in \u001b[0;36m_inform_init\u001b[0;34m(self, settings, run_id)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0msvc_iface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_service_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0msvc_iface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_svc_inform_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/service/service_sock.py\u001b[0m in \u001b[0;36m_svc_inform_init\u001b[0;34m(self, settings, run_id)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minform_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minform_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, inform_init, inform_start, inform_attach, inform_finish, inform_teardown)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unmatched\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendall_with_error_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_sent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# sent equal to 0 indicates a closed socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-098257f9a48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mstart_wandb_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FCNN-metrics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FCNN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1c7ac82602ae>\u001b[0m in \u001b[0;36mstart_wandb_run\u001b[0;34m(model_name, config)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     return wandb.init(project=f\"ml-p2\", entity=\"ml-p2\", name=f\"{model_name}-{timestamp}\" , \n\u001b[0;32m---> 16\u001b[0;31m         notes = f\"Training FCNN model @{timestamp}\", config = config)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"latest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexcept_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merror_seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: problem"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  del(parser)\n",
        "  del(modelName)\n",
        "  del(builder)\n",
        "  del(optimizationProfiler)\n",
        "  del(calib)\n",
        "  del(config)\n",
        "  del(network)\n",
        "  del(engine)\n",
        "  del(runtime)\n",
        "  del(context)\n",
        "  del(inputs)\n",
        "  del(outputs)\n",
        "  del(bindings)\n",
        "  del(stream)"
      ],
      "metadata": {
        "id": "0kkriKbKrsot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}