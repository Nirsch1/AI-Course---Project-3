{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onnx_resnet50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing TensorRT:**"
      ],
      "metadata": {
        "id": "xsqLK9j2F_Qq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0g3Kdj7FGEU",
        "outputId": "c78e2095-a122-4f6f-b091-5f3fda154122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.ngc.nvidia.com, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorrt/nvidia_tensorrt-8.4.1.5-cp37-none-linux_x86_64.whl (774.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 774.4 MB 16 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu11/nvidia-cudnn-cu11-2022.5.19.tar.gz (16 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu11/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu11\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu11/nvidia-cublas-cu11-2022.4.8.tar.gz (16 kB)\n",
            "Collecting nvidia-cublas-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cublas-cu117/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl (333.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 333.1 MB 38 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia-tensorrt) (57.4.0)\n",
            "Collecting nvidia-cuda-runtime-cu117\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-runtime-cu117/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu116\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cudnn-cu116/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl (719.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 719.3 MB 18 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-cublas-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11\n",
            "  Building wheel for nvidia-cublas-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cublas-cu11: filename=nvidia_cublas_cu11-2022.4.8-py3-none-any.whl size=15624 sha256=0068cfba4b7ce5a583b76d98e3340d26a68258a5f7d0e055be85ad87b866f3b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/c3/94/1ffd5bac267cfdc2b222a4ec6915278ef18a028a916b9a5ac3\n",
            "  Building wheel for nvidia-cuda-runtime-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cuda-runtime-cu11: filename=nvidia_cuda_runtime_cu11-2022.4.25-py3-none-any.whl size=15696 sha256=8ba87e1df3bf52e79ccdb981c5c6b44958fc45842860224f03be6505892b6c1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/fe/2b/e553db7867508b2268b14ac194e9ac5b3f51f21316c282c96c\n",
            "  Building wheel for nvidia-cudnn-cu11 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-cudnn-cu11: filename=nvidia_cudnn_cu11-2022.5.19-py3-none-any.whl size=15617 sha256=5a9b15a96e6b19c6b5f4c010ce38347cd3fd0a6ca2fd4b7467cd25431d16ecba\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/32/69/9787704b5f889217708864db5e00812c8c1c349ef89084c59c\n",
            "Successfully built nvidia-cublas-cu11 nvidia-cuda-runtime-cu11 nvidia-cudnn-cu11\n",
            "Installing collected packages: nvidia-cudnn-cu116, nvidia-cuda-runtime-cu117, nvidia-cublas-cu117, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-2022.4.8 nvidia-cublas-cu117-11.10.1.25 nvidia-cuda-runtime-cu11-2022.4.25 nvidia-cuda-runtime-cu117-11.7.60 nvidia-cudnn-cu11-2022.5.19 nvidia-cudnn-cu116-8.4.0.27 nvidia-tensorrt-8.4.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing prerequisites:**"
      ],
      "metadata": {
        "id": "ov-AVlcJGHYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy Pillow pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR3O0bYfFyYz",
        "outputId": "ea39f362-4480-4d8c-9651-89c1a155940b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.20.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.7/dist-packages (2022.1)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda) (2022.1.12)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.2.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.1.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (2.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **onnx_resnet50:**"
      ],
      "metadata": {
        "id": "72klUK7TKwPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# This sample uses an ONNX ResNet50 Model to create a TensorRT Inference Engine\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# This import causes pycuda to automatically manage CUDA context creation and cleanup.\n",
        "import pycuda.autoinit\n",
        "import tensorrt as trt\n",
        "from PIL import Image\n",
        "\n",
        "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))"
      ],
      "metadata": {
        "id": "jFvxrpcsK0TB"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelData(object):\n",
        "    MODEL_PATH = \"ResNet50.onnx\"\n",
        "    #MODEL_PATH = \"/content/gdrive/MyDrive/Colab Notebooks/introductory_parser_samples/ResNet50.onnx\"\n",
        "    INPUT_SHAPE = (3, 224, 224)\n",
        "    # We can convert TensorRT data types to numpy types with trt.nptype()\n",
        "    DTYPE = trt.float32"
      ],
      "metadata": {
        "id": "Lq9IJ7hwK0st"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can set the logger severity higher to suppress messages (or lower to display more messages).\n",
        "#TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)"
      ],
      "metadata": {
        "id": "lYA8JlleK0wc"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "\n",
        "def GiB(val):\n",
        "    return val * 1 << 30"
      ],
      "metadata": {
        "id": "WWc2KJhlTnQi"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Onnx path is used for Onnx models.\n",
        "def build_engine_onnx(model_file):\n",
        "    builder = trt.Builder(TRT_LOGGER)\n",
        "    network = builder.create_network(EXPLICIT_BATCH)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\n",
        "    config.max_workspace_size = GiB(1)\n",
        "    # Load the Onnx model and parse it in order to populate the TensorRT network.\n",
        "    with open(model_file, \"rb\") as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"ERROR: Failed to parse the ONNX file.\")\n",
        "            for error in range(parser.num_errors):\n",
        "                print(parser.get_error(error))\n",
        "            return None\n",
        "        print(\"model: \", model)\n",
        "    return builder.build_engine(network, config)"
      ],
      "metadata": {
        "id": "QLPnDlecK0z6"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_normalized_test_case(test_image, pagelocked_buffer):\n",
        "    # Converts the input image to a CHW Numpy array\n",
        "    def normalize_image(image):\n",
        "        # Resize, antialias and transpose the image to CHW.\n",
        "        c, h, w = ModelData.INPUT_SHAPE\n",
        "        image_arr = (\n",
        "            np.asarray(image.resize((w, h), Image.ANTIALIAS))\n",
        "            .transpose([2, 0, 1])\n",
        "            .astype(trt.nptype(ModelData.DTYPE))\n",
        "            .ravel()\n",
        "        )\n",
        "        # This particular ResNet50 model requires some preprocessing, specifically, mean normalization.\n",
        "        return (image_arr / 255.0 - 0.45) / 0.225\n",
        "\n",
        "    # Normalize the image and copy to pagelocked memory.\n",
        "    np.copyto(pagelocked_buffer, normalize_image(Image.open(test_image)))"
      ],
      "metadata": {
        "id": "4cNVs2UVK04R"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Common:**"
      ],
      "metadata": {
        "id": "Alr9IANKTLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import tensorrt as trt\n",
        "\n",
        "try:\n",
        "    # Sometimes python does not understand FileNotFoundError\n",
        "    FileNotFoundError\n",
        "except NameError:\n",
        "    FileNotFoundError = IOError\n",
        "\n",
        "def add_help(description):\n",
        "    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "\n",
        "def find_sample_data(description=\"Runs a TensorRT Python sample\", subfolder=\"\", find_files=[], err_msg=\"\"):\n",
        "    \"\"\"\n",
        "    Parses sample arguments.\n",
        "    Args:\n",
        "        description (str): Description of the sample.\n",
        "        subfolder (str): The subfolder containing data relevant to this sample\n",
        "        find_files (str): A list of filenames to find. Each filename will be replaced with an absolute path.\n",
        "    Returns:\n",
        "        str: Path of data directory.\n",
        "    \"\"\"\n",
        "\n",
        "    # Standard command-line arguments for all samples.\n",
        "    #kDEFAULT_DATA_ROOT = os.path.join(os.sep, \"usr\", \"src\", \"tensorrt\", \"data\")\n",
        "    kDEFAULT_DATA_ROOT = os.path.join(os.sep, \"content\", \"gdrive\", \"MyDrive\", \"Colab Notebooks\", \"introductory_parser_samples\")\n",
        "    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument(\n",
        "        \"-d\",\n",
        "        \"--datadir\",\n",
        "        help=\"Location of the TensorRT sample data directory, and any additional data directories.\",\n",
        "        action=\"append\",\n",
        "        default=[kDEFAULT_DATA_ROOT],\n",
        "    )\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    def get_data_path(data_dir):\n",
        "        # If the subfolder exists, append it to the path, otherwise use the provided path as-is.\n",
        "        data_path = os.path.join(data_dir, subfolder)\n",
        "        if not os.path.exists(data_path):\n",
        "            if data_dir != kDEFAULT_DATA_ROOT:\n",
        "                print(\"WARNING: \" + data_path + \" does not exist. Trying \" + data_dir + \" instead.\")\n",
        "            data_path = data_dir\n",
        "        # Make sure data directory exists.\n",
        "        if not (os.path.exists(data_path)) and data_dir != kDEFAULT_DATA_ROOT:\n",
        "            print(\n",
        "                \"WARNING: {:} does not exist. Please provide the correct data path with the -d option.\".format(\n",
        "                    data_path\n",
        "                )\n",
        "            )\n",
        "        return data_path\n",
        "\n",
        "    data_paths = [get_data_path(data_dir) for data_dir in args.datadir]\n",
        "    return data_paths, locate_files(data_paths, find_files, err_msg)\n",
        "\n",
        "\n",
        "def locate_files(data_paths, filenames, err_msg=\"\"):\n",
        "    \"\"\"\n",
        "    Locates the specified files in the specified data directories.\n",
        "    If a file exists in multiple data directories, the first directory is used.\n",
        "    Args:\n",
        "        data_paths (List[str]): The data directories.\n",
        "        filename (List[str]): The names of the files to find.\n",
        "    Returns:\n",
        "        List[str]: The absolute paths of the files.\n",
        "    Raises:\n",
        "        FileNotFoundError if a file could not be located.\n",
        "    \"\"\"\n",
        "    found_files = [None] * len(filenames)\n",
        "    for data_path in data_paths:\n",
        "        # Find all requested files.\n",
        "        for index, (found, filename) in enumerate(zip(found_files, filenames)):\n",
        "            if not found:\n",
        "                file_path = os.path.abspath(os.path.join(data_path, filename))\n",
        "                if os.path.exists(file_path):\n",
        "                    found_files[index] = file_path\n",
        "\n",
        "    # Check that all files were found\n",
        "    for f, filename in zip(found_files, filenames):\n",
        "        if not f or not os.path.exists(f):\n",
        "            raise FileNotFoundError(\n",
        "                \"Could not find {:}. Searched in data paths: {:}\\n{:}\".format(filename, data_paths, err_msg)\n",
        "            )\n",
        "    return found_files\n",
        "\n",
        "\n",
        "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "\n",
        "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
        "def allocate_buffers(engine):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    bindings = []\n",
        "    stream = cuda.Stream()\n",
        "    for binding in engine:\n",
        "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host and device buffers\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "    return inputs, outputs, bindings, stream\n",
        "\n",
        "\n",
        "# This function is generalized for multiple inputs/outputs.\n",
        "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
        "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
        "    # Transfer input data to the GPU.\n",
        "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "    # Run inference.\n",
        "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
        "    # Transfer predictions back from the GPU.\n",
        "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "    # Synchronize the stream\n",
        "    stream.synchronize()\n",
        "    # Return only the host outputs.\n",
        "    return [out.host for out in outputs]\n",
        "\n",
        "\n",
        "# This function is generalized for multiple inputs/outputs for full dimension networks.\n",
        "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
        "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
        "    # Transfer input data to the GPU.\n",
        "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "    # Run inference.\n",
        "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "    # Transfer predictions back from the GPU.\n",
        "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "    # Synchronize the stream\n",
        "    stream.synchronize()\n",
        "    # Return only the host outputs.\n",
        "    return [out.host for out in outputs]"
      ],
      "metadata": {
        "id": "cCeXH21qTGYy"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onnx_resnet50():\n",
        "    # Set the data path to the directory that contains the trained models and test images for inference.\n",
        "    _, data_files = find_sample_data(\n",
        "        description=\"Runs a ResNet50 network with a TensorRT inference engine.\",\n",
        "        subfolder=\"resnet50\",\n",
        "        find_files=[\n",
        "            \"binoculars.jpeg\",\n",
        "            \"reflex_camera.jpeg\",\n",
        "            \"tabby_tiger_cat.jpg\",\n",
        "            ModelData.MODEL_PATH,\n",
        "            \"class_labels.txt\",\n",
        "        ],\n",
        "    )\n",
        "    # Get test images, models and labels.\n",
        "    test_images = data_files[0:3]\n",
        "    onnx_model_file, labels_file = data_files[3:]\n",
        "    labels = open(labels_file, \"r\").read().split(\"\\n\")\n",
        "\n",
        "    # Build a TensorRT engine.\n",
        "    engine = build_engine_onnx(onnx_model_file)\n",
        "    print(f\"engine: {engine}\")\n",
        "    # Inference is the same regardless of which parser is used to build the engine, since the model architecture is the same.\n",
        "    # Allocate buffers and create a CUDA stream.\n",
        "    inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
        "    # Contexts are used to perform inference.\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # Load a normalized test case into the host input page-locked buffer.\n",
        "    test_image = random.choice(test_images)\n",
        "    test_case = load_normalized_test_case(test_image, inputs[0].host)\n",
        "    # Run the engine. The output will be a 1D tensor of length 1000, where each value represents the\n",
        "    # probability that the image corresponds to that label\n",
        "    trt_outputs = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
        "    # We use the highest probability as our prediction. Its index corresponds to the predicted label.\n",
        "    pred = labels[np.argmax(trt_outputs[0])]\n",
        "    if \"_\".join(pred.split()) in os.path.splitext(os.path.basename(test_case))[0]:\n",
        "        print(\"Correctly recognized \" + test_case + \" as \" + pred)\n",
        "    else:\n",
        "        print(\"Incorrectly recognized \" + test_case + \" as \" + pred)"
      ],
      "metadata": {
        "id": "G1mTtr3YTZDT"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_resnet50()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "FyXz0vBfnP6Z",
        "outputId": "a8c13e51-f55a-4bd4-9ead-543516d13d84"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Use set_memory_pool_limit instead.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  <_io.BufferedReader name='/content/gdrive/MyDrive/Colab Notebooks/introductory_parser_samples/ResNet50.onnx'>\n",
            "engine: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Use build_serialized_network instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-675c0400308e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monnx_resnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-157-65d75a00f568>\u001b[0m in \u001b[0;36monnx_resnet50\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Inference is the same regardless of which parser is used to build the engine, since the model architecture is the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Allocate buffers and create a CUDA stream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbindings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallocate_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Contexts are used to perform inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_execution_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-156-dc350d8a5478>\u001b[0m in \u001b[0;36mallocate_buffers\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbindings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbinding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_binding_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnptype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_binding_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 1 - mounting Google Drive:**"
      ],
      "metadata": {
        "id": "ISBC1nekdUBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cd \"gdrive/MyDrive/Colab Notebooks/introductory_parser_samples\" && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOQmtnQsdR45",
        "outputId": "fb29997d-1eb5-41f7-caa8-43a56a5eb7b7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "binoculars.jpeg\t\t\t   onnx_resnet50.ipynb\trequirements.txt\n",
            "_introductory_parser_samples.html  onnx_resnet50.py\n",
            "_introductory_parser_samples.url   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 2 - git clone:**"
      ],
      "metadata": {
        "id": "-UVC-fODdf9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NVIDIA/TensorRT.git"
      ],
      "metadata": {
        "id": "FOWGwvFubp7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cd TensorRT/samples/python/introductory_parser_samples/ && python onnx_resnet50.py"
      ],
      "metadata": {
        "id": "VKIqGf1LcE_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}