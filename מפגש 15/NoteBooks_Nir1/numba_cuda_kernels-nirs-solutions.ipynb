{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e82f1189",
      "metadata": {
        "id": "e82f1189"
      },
      "source": [
        "# Introduction to Nvidia GPU Hardware & Numba Cuda Kernels\n",
        "\n",
        "## GPU Architecture\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "### What is Flynn’s Taxonomy?\n",
        "Flynn’s Taxonomy is a categorization of computer architectures by Stanford University’s Michael J. Flynn. The basic idea behind Flynn’s Taxonomy is simple: computations consist of 2 streams (data and instructional streams) that can be processed in sequence(1 stream at a time) or in parallel (multiple streams at once). Two data streams with two possible methods to process them leads to the 4 different categories in Flynn’s Taxonomy. Let’s take a look at each.\n",
        "\n",
        "#### Single Instruction Single Data (SISD)\n",
        "SISD stream is an architecture where a single instruction stream (e.g. a program) executes on one data stream. This architecture is used in older computers with a single-core processor, as well as many simple compute devices.\n",
        "\n",
        "#### Single Instruction Multiple Data (SIMD)\n",
        "A SIMD stream architecture has a single control processor and instruction memory, so only one instruction can be run at any given point in time. That single instruction is copied and ran across each core at the same time. This is possible because each processor has its own dedicated memory which allows for parallelism at the data-level (a.k.a. “data parallelism”). \n",
        "\n",
        "The fundamental advantage of SIMD is that data parallelism allows it to execute computations quickly (multiple processors doing the same thing) and efficiently (only one instruction unit). \n",
        "\n",
        "#### Multiple Instruction Single Data (MISD)\n",
        "MISD stream architecture is effectively the reverse of SIMD architecture. With MISD multiple instructions are performed on the same data stream. The use cases for MISD are very limited today. Most practical applications are better addressed by one of the other architectures. \n",
        "\n",
        "#### Multiple Instruction Multiple Data (MIMD)\n",
        "MIMD stream architecture offers parallelism for both data and instruction streams. With MIMD, multiple processors execute instruction streams independently against different data streams. \n",
        "\n",
        "#### What makes SIMD best for GPUs? \n",
        "Now that we understand the different architectures, let’s consider why SIMD is the best choice for GPUs. The answer becomes intuitive when you understand that fundamentally graphics processing -- and many other common GPU computing use cases -- are simply running the same mathematical function over and over again at scale. In this case, many processors running the same instruction on multiple data sets is ideal.\n",
        "\n",
        "Case in point: adjusting video brightness of a pixel relies on simple arithmetic using RGB (red green blue) values. Executing the same function multiple times is what’s needed to produce the desired result, and SIMD is ideal for that use case. Conversely, MIMD is most effective in applications that call for multiple discrete computations to be executed such as computer-aided design (CAD). \n",
        "\n",
        "#### What about SIMT?\n",
        "If you’re familiar with GPUs, you’ve likely heard the term single instruction multiple threads (SIMT). So where does SIMT fit into Flynn’s Taxonomy? SIMT can be viewed as an extension of SIMD. It adds multithreading to SIMD which improves efficiency as there is less instruction fetching overhead. \n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "### CUDA parallel computing platform \n",
        "Stands for **Compute Unified Device Architecture** (CUDA), providing an API that enables developers to optimize how GPU resources are used -- without the need for specialized graphics programming knowledge -- CUDA has gone a long way in making GPUs useful for general purpose computing. \n",
        "\n",
        "#### CUDA compute hierarchy \n",
        "\n",
        "##### Threads \n",
        "A thread -- or CUDA core -- is a parallel processor that computes floating point math calculations in an Nvidia GPU. All the data processed by a GPU is processed via a CUDA core. Modern GPUs have hundreds or even thousands of CUDA cores.  Each CUDA core has its own memory register that is not available to other threads.\n",
        "\n",
        "While the relationship between compute power and CUDA cores is not perfectly linear, generally speaking -- and assuming all else is equal -- the more CUDA cores a GPU has, the more compute power it has. However, there are a variety of exceptions to this general idea. For example, different GPU microarchitectures can impact performance and make a GPU with fewer CUDA cores more powerful  \n",
        "\n",
        "##### Thread blocks \n",
        "As the name implies, a thread block -- or CUDA block -- is a grouping of CUDA cores (threads) that can be executed together in series or parallel. The logical grouping of cores enables more efficient data mapping. Thread blocks share memory on a per-block basis. Current CUDA architecture caps the amount of threads per block at 1024. Every thread in a given CUDA block can access the same shared memory (more on the different types of memory below). \n",
        "\n",
        "##### Kernel grids\n",
        "The next layer of abstraction up from thread blocks is the kernel grid. Kernel grids are groupings of thread blocks on the same kernel. Grids can be used to perform larger computations in parallel (e.g. those that require more than 1024 threads), however since different thread blocks cannot use the same shared memory, the same synchronization that occurs at the block-level does not occur at the grid-level. \n",
        "\n",
        "#### Cuda memory hirerchy\n",
        "Like compute resources, memory allocation follows a specific hierarchy in CUDA. While the CUDA compiler automatically handles memory allocation, CUDA developers can and do program to optimize memory usage directly. Here are the key concepts to understand about the CUDA memory hierarchy. \n",
        "\n",
        "##### Registers\n",
        "Registers are the memory that gets allocated to individual threads (CUDA cores). Because registers exist in “on-chip” memory and are dedicated to individual threads, the data stored in a register can be processed faster than any other data.  The allocation of memory in registers is a complicated process and is handled by compilers as opposed to being controlled by software CUDA developers write.\n",
        "\n",
        "##### Read-only memory\n",
        "Read-only (RO) is on-chip memory on GPU streaming multiprocessors. It is used for specific tasks such as texture memory which can be accessed using CUDA texture functions. In many cases, fetching data from read-only memory can be faster and more efficient than using global memory. \n",
        "\n",
        "##### L1 Cache/shared memory\n",
        "Layer 1 (L1)  cache and shared memory is on-chip memory that is shared within thread blocks (CUDA blocks). Because L1 cache and shared memory exists on-chip, it is faster than both L2 cache and global memory. The fundamental difference between L1 cache and shared memory is: shared memory usage is controlled via software while L1 cache is controlled by hardware. \n",
        "\n",
        "##### L2 Cache\n",
        "Layer 2 cache can be accessed by all threads in all CUDA blocks. L2 cache stores both global and local memory. Retrieving data from L2 cache is faster than retrieving data from global memory. \n",
        "\n",
        "##### Global memory\n",
        "Global memory is the memory that resides in a device’s DRAM. Using a CPU analogy, global memory is comparable to RAM. Fetching data from global memory is inherently slower than fetching it from L2 cache.  \n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "\n",
        "### Terminology\n",
        "* Host - the CPU and its memory\n",
        "* Device - the GPU and its memory\n",
        "* Kernel Function - a function that runs on the **device** but launched from the **host**\n",
        "* Device Function - a function that runs on the **device** but launched from the **kernel**\n",
        "\n",
        "\n",
        "### Simple Cuda Workflow\n",
        "![image-5.png](attachment:image-5.png)\n",
        "\n",
        "![image-6.png](attachment:image-6.png)\n",
        "\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "\n",
        "### Architecture vs Microarchitecture\n",
        "Architecture or ISA (Instruction set architecture) is an abstraction of data-types, registers and instruction types (add, subtract, multiply, etc..) to define a behavior of computational module.\n",
        "\n",
        "Microarchitecture is the way a given ISA (architecture) is implemented on a certain processor.\n",
        "\n",
        "Nvidia had launched throughout the years many different microarchitectures for their GPUs with different modifications to make the given ISA perform certain operations more efficiently on different hardwares developed by NVIDIA.\n",
        "\n",
        "Some of the recent microarchitectures (sorted from old to new):\n",
        "* Tesla\n",
        "* Fermi\n",
        "* Kepler\n",
        "* Maxwell\n",
        "* Pascal\n",
        "* Volta\n",
        "* Turing\n",
        "* Ampere"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc172667",
      "metadata": {
        "id": "fc172667"
      },
      "source": [
        "## Numba - Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "adcd1a2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adcd1a2e",
        "outputId": "0126a025-fa1d-4cfb-cb36-74fc8ad2aa34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.56.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.39.0)\n",
            "Requirement already satisfied: numpy<1.23,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49306ad",
      "metadata": {
        "id": "d49306ad"
      },
      "source": [
        "## Writing Cuda Kernels - Numba\n",
        "\n",
        "While targeting ufuncs with the cuda syntax is the most straightforward way to access the GPU with Numba, it may not be flexible enough for your needs. If you want to write a more detailed GPU program, at some point you are probably going to need to write CUDA kernels.\n",
        "\n",
        "As discussed in the lecture, the CUDA programming model allows you to abstract the GPU hardware into a software model composed of a grid containing blocks of threads. These threads are the smallest individual unit in the programming model, and they execute together in groups (traditionally called warps, consisting of 32 thread each). Determiming the best size for your grid of thread blocks is a complicated problem that often depends on the specific algorithm and hardware you're using, but here a few good rules of thumb:\n",
        "\n",
        "* the size of a block should be a multiple of 32 threads, with typical block sizes between 128 and 512 threads per block.\n",
        "* the size of the grid should ensure the full GPU is utilized where possible. Launching a grid where the number of blocks is 2x-4x the number of \"multiprocessors\" on the GPU is a good starting place. Something in the range of 20 - 100 blocks is usually a good starting point.\n",
        "* The CUDA kernel launch overhead does depend on the number of blocks, so it may not be best to launch a grid where the number of threads equals the number of input elements when the input size is very big. We'll show a pattern for dealing with large inputs below.\n",
        "\n",
        "\n",
        "As a first example, let's return to our vector addition function, but this time, we'll target it with the cuda.jit decorator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "52cb7cca",
      "metadata": {
        "id": "52cb7cca"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tidx = cuda.threadIdx.x # this is the unique thread ID within a 1D block\n",
        "    bidx = cuda.blockIdx.x  # Similarly, this is the unique block ID within the 1D grid\n",
        "\n",
        "    block_dimx = cuda.blockDim.x  # number of threads per block\n",
        "    grid_dimx = cuda.gridDim.x    # number of blocks in the grid\n",
        "    \n",
        "    start = tidx + bidx * block_dimx\n",
        "    stride = block_dimx * grid_dimx\n",
        "\n",
        "    # assuming x and y inputs are same length\n",
        "    for i in range(start, x.shape[0], stride):\n",
        "        out[i] = x[i] + y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5856951",
      "metadata": {
        "id": "c5856951"
      },
      "source": [
        "\n",
        "That's a lot more typing than our ufunc example, and it is much more limited: it only works on 1D arrays, it doesn't verify input sizes match, etc. Most of the function is spent figuring out how to turn the block and grid indices and dimensions into unique offsets in the input arrays. The pattern of computing a starting index and a stride is a common way to ensure that your grid size is independent of the input size. The striding will maximize bandwidth by ensuring that threads with consecuitive indices are accessing consecutive memory locations as much as possible. Thread indices beyond the length of the input (x.shape[0], since x is a NumPy array) automatically skip over the for loop.\n",
        "\n",
        "Let's call the function now on some data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "812df35b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "812df35b",
        "outputId": "7b1dbfbf-03fb-4446-96b2-1b6c6a2501fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 30 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 76.10 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 1.94 ms per loop\n",
            "[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "threads_per_block = 128\n",
        "blocks_per_grid = 30\n",
        "\n",
        "%timeit add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
        "print(out[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebad19e",
      "metadata": {
        "id": "9ebad19e"
      },
      "source": [
        "\n",
        "The calling syntax is designed to mimic the way CUDA kernels are launched in C, where the number of blocks per grid and threads per block are specified in the square brackets, and the arguments to the function are specified afterwards in parentheses.\n",
        "\n",
        "Note that, unlike the ufunc, the arguments are passed to the kernel as full NumPy arrays. A thread within the kernel can access any element in the array it wants, regardless of its position in the thread grid. This is why CUDA kernels are significantly more powerful than ufuncs. (But with great power, comes a greater amount of typing...)\n",
        "\n",
        "Numba has created some helper functions to cut down on the typing. We can write the previous kernel much more simply as: (http://numba.pydata.org/numba-doc/dev/cuda/kernels.html#absolute-positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ddf33b0e",
      "metadata": {
        "id": "ddf33b0e"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    start = cuda.grid(1)      # the 1 argument means a one dimensional thread grid, this returns a single value\n",
        "    stride = cuda.gridsize(1) # ditto\n",
        "\n",
        "    # assuming x and y inputs are same length\n",
        "    for i in range(start, x.shape[0], stride):\n",
        "        out[i] = x[i] + y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5605ff",
      "metadata": {
        "id": "cb5605ff"
      },
      "source": [
        "As before, using NumPy arrays forces Numba to allocate GPU memory, copy the arguments to the GPU, run the kernel, then copy the argument arrays back to the host. This not very efficient, so you will often want to allocate device arrays.\n",
        "\n",
        "#### Exercise - 1\n",
        "Allocate device arrays for x, y, and the output, then try out your new Cuda kernel using the pre-copied device arrays. Compare the time to a version without moving the data first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a04fb8d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a04fb8d0",
        "outputId": "1d8abeee-0cd8-4558-aa69-3952d1459c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 6.80 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 61.8 µs per loop\n",
            "[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"
          ]
        }
      ],
      "source": [
        "# Add your code here\n",
        "x_device = cuda.to_device(x)\n",
        "y_device = cuda.to_device(y)\n",
        "out_device = cuda.device_array(shape=(n,), dtype=np.float32)  # does not initialize the contents, much like np.empty()\n",
        "\n",
        "%timeit add_kernel[blocks_per_grid, threads_per_block](x_device, y_device, out_device)\n",
        "out = out_device.copy_to_host()\n",
        "print(out[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c3d4e5",
      "metadata": {
        "id": "90c3d4e5"
      },
      "source": [
        "### Atomic Operations and avoiding Race Conditions\n",
        "CUDA, like many general purpose parallel execution frameworks, makes it possible to have race conditions in your code. A race condition in CUDA arises when threads read or write a memory location that might be modified by another independent thread. Generally speaking, you need to worry about:\n",
        "\n",
        "__read-after-write hazards__: One thread is reading a memory location at the same time another thread might be writing to it.\n",
        "\n",
        "__write-after-write hazards__: Two threads are writing to the same memory location, and only one write will be visible when the kernel is complete.\n",
        "\n",
        "A common strategy to avoid both of these hazards is to organize your CUDA kernel algorithm such that each thread has exclusive responsibility for unique subsets of output array elements, and/or to never use the same array for both input and output in a single kernel call. (Iterative algorithms can use a double-buffering strategy if needed, and switch input and output arrays on each iteration.) However, there are many cases where different threads need to combine results. Consider something very simple, like: \"every thread increments a global counter.\" Implementing this in your kernel requires each thread to:\n",
        "\n",
        "* Read the current value of a global counter.\n",
        "* Compute counter + 1.\n",
        "* Write that value back to global memory. However, there is no guarantee that another thread has not changed the global counter between steps 1 and 3. To resolve this problem, CUDA provides __\"atomic operations\"__ which will read, modify and update a memory location in one, indivisible step. Numba supports several of these functions, described here.\n",
        "\n",
        "As an example, let's make a thread counter kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca59b3a9",
      "metadata": {
        "id": "ca59b3a9"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def thread_counter_race_condition(global_counter):\n",
        "    global_counter[0] += 1  # This is bad\n",
        "    \n",
        "@cuda.jit\n",
        "def thread_counter_safe(global_counter):\n",
        "    cuda.atomic.add(global_counter, 0, 1)  # Safely add 1 to offset 0 in global_counter array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2cc6e0",
      "metadata": {
        "id": "ce2cc6e0"
      },
      "outputs": [],
      "source": [
        "# This gets the wrong answer\n",
        "global_counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
        "thread_counter_race_condition[64, 64](global_counter)\n",
        "\n",
        "print('Should be %d:' % (64*64), global_counter.copy_to_host())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485e6e9e",
      "metadata": {
        "id": "485e6e9e"
      },
      "outputs": [],
      "source": [
        "# This works correctly\n",
        "global_counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
        "thread_counter_safe[64, 64](global_counter)\n",
        "\n",
        "print('Should be %d:' % (64*64), global_counter.copy_to_host())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebd7c50",
      "metadata": {
        "id": "9ebd7c50"
      },
      "source": [
        "#### Exercise - 2\n",
        "Let's practice writing a function that requires an atomic operation - a histogramming kernel. This will take an array of input data, a range and a number of bins, and count how many of the input data elements land in each bin. Below is an example CPU implementation of histogramming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6022acdc",
      "metadata": {
        "id": "6022acdc"
      },
      "outputs": [],
      "source": [
        "def cpu_histogram(x, xmin, xmax, histogram_out):\n",
        "    '''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n",
        "    # Note that we don't have to pass in nbins explicitly, because the size of histogram_out determines it\n",
        "    nbins = histogram_out.shape[0]\n",
        "    bin_width = (xmax - xmin) / nbins\n",
        "    \n",
        "    # This is a very slow way to do this with NumPy, but looks similar to what you will do on the GPU\n",
        "    for element in x:\n",
        "        bin_number = np.int32((element - xmin)/bin_width)\n",
        "        if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
        "            # only increment if in range\n",
        "            histogram_out[bin_number] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f963261",
      "metadata": {
        "id": "1f963261"
      },
      "outputs": [],
      "source": [
        "x = np.random.normal(size=10000, loc=0, scale=1).astype(np.float32)\n",
        "xmin = np.float32(-4.0)\n",
        "xmax = np.float32(4.0)\n",
        "histogram_out = np.zeros(shape=10, dtype=np.int32)\n",
        "\n",
        "cpu_histogram(x, xmin, xmax, histogram_out)\n",
        "\n",
        "histogram_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ae39c3",
      "metadata": {
        "id": "e5ae39c3"
      },
      "source": [
        "In the space below, create a cuda version of this kernel, then run it to check that you get the same answer as the CPU version.\n",
        "\n",
        "Hint: You can use much of the same syntax that we used in the CUDA addition kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2756200a",
      "metadata": {
        "id": "2756200a"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def cuda_histogram(x, xmin, xmax, histogram_out):\n",
        "    '''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n",
        "    \n",
        "    # add code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3c9818",
      "metadata": {
        "id": "5f3c9818"
      },
      "source": [
        "### Multi-dimensional grids\n",
        "For some problems, it makes sense to define a two- or three-dimensional grid of thread blocks. That way, when you're indexing a single thread, you can map it to, say, the pixel position in an image. Multi-dimensional grids are created by passing tuples to the kernel function. You can ensure that you launch a big enough grid by calculating the size of each dimension as a function of the array size and number of threads per block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d0d0cd",
      "metadata": {
        "id": "98d0d0cd"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install libgl1-mesa-glx -y\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06806b6a",
      "metadata": {
        "id": "06806b6a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "r = requests.get('https://images.theconversation.com/files/387712/original/file-20210304-15-n1wg4d.jpg?ixlib=rb-1.1.0&rect=133%2C110%2C3701%2C2473&q=45&auto=format&w=496&fit=clip', allow_redirects=True)\n",
        "open('frog.jpg', 'wb').write(r.content)\n",
        "\n",
        "test_image = cv2.imread('frog.jpg')\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(test_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60eabced",
      "metadata": {
        "id": "60eabced"
      },
      "source": [
        "Calculate blocks per image axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9e01fb",
      "metadata": {
        "id": "3e9e01fb"
      },
      "outputs": [],
      "source": [
        "threadsperblock = 32\n",
        "xblocks = (test_image.shape[1] + (threadsperblock - 1)) // threadsperblock\n",
        "yblocks = (test_image.shape[0] + (threadsperblock - 1)) // threadsperblock\n",
        "\n",
        "print(\"Xblocks: \", xblocks)\n",
        "print(\"Yblocks: \", yblocks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "702d7156",
      "metadata": {
        "id": "702d7156"
      },
      "source": [
        "Lets write and histogram function for an opencv image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de8fdd0",
      "metadata": {
        "id": "8de8fdd0"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def cv_histogram(x, histogram_out):\n",
        "    nbins = histogram_out.shape[1]\n",
        "    bin_width = 255 / nbins ## depth of each color\n",
        "    \n",
        "    pos = cuda.grid(2)\n",
        "    \n",
        "    r = np.int32(x[pos[0], pos[1]][0] / bin_width)\n",
        "    g = np.int32(x[pos[0], pos[1]][1] / bin_width)\n",
        "    b = np.int32(x[pos[0], pos[1]][2] / bin_width)\n",
        "    \n",
        "    if r >= 0 and r < histogram_out.shape[1]:\n",
        "        cuda.atomic.add(histogram_out[0], r, 1)\n",
        "    if g >= 0 and g < histogram_out.shape[1]:\n",
        "        cuda.atomic.add(histogram_out[1], g, 1)\n",
        "    if b >= 0 and b < histogram_out.shape[1]:\n",
        "        cuda.atomic.add(histogram_out[1], b, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd44758",
      "metadata": {
        "id": "acd44758"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "histogram_out = np.zeros(shape=[3, 20], dtype=np.int32)\n",
        "\n",
        "blocks_per_grid = (xblocks, yblocks)\n",
        "\n",
        "cv_histogram[blocks_per_grid, (threadsperblock, threadsperblock)](test_image, histogram_out)\n",
        "\n",
        "histogram_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f701540f",
      "metadata": {
        "id": "f701540f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(histogram_out[0], bins=histogram_out.shape[1], color='red')\n",
        "plt.hist(histogram_out[1], bins=histogram_out.shape[1], color='green')\n",
        "plt.hist(histogram_out[2], bins=histogram_out.shape[1], color='blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c23810",
      "metadata": {
        "id": "99c23810"
      },
      "source": [
        "### Exercise 3\n",
        "Create a CPU & GPU (cuda) function with numba to convert colored input image to grayscale, compare time of execution for the both of them\n",
        "\n",
        "Convert the image using the following formula\n",
        "\n",
        "Gray = 0.2126*R + + 0.7152*B + 0.0722*G"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "numba-cuda-kernels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}