{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab864ae3",
      "metadata": {
        "id": "ab864ae3"
      },
      "source": [
        "# Numba Introduction\n",
        "\n",
        "## What is Numba?\n",
        "Numba is a just-in-time, type-specializing, function compiler for accelerating numerically-focused Python. That's a long list, so let's break down those terms:\n",
        "\n",
        "* __function compiler__: Numba compiles Python functions, not entire applications, and not parts of functions. Numba does not replace your Python interpreter, but is just another Python module that can turn a function into a (usually) faster function.\n",
        "* __type-specializing__: Numba speeds up your function by generating a specialized implementation for the specific data types you are using. Python functions are designed to operate on generic data types, which makes them very flexible, but also very slow. In practice, you only will call a function with a small number of argument types, so Numba will generate a fast implementation for each set of types.\n",
        "* __just-in-time__: Numba translates functions when they are first called. This ensures the compiler knows what argument types you will be using. This also allows Numba to be used interactively in a Jupyter notebook just as easily as a traditional application.\n",
        "* __numerically-focused__: Currently, Numba is focused on numerical data types, like int, float, and complex. There is very limited string processing support, and many string use cases are not going to work well on the GPU. To get best results with Numba, you will likely be using NumPy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5370ce3a",
      "metadata": {
        "id": "5370ce3a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fbf9d5b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbf9d5b9",
        "outputId": "905ed365-0e7f-40c6-c072-6dada7477217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.56.0)\n",
            "Requirement already satisfied: numpy<1.23,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba) (4.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.39.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "364ce99e",
      "metadata": {
        "id": "364ce99e"
      },
      "source": [
        "## Problem I - First Numba Function\n",
        "To start our exploration of Numba's features, let's write a python function to add two numbers. We'll creatively name it add:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9975e27",
      "metadata": {
        "id": "e9975e27"
      },
      "outputs": [],
      "source": [
        "def add(a,b):\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7821f3a",
      "metadata": {
        "id": "f7821f3a"
      },
      "source": [
        "Just by looking at this function a clear issue comes to mind, lets demonstrate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f08a54e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f08a54e0",
        "outputId": "a29a89b5-4d86-4393-f0d1-e26f2f0bc61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int:  3\n",
            "Float:  3.8\n",
            "Numpy:  [4 6 8]\n",
            "Lists:  [1, 2, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Int: \", add(1, 2)) ## Add can sum up integers\n",
        "print(\"Float: \", add(1.5, 2.3)) ## Add can sum up floats\n",
        "print(\"Numpy: \", add(np.array([1,2,3]), np.array([3,4,5]))) ## Add can sum up numpy arrays\n",
        "\n",
        "print(\"Lists: \", add([1,2], [2,3])) ## Add can concate two python list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bdbb8a",
      "metadata": {
        "id": "a7bdbb8a"
      },
      "source": [
        "That means that python spends much time on being type agnostic, lets use numba to perform jit on add function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cf7262df",
      "metadata": {
        "id": "cf7262df"
      },
      "outputs": [],
      "source": [
        "from numba import jit\n",
        "\n",
        "numba_add = jit(add) # wrap function with jit function to generate just in time function\n",
        "\n",
        "@jit # one can also use the decorator which essentially does the same thing\n",
        "def add_test(a, b):\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8056e54",
      "metadata": {
        "id": "d8056e54"
      },
      "source": [
        "### Exercise \n",
        "Numba stores the old function in a class member called 'py_func', lets compare the results to see it fits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119c0e4c",
      "metadata": {
        "id": "119c0e4c"
      },
      "source": [
        "### Performance Test\n",
        "%timeit is running our function many times, and then reporting the average time it takes to run. This is generally a better approach than timing a single function execution, because it accounts for random events that may cause any given run to perform poorly.\n",
        "\n",
        "Lets time the original function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f0699b13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0699b13",
        "outputId": "6d00f797-004f-4100-f8cb-67e1b27a9de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000000 loops, best of 5: 87.3 ns per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit add(1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb3c10f",
      "metadata": {
        "id": "afb3c10f"
      },
      "source": [
        "Time the jit function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5e7ac7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5e7ac7f",
        "outputId": "f3910da8-c453-4240-b1dc-646dd3cd69f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 818579.81 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 297 ns per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit numba_add(1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee0e28f",
      "metadata": {
        "id": "4ee0e28f"
      },
      "source": [
        "Hold on - our new pre-compiled function is running even slower than the original python version! What's going on here?\n",
        "Numba isn't going to speed up everything. Generally, Numba will help you most in circumstances where python's line-by-line interperability and lack of type casting is slowing it down."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d2caba",
      "metadata": {
        "id": "14d2caba"
      },
      "source": [
        "Lets do more complicated function to demonstrate the difference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a13ea8f",
      "metadata": {
        "id": "2a13ea8f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def hypotenuse(x, y):\n",
        "  x = abs(x);\n",
        "  y = abs(y);\n",
        "  t = min(x, y);\n",
        "  x = max(x, y);\n",
        "  t = t / x;\n",
        "  return x * math.sqrt(1+t*t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd7ef6c",
      "metadata": {
        "id": "9dd7ef6c"
      },
      "source": [
        "### Exercise\n",
        "Convert the function hypotenuse to numba function and compare timing with original function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numba_hypotenuse = jit(hypotenuse)\n",
        "%timeit hypotenuse(3.4,4.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBbFBRqnvBOQ",
        "outputId": "86bdcb4a-dfdb-42cb-8e91-ebbba5206c85"
      },
      "id": "rBbFBRqnvBOQ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 6.39 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000000 loops, best of 5: 675 ns per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit numba_hypotenuse(3.4,4.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foXSTTzzvBZu",
        "outputId": "4e3ee987-a203-489f-cb18-53e6696066e4"
      },
      "id": "foXSTTzzvBZu",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 605072.99 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000000 loops, best of 5: 206 ns per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b49c01",
      "metadata": {
        "id": "02b49c01"
      },
      "source": [
        "Numba functions can call other functions, provided they are also Numba functions.\n",
        "\n",
        "**No Python mode vs Object mode** - \n",
        "A common pattern is to decorate functions with @jit as this is the most flexible decorator offered by Numba. @jit essentially encompasses two modes of compilation, first it will try and compile the decorated function in no Python mode, if this fails it will try again to compile the function using object mode. Whilst the use of looplifting in object mode can enable some performance increase, getting functions to compile under no python mode is really the key to good performance. To make it such that only no python mode is used and if compilation fails an exception is raised the decorators @njit and @jit(nopython=True) can be used (the first is an alias of the second for convenience).\n",
        "\n",
        "> **Note**: The behaviour of the nopython compilation mode is to essentially compile the decorated function so that it will run entirely without the involvement of the Python interpreter. This is the recommended and best-practice way to use the Numba jit decorator as it leads to the best performance.\n",
        "\n",
        "Numba, compiles each function based on the types of its arguments, and infers the type of the result. You can see this if you run the inspect_types function on a numba function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d98a25c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d98a25c2",
        "outputId": "8a2beb46-49ea-4ae4-bc62-eb69ee05406d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add (int64, int64)\n",
            "--------------------------------------------------------------------------------\n",
            "# File: <ipython-input-3-494b6d29bcde>\n",
            "# --- LINE 1 --- \n",
            "\n",
            "def add(a,b):\n",
            "\n",
            "    # --- LINE 2 --- \n",
            "    # label 0\n",
            "    #   a = arg(0, name=a)  :: int64\n",
            "    #   b = arg(1, name=b)  :: int64\n",
            "    #   $6binary_add.2 = a + b  :: int64\n",
            "    #   del b\n",
            "    #   del a\n",
            "    #   $8return_value.3 = cast(value=$6binary_add.2)  :: int64\n",
            "    #   del $6binary_add.2\n",
            "    #   return $8return_value.3\n",
            "\n",
            "    return a + b\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "numba_add.inspect_types()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numba_hypotenuse.inspect_types()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aurdEUVvwFjX",
        "outputId": "112b0499-794d-4543-90f3-df8ae2d4b08a"
      },
      "id": "aurdEUVvwFjX",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hypotenuse (float64, float64)\n",
            "--------------------------------------------------------------------------------\n",
            "# File: <ipython-input-8-9edeb24715a4>\n",
            "# --- LINE 3 --- \n",
            "\n",
            "def hypotenuse(x, y):\n",
            "\n",
            "  # --- LINE 4 --- \n",
            "  # label 0\n",
            "  #   x = arg(0, name=x)  :: float64\n",
            "  #   y = arg(1, name=y)  :: float64\n",
            "  #   $2load_global.0 = global(abs: <built-in function abs>)  :: Function(<built-in function abs>)\n",
            "  #   x.1 = call $2load_global.0(x, func=$2load_global.0, args=[Var(x, <ipython-input-8-9edeb24715a4>:4)], kws=(), vararg=None, varkwarg=None, target=None)  :: (float64,) -> float64\n",
            "  #   del x\n",
            "  #   del $2load_global.0\n",
            "\n",
            "  x = abs(x);\n",
            "\n",
            "  # --- LINE 5 --- \n",
            "  #   $10load_global.3 = global(abs: <built-in function abs>)  :: Function(<built-in function abs>)\n",
            "  #   y.1 = call $10load_global.3(y, func=$10load_global.3, args=[Var(y, <ipython-input-8-9edeb24715a4>:4)], kws=(), vararg=None, varkwarg=None, target=None)  :: (float64,) -> float64\n",
            "  #   del y\n",
            "  #   del $10load_global.3\n",
            "\n",
            "  y = abs(y);\n",
            "\n",
            "  # --- LINE 6 --- \n",
            "  #   $18load_global.6 = global(min: <built-in function min>)  :: Function(<built-in function min>)\n",
            "  #   t = call $18load_global.6(x.1, y.1, func=$18load_global.6, args=[Var(x.1, <ipython-input-8-9edeb24715a4>:4), Var(y.1, <ipython-input-8-9edeb24715a4>:5)], kws=(), vararg=None, varkwarg=None, target=None)  :: (float64, float64) -> float64\n",
            "  #   del $18load_global.6\n",
            "\n",
            "  t = min(x, y);\n",
            "\n",
            "  # --- LINE 7 --- \n",
            "  #   $28load_global.10 = global(max: <built-in function max>)  :: Function(<built-in function max>)\n",
            "  #   x.2 = call $28load_global.10(x.1, y.1, func=$28load_global.10, args=[Var(x.1, <ipython-input-8-9edeb24715a4>:4), Var(y.1, <ipython-input-8-9edeb24715a4>:5)], kws=(), vararg=None, varkwarg=None, target=None)  :: (float64, float64) -> float64\n",
            "  #   del y.1\n",
            "  #   del x.1\n",
            "  #   del $28load_global.10\n",
            "\n",
            "  x = max(x, y);\n",
            "\n",
            "  # --- LINE 8 --- \n",
            "  #   t.1 = t / x.2  :: float64\n",
            "  #   del t\n",
            "\n",
            "  t = t / x;\n",
            "\n",
            "  # --- LINE 9 --- \n",
            "  #   $48load_global.18 = global(math: <module 'math' (built-in)>)  :: Module(<module 'math' (built-in)>)\n",
            "  #   $50load_method.19 = getattr(value=$48load_global.18, attr=sqrt)  :: Function(<built-in function sqrt>)\n",
            "  #   del $48load_global.18\n",
            "  #   $const52.20 = const(int, 1)  :: Literal[int](1)\n",
            "  #   $58binary_multiply.23 = t.1 * t.1  :: float64\n",
            "  #   del t.1\n",
            "  #   $60binary_add.24 = $const52.20 + $58binary_multiply.23  :: float64\n",
            "  #   del $const52.20\n",
            "  #   del $58binary_multiply.23\n",
            "  #   $62call_method.25 = call $50load_method.19($60binary_add.24, func=$50load_method.19, args=[Var($60binary_add.24, <ipython-input-8-9edeb24715a4>:9)], kws=(), vararg=None, varkwarg=None, target=None)  :: (float64,) -> float64\n",
            "  #   del $60binary_add.24\n",
            "  #   del $50load_method.19\n",
            "  #   $64binary_multiply.26 = x.2 * $62call_method.25  :: float64\n",
            "  #   del x.2\n",
            "  #   del $62call_method.25\n",
            "  #   $66return_value.27 = cast(value=$64binary_multiply.26)  :: float64\n",
            "  #   del $64binary_multiply.26\n",
            "  #   return $66return_value.27\n",
            "\n",
            "  return x * math.sqrt(1+t*t)\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d9290a",
      "metadata": {
        "id": "15d9290a"
      },
      "source": [
        "So far we have been using what Numba refers to as \"lazy\" (or \"call-time\") decoration. Basically, we've been letting Numba do the work of figuring out how we're using the function and inferring the types for us. Alternatively, if we know how we are going to use a given function, we can use \"eager\" (or \"compile-time\") decoration. To do this, we make use of the vectorize decorator. For example, if we want to make an integer-only version of our addition function, we could write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "535797df",
      "metadata": {
        "id": "535797df"
      },
      "outputs": [],
      "source": [
        "from numba import vectorize\n",
        "\n",
        "@vectorize(['int64(int64, int64)'], target='cpu')\n",
        "def add_ufunc_cpu(x, y):\n",
        "  return x + y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#numba_add_ufunc_cpu = jit(add_ufunc_cpu)\n",
        "#numba_add_ufunc_cpu.inspect_types()"
      ],
      "metadata": {
        "id": "cU_7Zhlxw5y2"
      },
      "id": "cU_7Zhlxw5y2",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "15f96562",
      "metadata": {
        "id": "15f96562"
      },
      "source": [
        "You'll notice a couple of new things here. In the first set of brackets, we have specified both the argument types of the function (those are inside the parentheses), as well as the return type of the function. This is just making explicit what Numba was previously inferring on our behalf. In second set of brackets you'll see that we have specified a 'target' architechture for the function. The default is cpu, which means that Numba is optimizing the function to your specific machine. Other options include parallel, which allows you to take advantage of multicore processors, and cuda\n",
        "\n",
        "> **Note**: the 'u' prefix in add_ufunc numpy means universal functions, a universal function (or ufunc for short) is a function that operates on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features. That is, a ufunc is a “vectorized” wrapper for a function that takes a fixed number of specific inputs and produces a fixed number of specific outputs.\n",
        "\n",
        "### Exercise\n",
        "Run our add_ufunc with cpu and parallel mode and time the difference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc_cpu(345, 543)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9PMntH9yIvV",
        "outputId": "e853fdd4-e1f9-4584-cfef-4ee6a4283ba8"
      },
      "id": "j9PMntH9yIvV",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 86.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000000 loops, best of 5: 934 ns per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@vectorize(['int64(int64, int64)'], target='parallel')\n",
        "def add_ufunc_par(x, y):\n",
        "  return x + y\n",
        "\n",
        "%timeit add_ufunc_par(345, 543)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbeaYkJCyJJv",
        "outputId": "00ab3222-5ea4-4ee3-aa9a-2eec58988df2"
      },
      "id": "bbeaYkJCyJJv",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 25.42 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100000 loops, best of 5: 2.92 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771aa699",
      "metadata": {
        "id": "771aa699"
      },
      "source": [
        "## GPU interaction with Numba\n",
        "\n",
        "As mentioned before numba can interact with Nvidia GPU (cuda) and even write fully functional kernels in python API only.\n",
        "\n",
        "### Problem II - Vector addition running on GPU\n",
        "Lets run the previous ufunction that we ran before with 'cuda' as a target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5a23f97a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a23f97a",
        "outputId": "04c5b6e9-a92f-4c76-de38-a76cf5cc0c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "@vectorize(['int64(int64, int64)'], target='cuda')\n",
        "def add_ufunc_cuda(x, y):\n",
        "    return x + y\n",
        "\n",
        "x = np.arange(10)\n",
        "y = 2 * x\n",
        "\n",
        "add_ufunc_cuda(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "675ed7e9",
      "metadata": {
        "id": "675ed7e9"
      },
      "source": [
        "\n",
        "What actually just happened?\n",
        "\n",
        "* Numba compiled a CUDA kernel to execute the ufunc operation in parallel over all the input elements.\n",
        "* It allocated GPU memory for the inputs and the output.\n",
        "* It also copied the input data to the GPU.\n",
        "* Numba executed the CUDA kernel with the correct kernel dimensions given the input sizes.\n",
        "* Copied the result back from the GPU to the CPU.\n",
        "* Returned the result as a NumPy array on the host.\n",
        "\n",
        "### Exercise\n",
        "Compare the cuda based function to the CPU based function, measure the execution time difference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc_cpu(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R96WRDcH0lbh",
        "outputId": "6bd3858c-4d22-4c63-cd4f-f968b6a22d46"
      },
      "id": "R96WRDcH0lbh",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 59.96 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000000 loops, best of 5: 538 ns per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc_cuda(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ofFvbv1I-g",
        "outputId": "b33e8fda-438b-4206-f2ab-a6a16c196b63"
      },
      "id": "n9ofFvbv1I-g",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 1.02 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is the GPU function slower?** \n",
        "\n",
        "* Our inputs are too small: the GPU achieves performance through parallelism, operating on thousands of values at once. Our test inputs have only 10 integers. We need a much larger array to even keep the GPU busy.\n",
        "* Our calculation is too simple: Sending a calculation to the GPU involves quite a bit of overhead compared to calling a function on the CPU. If our calculation does not involve enough math operations (\"arithmetic intensity\"), then the GPU will spend most of its time waiting for data to move around.\n",
        "* We copy the data to and from the GPU: While including the copy time can be realistic for a single function, often we want to run several GPU operations in sequence. In those cases, it makes sense to send data to the GPU and keep it there until all of our processing is complete.\n",
        "* Our data types are larger than necessary: Our example uses int64 when we probably don't need it. Scalar code using data types that are 32 and 64-bit run basically the same speed on the CPU, but 64-bit data types have a significant performance cost on the GPU. Basic arithmetic on 64-bit floats can be anywhere from 2x (Pascal-architecture Tesla) to 24x (Maxwell-architecture GeForce) slower than 32-bit floats. NumPy defaults to 64-bit data types when creating arrays, so it is important to set the dtype attribute or use the ndarray.astype() method to pick 32-bit types when you need them."
      ],
      "metadata": {
        "id": "gDjYaokj0mcf"
      },
      "id": "gDjYaokj0mcf"
    },
    {
      "cell_type": "markdown",
      "id": "b25137ff",
      "metadata": {
        "id": "b25137ff"
      },
      "source": [
        "As we saw in the last problem, Numba can automatically handle transferring data to and from the GPU for us. However, that's not always what we want. Sometimes we will want to perform several functions in a row on the GPU without transferring the data back to the CPU in between.\n",
        "\n",
        "Lets take the previous function and change the data type to float32, so it will execute efficiently on our GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1bc3596f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bc3596f",
        "outputId": "068054a9-3c00-4741-addd-e2f875ce74eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 114.74 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 984 µs per loop\n"
          ]
        }
      ],
      "source": [
        "@vectorize(['float32(float32, float32)'], target='cuda') # add code here\n",
        "def add_ufunc_cuda_flt32(x, y):\n",
        "    return x + y\n",
        "\n",
        "x = np.arange(10).astype(np.float32)\n",
        "y = 2 * x\n",
        "\n",
        "%timeit add_ufunc_cuda_flt32(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe0f75",
      "metadata": {
        "id": "66fe0f75"
      },
      "source": [
        "As we saw in the last problem, copying the data to and from the GPU for every function is not necessarily the most efficient way to use the GPU. To address this, Numba provides the to_device function in the cuda module to allocate and copy arrays to the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "465661df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465661df",
        "outputId": "5080e76f-72e3-49a2-db73-aeac630d5456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x7fc38a61e0d0>\n",
            "(100000,)\n",
            "float32\n"
          ]
        }
      ],
      "source": [
        "from numba import cuda\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "\n",
        "x_device = cuda.to_device(x)\n",
        "y_device = cuda.to_device(y)\n",
        "\n",
        "print(x_device)\n",
        "print(x_device.shape)\n",
        "print(x_device.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2339186",
      "metadata": {
        "id": "a2339186"
      },
      "source": [
        "x_device and y_device are now Numba \"device arrays\" that are in many ways equivalent to Numpy ndarrays except that they live in the GPU's global memory, rather than on the CPU. These device arrays can be passed to Numba cuda functions just the way Numpy arrays can, but without the memory copying overhead.\n",
        "\n",
        "### Exercise\n",
        "Try running the cuda kernel using the host memory and device memory, time the difference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc_cuda_flt32(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T74Uahv14UDb",
        "outputId": "777afd04-085e-44f5-c9e0-5d0edf1e8b88"
      },
      "id": "T74Uahv14UDb",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 98 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 1.53 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc_cuda_flt32(x_device, y_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycmLbFRM4eSZ",
        "outputId": "23f43ce7-a0e0-4993-91d3-804a8a55407e"
      },
      "id": "ycmLbFRM4eSZ",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 998 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7a7347",
      "metadata": {
        "id": "ff7a7347"
      },
      "source": [
        "You should see a big performance improvement already, but we are still allocating a device array for the output of the ufunc and copying it back to the host. We can create an output buffer on the GPU with the numba.cuda.device_array() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c0d9418b",
      "metadata": {
        "id": "c0d9418b"
      },
      "outputs": [],
      "source": [
        "out_device = cuda.device_array(shape=(n,), dtype=np.float32)  # does not initialize the contents, much like np.empty()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299aa09d",
      "metadata": {
        "id": "299aa09d"
      },
      "source": [
        "And then we can use a special out keyword argument to the ufunc to specify the output buffer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "82532d26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82532d26",
        "outputId": "c40c10fb-e562-4621-949c-64490063158c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 851 µs per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit add_ufunc_cuda_flt32(x_device, y_device, out=out_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc691352",
      "metadata": {
        "id": "cc691352"
      },
      "source": [
        "That way we can sepperate the GPU operation from the memory operations and manage it more efficiently\n",
        "\n",
        "### Exercise\n",
        "Remake a new version of the addition ufunc with 32bit floats that targets the cpu. Compare the resulting time to execute with the gpu version you just timed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "553ec0c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "553ec0c3",
        "outputId": "b2156a96-517e-4e42-e491-94ea8289df68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 69.5 µs per loop\n"
          ]
        }
      ],
      "source": [
        "from numba import vectorize\n",
        "\n",
        "@vectorize(['float32(float32, float32)'], target='cpu')\n",
        "def add_ufunc_cpu_flt32(x, y):\n",
        "  return x + y\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "\n",
        "%timeit add_ufunc_cpu_flt32(x, y)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "numba-introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}